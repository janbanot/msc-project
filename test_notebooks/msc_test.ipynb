{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-project/blob/main/test_notebooks/msc_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Toxic Comment Classification with DistilBERT and XAI Analysis\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Fine-tuning DistilBERT for multi-label toxicity classification\n",
        "2. Explainability methods (Integrated Gradients, Input×Gradient)\n",
        "3. Layer-wise probing analysis\n",
        "4. Representation engineering for model steering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N9kkURRXxcaG"
      },
      "outputs": [],
      "source": [
        "!uv pip install --upgrade transformers datasets captum quantus accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnzL_6SCy5qj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7bbe3ea"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_path = '/drive/MyDrive/msc-project/jigsaw-toxic-comment/train.csv'\n",
        "\n",
        "try:\n",
        "    dataframe = pd.read_csv(csv_path)\n",
        "    print(\"CSV file loaded successfully!\")\n",
        "    display(dataframe.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file was not found at {csv_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(example):\n",
        "    \"\"\"\n",
        "    Applies comprehensive cleaning steps to the 'comment_text' field.\n",
        "    \n",
        "    Cleaning operations:\n",
        "    - Converts text to lowercase (important for uncased BERT models)\n",
        "    - Removes URLs and IP addresses\n",
        "    - Removes Wikipedia metadata (talk pages, timestamps)\n",
        "    - Removes special characters and normalizes whitespace\n",
        "    \n",
        "    Args:\n",
        "        example: Dictionary containing 'comment_text' field\n",
        "        \n",
        "    Returns:\n",
        "        Updated example dictionary with cleaned text\n",
        "    \"\"\"\n",
        "    text = example['comment_text']\n",
        "    \n",
        "    # Convert to lowercase for uncased BERT models\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove URLs (http/https and www patterns)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    \n",
        "    # Remove IP addresses (e.g., 192.168.1.1)\n",
        "    text = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', '', text)\n",
        "    \n",
        "    # Remove Wikipedia-specific metadata\n",
        "    text = re.sub(r'\\(talk\\)', '', text)\n",
        "    text = re.sub(r'\\d{2}:\\d{2}, \\w+ \\d{1,2}, \\d{4} \\(utc\\)', '', text)\n",
        "    \n",
        "    # Remove newlines and non-breaking spaces\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = text.replace('\\xa0', ' ')\n",
        "    \n",
        "    # Strip quotes from beginning/end\n",
        "    text = text.strip(' \"')\n",
        "    \n",
        "    # Normalize whitespace (collapse multiple spaces into one)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    example['comment_text'] = text\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMcBMwOZZOYj"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "# Use first 2000 samples for faster experimentation\n",
        "train_dataframe = dataframe.head(2000)\n",
        "dataset = datasets.Dataset.from_pandas(train_dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XSauFnTY4GY"
      },
      "outputs": [],
      "source": [
        "print(\"Cleaning data...\")\n",
        "cleaned_dataset = dataset.map(clean_text)\n",
        "print(\"Data cleaning complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Eij-Oi0ZTkh"
      },
      "outputs": [],
      "source": [
        "# Compare before and after cleaning\n",
        "print(\"=== BEFORE CLEANING ===\")\n",
        "print(dataset[1]['comment_text'])\n",
        "print(\"\\n\" + dataset[6]['comment_text'])\n",
        "print(\"\\n\" + dataset[0]['comment_text'])\n",
        "\n",
        "print(\"\\n\\n=== AFTER CLEANING ===\")\n",
        "print(cleaned_dataset[1]['comment_text'])\n",
        "print(\"\\n\" + cleaned_dataset[6]['comment_text'])\n",
        "print(\"\\n\" + cleaned_dataset[0]['comment_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdmXlYxUczCB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer for DistilBERT uncased model\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    print(\"Tokenizer loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7di3UMZHdB1U"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenizes a batch of text for BERT models.\n",
        "    \n",
        "    - padding=\"max_length\": Pads short comments to uniform length\n",
        "    - truncation=True: Cuts off comments exceeding max length\n",
        "    - max_length=256: Balance between context and speed (DistilBERT max is 512)\n",
        "    \n",
        "    Args:\n",
        "        examples: Batch of examples with 'comment_text' field\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with tokenized outputs (input_ids, attention_mask)\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"comment_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "# Apply tokenization with batching for efficiency\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_dataset = cleaned_dataset.map(tokenize_function, batched=True)\n",
        "print(\"Tokenization complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn-sppLedLSN"
      },
      "outputs": [],
      "source": [
        "# Example of tokenized entry\n",
        "print(\"=== Example Tokenized Entry ===\")\n",
        "print(tokenized_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Label Preparation for Multi-Label Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8F9I2V2d-I0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the 6 toxicity label columns in order\n",
        "label_columns = [\n",
        "    'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'\n",
        "]\n",
        "\n",
        "def create_labels_column(example):\n",
        "    \"\"\"\n",
        "    Consolidates 6 separate binary labels into a single 'labels' array.\n",
        "    Converts values to float32 for PyTorch compatibility.\n",
        "    \n",
        "    Args:\n",
        "        example: Dictionary with individual label columns\n",
        "        \n",
        "    Returns:\n",
        "        Updated example with 'labels' list\n",
        "    \"\"\"\n",
        "    labels_list = [float(example[col]) for col in label_columns]\n",
        "    example['labels'] = labels_list\n",
        "    return example\n",
        "\n",
        "print(\"Consolidating labels...\")\n",
        "final_dataset = tokenized_dataset.map(create_labels_column)\n",
        "print(\"Labels consolidated!\")\n",
        "\n",
        "# Display example with all labels\n",
        "print(\"\\n=== Example Processed Entry ===\")\n",
        "print(final_dataset[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdvRwT_leOM6"
      },
      "outputs": [],
      "source": [
        "# Remove unnecessary columns to prepare for training\n",
        "columns_to_remove = [\n",
        "    'id', 'comment_text', 'toxic', 'severe_toxic',\n",
        "    'obscene', 'threat', 'insult', 'identity_hate'\n",
        "]\n",
        "\n",
        "print(f\"Original columns: {final_dataset.column_names}\")\n",
        "final_dataset = final_dataset.remove_columns(columns_to_remove)\n",
        "print(f\"Remaining columns: {final_dataset.column_names}\")\n",
        "\n",
        "# Set dataset format to PyTorch tensors\n",
        "try:\n",
        "    final_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "    print(\"\\nDataset format set to 'torch'!\")\n",
        "except ImportError:\n",
        "    print(\"\\nPyTorch not installed. Skipping .set_format('torch').\")\n",
        "    print(\"Install with: pip install torch\")\n",
        "\n",
        "print(\"\\n=== Final Model-Ready Item ===\")\n",
        "print(final_dataset[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Setup and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY2kk5asekT8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "num_labels = 6  # Six toxicity categories\n",
        "\n",
        "# Load pre-trained model and configure for multi-label classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(\"Configured for multi-label classification with 6 outputs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EUdT7kYeyof"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training and evaluation sets\n",
        "data_splits = final_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "train_dataset = data_splits['train']\n",
        "evaluation_dataset = data_splits['test']\n",
        "\n",
        "print(\"Data split complete:\")\n",
        "print(f\"  Training samples: {len(train_dataset)}\")\n",
        "print(f\"  Evaluation samples: {len(evaluation_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX52g5ElfBO7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "import torch\n",
        "\n",
        "def compute_metrics(prediction: EvalPrediction):\n",
        "    \"\"\"\n",
        "    Computes evaluation metrics for multi-label classification.\n",
        "    \n",
        "    Args:\n",
        "        prediction: EvalPrediction object with predictions and label_ids\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with F1 micro and overall accuracy\n",
        "    \"\"\"\n",
        "    # Apply sigmoid to logits to get probabilities\n",
        "    logits = prediction.predictions\n",
        "    probabilities = 1 / (1 + np.exp(-logits))\n",
        "    \n",
        "    # Convert probabilities to binary predictions (threshold = 0.5)\n",
        "    threshold = 0.5\n",
        "    predictions = (probabilities > threshold).astype(int)\n",
        "    \n",
        "    labels = prediction.label_ids\n",
        "    \n",
        "    # Micro-averaged F1 (good for imbalanced labels)\n",
        "    f1_micro = f1_score(labels, predictions, average='micro')\n",
        "    \n",
        "    # Overall accuracy across all labels\n",
        "    overall_accuracy = accuracy_score(labels.flatten(), predictions.flatten())\n",
        "    \n",
        "    return {\n",
        "        'f1_micro': f1_micro,\n",
        "        'accuracy': overall_accuracy\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd4r8HXrfO8K"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "model_output_dir = \"/drive/MyDrive/msc-project/models/distilbert-jigsaw-finetuned\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_output_dir,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,  # Regularization to prevent overfitting\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=5,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_micro\",\n",
        "    report_to=\"none\",  # Disable wandb logging\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgN5hlo3fowE"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=evaluation_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"=== Starting Training ===\")\n",
        "trainer.train()\n",
        "print(\"=== Training Complete ===\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlksX5w2b_-H"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Create timestamped save directory\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "base_path = \"/drive/MyDrive/msc-project/models/final_distilbert_jigsaw\"\n",
        "save_directory = f\"{base_path}_{timestamp}\"\n",
        "\n",
        "# Save both model and tokenizer\n",
        "trainer.save_model(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "print(f\"Model and tokenizer saved to: {save_directory}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Inference and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjKLCf6bcgqw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Test sentence with mixed toxic and positive content\n",
        "test_text = \"you are a fucking moron, who should die in hell but I love your lovely kitten\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    # Use sigmoid for multi-label classification\n",
        "    probabilities = torch.sigmoid(logits)\n",
        "\n",
        "# Display results for each label\n",
        "labels_list = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "print(f\"Text: '{test_text}'\\n\")\n",
        "print(\"Toxicity Probabilities:\")\n",
        "for label, probability in zip(labels_list, probabilities[0]):\n",
        "    print(f\"  {label}: {probability:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Explainability Analysis with Integrated Gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVYwRbBVdZB0"
      },
      "outputs": [],
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "# Define prediction function wrapper for Captum\n",
        "def predict_function(inputs_embeds, attention_mask=None):\n",
        "    \"\"\"Wrapper function that returns model logits for Captum.\"\"\"\n",
        "    output = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "    return output.logits\n",
        "\n",
        "# Initialize Integrated Gradients\n",
        "integrated_gradients = IntegratedGradients(predict_function)\n",
        "\n",
        "# Select target label for attribution\n",
        "# 0=toxic, 1=severe_toxic, 2=obscene, 3=threat, 4=insult, 5=identity_hate\n",
        "target_label_index = 0\n",
        "target_name = labels_list[target_label_index]\n",
        "\n",
        "# Prepare input embeddings\n",
        "input_ids = inputs.input_ids\n",
        "input_embeddings = model.distilbert.embeddings(input_ids)\n",
        "\n",
        "# Create baseline (padding tokens as reference)\n",
        "reference_input_ids = torch.tensor(\n",
        "    [tokenizer.pad_token_id] * input_ids.size(1), \n",
        "    device=device\n",
        ").unsqueeze(0)\n",
        "reference_embeddings = model.distilbert.embeddings(reference_input_ids)\n",
        "\n",
        "# Prepare attention mask\n",
        "attention_mask = inputs.attention_mask\n",
        "\n",
        "# Compute attributions\n",
        "print(f\"Computing attributions for: {target_name}...\")\n",
        "\n",
        "attributions, delta = integrated_gradients.attribute(\n",
        "    inputs=input_embeddings,\n",
        "    baselines=reference_embeddings,\n",
        "    target=target_label_index,\n",
        "    additional_forward_args=(attention_mask,),\n",
        "    return_convergence_delta=True\n",
        ")\n",
        "\n",
        "print(f\"Attribution complete. Convergence delta: {delta.item():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0ur6jyFd7E3"
      },
      "outputs": [],
      "source": [
        "from captum.attr import visualization\n",
        "\n",
        "# Process attributions for visualization\n",
        "attributions_sum = attributions.sum(dim=-1).squeeze(0)\n",
        "attributions_sum = attributions_sum / torch.norm(attributions_sum)\n",
        "attributions_numpy = attributions_sum.cpu().detach().numpy()\n",
        "\n",
        "# Get probability for target label\n",
        "probability_score = probabilities[0][target_label_index].item()\n",
        "predicted_class_label = \"True\" if probability_score > 0.5 else \"False\"\n",
        "\n",
        "# Convert input IDs to tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "# Create visualization data record\n",
        "visualization_data = visualization.VisualizationDataRecord(\n",
        "    word_attributions=attributions_numpy,\n",
        "    pred_prob=probability_score,\n",
        "    pred_class=predicted_class_label,\n",
        "    true_class=1,  # Assume text is toxic\n",
        "    attr_class=target_name,\n",
        "    attr_score=attributions_numpy.sum(),\n",
        "    raw_input_ids=tokens,\n",
        "    convergence_score=delta\n",
        ")\n",
        "\n",
        "print(f\"\\nLabel explanation: {target_name}\")\n",
        "visualization.visualize_text([visualization_data])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Layer-wise Probing Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exduABBQeocg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Enable hidden states output in model configuration\n",
        "model.config.output_hidden_states = True\n",
        "\n",
        "def extract_hidden_states(data_subset, layer_index=4):\n",
        "    \"\"\"\n",
        "    Extracts hidden state representations from a specific layer.\n",
        "    \n",
        "    Args:\n",
        "        data_subset: Dataset subset to extract from\n",
        "        layer_index: Which transformer layer to extract (0-6 for DistilBERT)\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (hidden_states_array, labels_array)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_hidden_states = []\n",
        "    all_labels = []\n",
        "    \n",
        "    print(f\"Extracting representations from layer {layer_index}...\")\n",
        "    \n",
        "    for i in tqdm(range(len(data_subset))):\n",
        "        entry = data_subset[i]\n",
        "        \n",
        "        text_tensor = entry['input_ids'].unsqueeze(0).to(device)\n",
        "        mask_tensor = entry['attention_mask'].unsqueeze(0).to(device)\n",
        "        label = entry['labels'][0].item()  # Extract first label (toxic)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(text_tensor, attention_mask=mask_tensor)\n",
        "            hidden_state = outputs.hidden_states[layer_index]\n",
        "            # Extract CLS token embedding (first token)\n",
        "            cls_embedding = hidden_state[0, 0, :].cpu().numpy()\n",
        "            \n",
        "            all_hidden_states.append(cls_embedding)\n",
        "            all_labels.append(label)\n",
        "    \n",
        "    return np.array(all_hidden_states), np.array(all_labels)\n",
        "\n",
        "# Determine subset size for analysis\n",
        "total_evaluation_samples = len(evaluation_dataset)\n",
        "target_size = 500\n",
        "subset_size = min(target_size, total_evaluation_samples)\n",
        "\n",
        "print(f\"Available samples: {total_evaluation_samples}. Using: {subset_size}\")\n",
        "\n",
        "test_subset = evaluation_dataset.select(range(subset_size))\n",
        "\n",
        "# Extract hidden states from layer 4\n",
        "hidden_states_matrix, labels_array = extract_hidden_states(test_subset, layer_index=4)\n",
        "\n",
        "print(f\"\\nHidden states shape: {hidden_states_matrix.shape}\")\n",
        "print(f\"Labels shape: {labels_array.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GAOZapZe5tn"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Split extracted representations into train/test sets for probe\n",
        "X_train_probe, X_test_probe, y_train_probe, y_test_probe = train_test_split(\n",
        "    hidden_states_matrix, labels_array, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train linear probe classifier\n",
        "probe_classifier = LogisticRegression(max_iter=1000)\n",
        "probe_classifier.fit(X_train_probe, y_train_probe)\n",
        "\n",
        "# Evaluate probe performance\n",
        "y_predictions_probe = probe_classifier.predict(X_test_probe)\n",
        "\n",
        "accuracy = accuracy_score(y_test_probe, y_predictions_probe)\n",
        "f1 = f1_score(y_test_probe, y_predictions_probe)\n",
        "\n",
        "print(\"=== Probe Results (Layer 4) ===\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "if accuracy > 0.80:\n",
        "    print(\"\\n✓ Layer 4 has strong toxicity representation\")\n",
        "else:\n",
        "    print(\"\\n✗ Layer 4 does not have strong toxicity representation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6x3w5-0fO3e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Extract CAV (Concept Activation Vector) from trained probe\n",
        "concept_vector = probe_classifier.coef_[0]\n",
        "intercept = probe_classifier.intercept_[0]\n",
        "\n",
        "# Project test data onto the toxicity concept vector\n",
        "projected_scores = np.dot(X_test_probe, concept_vector) + intercept\n",
        "\n",
        "# Separate scores by true label (toxic vs non-toxic)\n",
        "scores_toxic = projected_scores[y_test_probe == 1]\n",
        "scores_safe = projected_scores[y_test_probe == 0]\n",
        "\n",
        "# Create histogram visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "sns.histplot(scores_safe, color=\"green\", label=\"Non-Toxic\", kde=True, alpha=0.5)\n",
        "sns.histplot(scores_toxic, color=\"red\", label=\"Toxic\", kde=True, alpha=0.5)\n",
        "\n",
        "plt.axvline(0, color='black', linestyle='--', label=\"Decision Boundary\")\n",
        "plt.title(f\"Activation Distribution Along CAV (Layer 4)\\nAccuracy: {accuracy:.2f}, F1: {f1:.2f}\")\n",
        "plt.xlabel(\"Projection Score (higher = more toxic)\")\n",
        "plt.ylabel(\"Number of Examples\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. XAI Methods Comparison and Faithfulness Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGCEVlF6fqPC"
      },
      "outputs": [],
      "source": [
        "import quantus\n",
        "import numpy as np\n",
        "\n",
        "def model_predict_numpy(model, inputs, **kwargs):\n",
        "    \"\"\"\n",
        "    Prediction function wrapper for Quantus (accepts numpy arrays).\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        inputs: Numpy array of token IDs [batch_size, seq_len]\n",
        "        \n",
        "    Returns:\n",
        "        Numpy array of probabilities\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    input_tensor = torch.tensor(inputs, device=device).long()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        return torch.sigmoid(outputs.logits).cpu().numpy()\n",
        "\n",
        "def explain_function_numpy(model, inputs, targets, **kwargs):\n",
        "    \"\"\"\n",
        "    Explanation function wrapper for Quantus using Integrated Gradients.\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        inputs: Numpy array of token IDs\n",
        "        targets: Array of target class indices\n",
        "        \n",
        "    Returns:\n",
        "        Numpy array of attribution scores per token\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    input_tensor = torch.tensor(inputs, device=device).long()\n",
        "    \n",
        "    # Create embeddings\n",
        "    input_embeddings = model.distilbert.embeddings(input_tensor)\n",
        "    \n",
        "    # Create baseline (padding)\n",
        "    reference_input_ids = torch.tensor(\n",
        "        [tokenizer.pad_token_id] * inputs.shape[1], \n",
        "        device=device\n",
        "    ).unsqueeze(0)\n",
        "    reference_embeddings = model.distilbert.embeddings(reference_input_ids)\n",
        "    \n",
        "    # Initialize Integrated Gradients\n",
        "    integrated_gradients = IntegratedGradients(lambda x: model(inputs_embeds=x).logits)\n",
        "    \n",
        "    # Process each example in batch\n",
        "    attributions_list = []\n",
        "    for i in range(len(inputs)):\n",
        "        target_index = int(targets[i])\n",
        "        \n",
        "        attribution = integrated_gradients.attribute(\n",
        "            inputs=input_embeddings[i].unsqueeze(0),\n",
        "            baselines=reference_embeddings,\n",
        "            target=target_index,\n",
        "            n_steps=20  # Fewer steps for faster computation\n",
        "        )\n",
        "        \n",
        "        # Sum over embedding dimension to get per-token importance\n",
        "        attribution_sum = attribution.sum(dim=-1).squeeze(0).cpu().detach().numpy()\n",
        "        attributions_list.append(attribution_sum)\n",
        "    \n",
        "    return np.array(attributions_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar39KJjKftNJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Find toxic examples in dataset\n",
        "toxic_indices = np.where(labels_array == 1)[0]\n",
        "\n",
        "# Select batch of examples for evaluation\n",
        "batch_size = 16\n",
        "selected_indices = toxic_indices[:batch_size]\n",
        "\n",
        "# Extract input IDs for selected examples\n",
        "input_batch_toxic = [test_subset[int(i)]['input_ids'] for i in selected_indices]\n",
        "target_batch = labels_array[selected_indices]\n",
        "\n",
        "print(f\"Selected {len(input_batch_toxic)} toxic examples for evaluation.\")\n",
        "\n",
        "# Configuration for faithfulness test\n",
        "top_k_tokens = 5  # Number of most important tokens to remove\n",
        "dataset_samples = input_batch_toxic\n",
        "targets = target_batch\n",
        "\n",
        "print(f\"\\n=== Faithfulness Evaluation (Comprehensiveness) ===\")\n",
        "print(f\"Testing on {len(dataset_samples)} examples\")\n",
        "print(f\"Removing {top_k_tokens} most important tokens from each sentence\\n\")\n",
        "\n",
        "comprehensiveness_scores = []\n",
        "\n",
        "# Evaluate each example\n",
        "for i in range(len(dataset_samples)):\n",
        "    # Prepare single input\n",
        "    input_id = dataset_samples[i].unsqueeze(0).to(device)\n",
        "    \n",
        "    # Get original prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        original_output = model(input_id)\n",
        "        original_probability = torch.sigmoid(original_output.logits)[0][0].item()\n",
        "    \n",
        "    # Compute attributions using Integrated Gradients\n",
        "    integrated_gradients = IntegratedGradients(predict_function)\n",
        "    \n",
        "    # Prepare embeddings\n",
        "    input_embedding = model.distilbert.embeddings(input_id)\n",
        "    baseline_embedding = model.distilbert.embeddings(\n",
        "        torch.tensor([tokenizer.pad_token_id] * input_id.size(1), device=device).unsqueeze(0)\n",
        "    )\n",
        "    \n",
        "    # Compute attributions\n",
        "    attribution, _ = integrated_gradients.attribute(\n",
        "        inputs=input_embedding,\n",
        "        baselines=baseline_embedding,\n",
        "        target=0,  # Targeting toxic class\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "    \n",
        "    # Sum attributions to token level\n",
        "    attribution_sum = attribution.sum(dim=-1).squeeze(0)\n",
        "    \n",
        "    # Find top-K most important tokens\n",
        "    _, top_indices = torch.topk(attribution_sum, k=top_k_tokens)\n",
        "    \n",
        "    # Perturb input by masking important tokens\n",
        "    perturbed_input_id = input_id.clone()\n",
        "    perturbed_input_id[0, top_indices] = tokenizer.pad_token_id\n",
        "    \n",
        "    # Get prediction on perturbed input\n",
        "    with torch.no_grad():\n",
        "        perturbed_output = model(perturbed_input_id)\n",
        "        perturbed_probability = torch.sigmoid(perturbed_output.logits)[0][0].item()\n",
        "    \n",
        "    # Compute comprehensiveness score (confidence drop)\n",
        "    confidence_drop = original_probability - perturbed_probability\n",
        "    comprehensiveness_scores.append(confidence_drop)\n",
        "    \n",
        "    # Display first example details\n",
        "    if i == 0:\n",
        "        print(f\"Example 1 - Original confidence: {original_probability:.4f}\")\n",
        "        print(f\"Example 1 - After removing top-{top_k_tokens} tokens: {perturbed_probability:.4f}\")\n",
        "        print(f\"Example 1 - Confidence drop: {confidence_drop:.4f}\")\n",
        "        removed_words = tokenizer.convert_ids_to_tokens(input_id[0, top_indices])\n",
        "        print(f\"Removed tokens: {removed_words}\\n\")\n",
        "\n",
        "# Display final results\n",
        "average_score = np.mean(comprehensiveness_scores)\n",
        "std_score = np.std(comprehensiveness_scores)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(f\"Average Comprehensiveness Score: {average_score:.4f}\")\n",
        "print(f\"Standard Deviation: {std_score:.4f}\")\n",
        "\n",
        "if average_score > 0.1:\n",
        "    print(\"\\n✅ Integrated Gradients works well!\")\n",
        "    print(\"   Removing identified tokens significantly reduces toxicity.\")\n",
        "else:\n",
        "    print(\"\\n❌ Integrated Gradients poorly identifies important tokens.\")\n",
        "    print(\"   Model still detects toxicity after perturbation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz8ZxFLy-j23"
      },
      "outputs": [],
      "source": [
        "from captum.attr import InputXGradient\n",
        "\n",
        "# Initialize Input × Gradient method\n",
        "input_x_gradient = InputXGradient(predict_function)\n",
        "\n",
        "print(f\"Computing attributions using Input×Gradient for: {target_name}...\")\n",
        "\n",
        "# Compute attributions (no baseline needed for Input×Gradient)\n",
        "attributions_ixg = input_x_gradient.attribute(\n",
        "    inputs=input_embeddings,\n",
        "    target=target_label_index,\n",
        "    additional_forward_args=(attention_mask,)\n",
        ")\n",
        "\n",
        "# Process results for visualization\n",
        "attributions_ixg_sum = attributions_ixg.sum(dim=-1).squeeze(0)\n",
        "attributions_ixg_sum = attributions_ixg_sum / torch.norm(attributions_ixg_sum)\n",
        "attributions_ixg_numpy = attributions_ixg_sum.cpu().detach().numpy()\n",
        "\n",
        "# Create visualization data record for Input×Gradient\n",
        "visualization_data_ixg = visualization.VisualizationDataRecord(\n",
        "    word_attributions=attributions_ixg_numpy,\n",
        "    pred_prob=probability_score,\n",
        "    pred_class=predicted_class_label,\n",
        "    true_class=1,\n",
        "    attr_class=f\"{target_name} (Input×Gradient)\",\n",
        "    attr_score=attributions_ixg_numpy.sum(),\n",
        "    raw_input_ids=tokens,\n",
        "    convergence_score=None  # Input×Gradient doesn't compute convergence delta\n",
        ")\n",
        "\n",
        "print(\"\\n=== XAI Methods Comparison ===\")\n",
        "print(\"Row 1: Integrated Gradients\")\n",
        "print(\"Row 2: Input × Gradient\")\n",
        "\n",
        "# Visualize both methods side by side\n",
        "visualization.visualize_text([visualization_data, visualization_data_ixg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Comprehensive Layer Analysis Across All Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQQkWggCADaq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_all_layers(dataset, model, device, batch_size=32):\n",
        "    \"\"\"\n",
        "    Efficiently extracts CLS embeddings from all transformer layers in a single pass.\n",
        "    \n",
        "    Args:\n",
        "        dataset: HuggingFace dataset to extract from\n",
        "        model: Transformer model with output_hidden_states enabled\n",
        "        device: PyTorch device (cuda/cpu)\n",
        "        batch_size: Batch size for processing\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (layers_dict, labels_array)\n",
        "        - layers_dict: Dictionary mapping layer_index -> numpy array of embeddings\n",
        "        - labels_array: Numpy array of labels\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Create DataLoader for efficient batching\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    # Dictionary to store activations per layer\n",
        "    # DistilBERT has: 1 embedding layer + 6 transformer layers = 7 hidden states\n",
        "    layers_data = {}\n",
        "    all_labels = []\n",
        "    \n",
        "    print(f\"Extracting from {len(dataset)} samples...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Layer Extraction\"):\n",
        "            # Move batch to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels']\n",
        "            \n",
        "            # Single forward pass retrieves all hidden states\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "            \n",
        "            # Extract CLS token (index 0) from each layer\n",
        "            for layer_index, hidden_state in enumerate(outputs.hidden_states):\n",
        "                if layer_index not in layers_data:\n",
        "                    layers_data[layer_index] = []\n",
        "                \n",
        "                # Extract CLS embeddings [batch_size, hidden_dim]\n",
        "                cls_embeddings = hidden_state[:, 0, :].cpu().numpy()\n",
        "                layers_data[layer_index].append(cls_embeddings)\n",
        "            \n",
        "            # Extract toxic label (first column)\n",
        "            if labels.dim() > 1:\n",
        "                toxic_labels = labels[:, 0].cpu().numpy()\n",
        "            else:\n",
        "                toxic_labels = labels.cpu().numpy()\n",
        "            \n",
        "            all_labels.extend(toxic_labels)\n",
        "    \n",
        "    # Concatenate all batches\n",
        "    final_layer_activations = {\n",
        "        layer: np.concatenate(data, axis=0)\n",
        "        for layer, data in layers_data.items()\n",
        "    }\n",
        "    final_labels = np.array(all_labels)\n",
        "    \n",
        "    return final_layer_activations, final_labels\n",
        "\n",
        "# Determine analysis subset size\n",
        "evaluation_subset_size = 1000\n",
        "if len(evaluation_dataset) > evaluation_subset_size:\n",
        "    analysis_dataset = evaluation_dataset.select(range(evaluation_subset_size))\n",
        "else:\n",
        "    analysis_dataset = evaluation_dataset\n",
        "\n",
        "# Extract representations from all layers\n",
        "layers_activations_dict, all_labels = extract_all_layers(\n",
        "    analysis_dataset, model, device, batch_size=32\n",
        ")\n",
        "\n",
        "print(f\"\\nExtraction complete. Layers extracted: {list(layers_activations_dict.keys())}\")\n",
        "print(f\"Layer 0 shape: {layers_activations_dict[0].shape}\")\n",
        "\n",
        "# Train probes for each layer\n",
        "probe_results = []\n",
        "\n",
        "print(\"\\nTraining linear probes for each layer...\")\n",
        "\n",
        "for layer_index in sorted(layers_activations_dict.keys()):\n",
        "    X = layers_activations_dict[layer_index]\n",
        "    y = all_labels\n",
        "    \n",
        "    # Binarize labels\n",
        "    y = (y > 0.5).astype(int)\n",
        "    \n",
        "    # Split into train/test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    # Train logistic regression probe\n",
        "    classifier = LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')\n",
        "    classifier.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate\n",
        "    y_pred = classifier.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    \n",
        "    probe_results.append({\n",
        "        'layer': layer_index,\n",
        "        'accuracy': acc,\n",
        "        'f1': f1\n",
        "    })\n",
        "    \n",
        "    print(f\"Layer {layer_index}: Accuracy={acc:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_dataframe = pd.DataFrame(probe_results)\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Plot lines for accuracy and F1\n",
        "sns.lineplot(data=results_dataframe, x='layer', y='accuracy', \n",
        "             marker='o', label='Accuracy', linewidth=2.5)\n",
        "sns.lineplot(data=results_dataframe, x='layer', y='f1', \n",
        "             marker='s', label='F1 Score', linewidth=2.5)\n",
        "\n",
        "# Format plot\n",
        "plt.title(\"Linear Separability of Toxicity Across Layers (DistilBERT)\", fontsize=14, pad=15)\n",
        "plt.xlabel(\"Layer Number (0=Embeddings, 1-6=Transformer Layers)\", fontsize=12)\n",
        "plt.ylabel(\"Metric Value\", fontsize=12)\n",
        "plt.ylim(0.0, 1.05)\n",
        "plt.xticks(results_dataframe['layer'])\n",
        "plt.legend(fontsize=11)\n",
        "\n",
        "# Add value labels above points\n",
        "for index, row in results_dataframe.iterrows():\n",
        "    plt.text(row['layer'], row['accuracy'] + 0.01, f\"{row['accuracy']:.2f}\",\n",
        "             ha='center', color='blue', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Stability Analysis with Paraphrase Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsUmG5UsBrc9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "# Load T5 paraphrase generation model\n",
        "print(\"Loading T5 paraphrase model...\")\n",
        "paraphrase_model_name = \"Vamsi/T5_Paraphrase_Paws\"\n",
        "paraphrase_tokenizer = AutoTokenizer.from_pretrained(paraphrase_model_name)\n",
        "paraphrase_model = AutoModelForSeq2SeqLM.from_pretrained(paraphrase_model_name).to(device)\n",
        "print(\"T5 model loaded!\")\n",
        "\n",
        "def generate_paraphrase(text, num_return_sequences=1):\n",
        "    \"\"\"\n",
        "    Generates paraphrase for given text using T5 model.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text to paraphrase\n",
        "        num_return_sequences: Number of paraphrases to generate\n",
        "        \n",
        "    Returns:\n",
        "        Paraphrased text string\n",
        "    \"\"\"\n",
        "    paraphrase_model.eval()\n",
        "    \n",
        "    # T5 requires task prefix for this specific model\n",
        "    text = \"paraphrase: \" + text + \" </s>\"\n",
        "    \n",
        "    encoding = paraphrase_tokenizer.encode_plus(\n",
        "        text,\n",
        "        padding=\"longest\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    \n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    attention_masks = encoding[\"attention_mask\"].to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = paraphrase_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_masks,\n",
        "            max_length=256,\n",
        "            do_sample=True,  # Sampling allows greater diversity\n",
        "            top_k=120,\n",
        "            top_p=0.95,\n",
        "            early_stopping=True,\n",
        "            num_return_sequences=num_return_sequences\n",
        "        )\n",
        "    \n",
        "    # Decode output\n",
        "    paraphrase = paraphrase_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return paraphrase\n",
        "\n",
        "def get_top_k_tokens(text_input, model, tokenizer, k=5):\n",
        "    \"\"\"\n",
        "    Computes Integrated Gradients attributions and returns set of k most important tokens.\n",
        "    \n",
        "    Args:\n",
        "        text_input: Input text to analyze\n",
        "        model: Classification model\n",
        "        tokenizer: Tokenizer for the model\n",
        "        k: Number of top tokens to return\n",
        "        \n",
        "    Returns:\n",
        "        Set of most important token strings\n",
        "    \"\"\"\n",
        "    # Prepare input for DistilBERT\n",
        "    inputs = tokenizer(text_input, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    input_ids = inputs.input_ids\n",
        "    attention_mask = inputs.attention_mask\n",
        "    \n",
        "    # Define prediction function for IG\n",
        "    def predict_func(inputs_embeds):\n",
        "        out = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "        return out.logits\n",
        "    \n",
        "    integrated_gradients = IntegratedGradients(predict_func)\n",
        "    \n",
        "    # Prepare embeddings\n",
        "    input_embeddings = model.distilbert.embeddings(input_ids)\n",
        "    reference_input_ids = torch.tensor(\n",
        "        [tokenizer.pad_token_id] * input_ids.size(1), \n",
        "        device=device\n",
        "    ).unsqueeze(0)\n",
        "    reference_embeddings = model.distilbert.embeddings(reference_input_ids)\n",
        "    \n",
        "    # Compute attributions (target=0 for 'toxic' class)\n",
        "    target_index = 0\n",
        "    \n",
        "    attributions, _ = integrated_gradients.attribute(\n",
        "        inputs=input_embeddings,\n",
        "        baselines=reference_embeddings,\n",
        "        target=target_index,\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "    \n",
        "    # Sum and select top-K\n",
        "    attribution_sum = attributions.sum(dim=-1).squeeze(0)\n",
        "    _, top_indices = torch.topk(attribution_sum, k=min(k, len(attribution_sum)))\n",
        "    \n",
        "    # Convert IDs to tokens\n",
        "    top_tokens = tokenizer.convert_ids_to_tokens(input_ids[0][top_indices])\n",
        "    \n",
        "    # Clean tokens (remove '##' from subwords and convert to lowercase)\n",
        "    clean_tokens = set([\n",
        "        token.replace(\"##\", \"\").lower() \n",
        "        for token in top_tokens \n",
        "        if token not in ['[CLS]', '[SEP]', '[PAD]']\n",
        "    ])\n",
        "    \n",
        "    return clean_tokens\n",
        "\n",
        "def evaluate_stability(original_text, layer_index, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Computes three stability metrics:\n",
        "    1. Output Stability - How much prediction changes\n",
        "    2. Layer Stability - Cosine similarity of representations\n",
        "    3. Attribution Stability - Jaccard similarity of important tokens\n",
        "    \n",
        "    Args:\n",
        "        original_text: Original input text\n",
        "        layer_index: Which layer to analyze\n",
        "        model: Classification model\n",
        "        tokenizer: Tokenizer for the model\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with stability metrics\n",
        "    \"\"\"\n",
        "    # Generate paraphrase\n",
        "    paraphrase_text = generate_paraphrase(original_text)\n",
        "    \n",
        "    # Prepare both texts\n",
        "    inputs_original = tokenizer(\n",
        "        original_text, return_tensors=\"pt\", truncation=True, max_length=512\n",
        "    ).to(device)\n",
        "    inputs_paraphrase = tokenizer(\n",
        "        paraphrase_text, return_tensors=\"pt\", truncation=True, max_length=512\n",
        "    ).to(device)\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    # Run model with hidden states output\n",
        "    with torch.no_grad():\n",
        "        output_original = model(**inputs_original, output_hidden_states=True)\n",
        "        output_paraphrase = model(**inputs_paraphrase, output_hidden_states=True)\n",
        "    \n",
        "    # A. Output Stability (prediction difference)\n",
        "    probability_original = torch.sigmoid(output_original.logits)[0][0].item()\n",
        "    probability_paraphrase = torch.sigmoid(output_paraphrase.logits)[0][0].item()\n",
        "    prediction_difference = abs(probability_original - probability_paraphrase)\n",
        "    \n",
        "    # B. Layer Stability (cosine similarity of CLS representations)\n",
        "    cls_original = output_original.hidden_states[layer_index][:, 0, :]  # [1, 768]\n",
        "    cls_paraphrase = output_paraphrase.hidden_states[layer_index][:, 0, :]  # [1, 768]\n",
        "    \n",
        "    cosine_similarity = F.cosine_similarity(cls_original, cls_paraphrase).item()\n",
        "    \n",
        "    # C. Attribution Stability (Jaccard Index of top tokens)\n",
        "    tokens_original = get_top_k_tokens(original_text, model, tokenizer, k=5)\n",
        "    tokens_paraphrase = get_top_k_tokens(paraphrase_text, model, tokenizer, k=5)\n",
        "    \n",
        "    # Compute Jaccard Index\n",
        "    intersection = len(tokens_original.intersection(tokens_paraphrase))\n",
        "    union = len(tokens_original.union(tokens_paraphrase))\n",
        "    jaccard_score = intersection / union if union > 0 else 0.0\n",
        "    \n",
        "    return {\n",
        "        \"Original Text\": original_text,\n",
        "        \"Paraphrase\": paraphrase_text,\n",
        "        \"Prob Original\": round(probability_original, 4),\n",
        "        \"Prob Paraphrase\": round(probability_paraphrase, 4),\n",
        "        \"Pred Diff (Output)\": round(prediction_difference, 4),\n",
        "        \"Layer Cosine Sim\": round(cosine_similarity, 4),\n",
        "        \"Attribution Jaccard\": round(jaccard_score, 4),\n",
        "        \"Top Tokens Orig\": list(tokens_original),\n",
        "        \"Top Tokens Para\": list(tokens_paraphrase)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oshWrc9Cjd5"
      },
      "outputs": [],
      "source": [
        "# Find toxic examples for stability testing\n",
        "toxic_indices = [i for i, x in enumerate(y_test_probe) if x == 1][:15]\n",
        "if len(toxic_indices) == 0:\n",
        "    print(\"No toxic samples found, using random selection...\")\n",
        "    toxic_indices = range(10)\n",
        "\n",
        "print(f\"\\nStarting stability analysis for {len(toxic_indices)} examples...\")\n",
        "print(f\"Analyzing layer: 5 (based on previous analysis results)\")\n",
        "\n",
        "stability_results = []\n",
        "\n",
        "# Evaluate stability for each example\n",
        "for idx in toxic_indices:\n",
        "    # Decode tokenized text\n",
        "    input_ids_raw = test_subset[idx]['input_ids']\n",
        "    original_text = tokenizer.decode(input_ids_raw, skip_special_tokens=True)\n",
        "    \n",
        "    # Run stability evaluation on Layer 5\n",
        "    metrics = evaluate_stability(original_text, layer_index=5, model=model, tokenizer=tokenizer)\n",
        "    stability_results.append(metrics)\n",
        "\n",
        "# Create DataFrame with results\n",
        "stability_dataframe = pd.DataFrame(stability_results)\n",
        "\n",
        "# Display results\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "display(stability_dataframe[[\n",
        "    \"Original Text\", \"Paraphrase\",\n",
        "    \"Pred Diff (Output)\", \"Layer Cosine Sim\", \"Attribution Jaccard\"\n",
        "]])\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n=== Stability Summary (Averages) ===\")\n",
        "print(f\"Mean Prediction Stability (Diff): {stability_dataframe['Pred Diff (Output)'].mean():.4f}\")\n",
        "print(f\"  (Lower is better - less variation in predictions)\")\n",
        "print(f\"Mean Layer Stability (Cosine):    {stability_dataframe['Layer Cosine Sim'].mean():.4f}\")\n",
        "print(f\"  (Higher is better - closer to 1.0 indicates more stable representations)\")\n",
        "print(f\"Mean Attribution Stability (Jacc): {stability_dataframe['Attribution Jaccard'].mean():.4f}\")\n",
        "print(f\"  (Higher is better - closer to 1.0 indicates consistent explanations)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Representation Engineering - Building Steering Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TudqH85Dkqm"
      },
      "outputs": [],
      "source": [
        "# Extract steering vector using Difference of Means method\n",
        "\n",
        "# Separate Layer 5 activations by toxic/safe labels\n",
        "layer_5_activations = layers_activations_dict[5]\n",
        "is_toxic = (all_labels > 0.5)  # Boolean mask for toxic examples\n",
        "\n",
        "# Compute centroids for both groups\n",
        "mean_toxic = np.mean(layer_5_activations[is_toxic], axis=0)\n",
        "mean_safe = np.mean(layer_5_activations[~is_toxic], axis=0)\n",
        "\n",
        "# Compute direction vector (from safe to toxic)\n",
        "direction_vector = mean_toxic - mean_safe\n",
        "\n",
        "# Analyze vector properties for debugging\n",
        "vector_norm = np.linalg.norm(direction_vector)\n",
        "hidden_state_norm = np.linalg.norm(mean_safe)\n",
        "\n",
        "print(f\"Steering vector norm (Difference of Means): {vector_norm:.4f}\")\n",
        "print(f\"Average activation norm in model: {hidden_state_norm:.4f}\")\n",
        "print(f\"Ratio: {vector_norm / hidden_state_norm:.4f}\")\n",
        "\n",
        "# Convert to PyTorch tensor (preserve natural scale)\n",
        "steering_tensor = torch.tensor(direction_vector, dtype=torch.float32).to(device)\n",
        "\n",
        "print(\"\\nSteering vector (Mean Difference) ready!\")\n",
        "\n",
        "# Determine optimal alpha scaling based on vector norm\n",
        "scale_factor = 5.0\n",
        "suggested_alpha = scale_factor\n",
        "\n",
        "print(f\"\\nSuggested alpha strength: +/- {suggested_alpha}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_with_steering(text, model, tokenizer, steering_vector, alpha=0):\n",
        "    \"\"\"\n",
        "    Makes prediction with optional steering vector intervention.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text to classify\n",
        "        model: Classification model\n",
        "        tokenizer: Tokenizer for the model\n",
        "        steering_vector: Direction vector for intervention\n",
        "        alpha: Steering strength (negative detoxifies, positive amplifies)\n",
        "        \n",
        "    Returns:\n",
        "        Probability of toxic class\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    \n",
        "    # Define hook for intervention\n",
        "    class SteeringHook:\n",
        "        def __init__(self, vector, coefficient):\n",
        "            self.vector = vector\n",
        "            self.coefficient = coefficient\n",
        "        \n",
        "        def __call__(self, module, inputs, output):\n",
        "            hidden_states = output[0]\n",
        "            steered_states = hidden_states + (self.coefficient * self.vector)\n",
        "            return (steered_states,) + output[1:]\n",
        "    \n",
        "    # Register hook on Layer 5 if alpha != 0\n",
        "    if alpha != 0:\n",
        "        hook_handle = model.distilbert.transformer.layer[5].register_forward_hook(\n",
        "            SteeringHook(steering_vector, alpha)\n",
        "        )\n",
        "    \n",
        "    # Run prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probability = torch.sigmoid(outputs.logits)[0][0].item()\n",
        "    \n",
        "    # Remove hook\n",
        "    if alpha != 0:\n",
        "        hook_handle.remove()\n",
        "    \n",
        "    return probability\n",
        "\n",
        "# Test steering on toxic sentence\n",
        "test_toxic = \"You are a complete idiot and a waste of time.\"\n",
        "\n",
        "score_original = predict_with_steering(test_toxic, model, tokenizer, steering_tensor, alpha=0)\n",
        "score_detoxified = predict_with_steering(test_toxic, model, tokenizer, steering_tensor, alpha=-suggested_alpha)\n",
        "score_amplified = predict_with_steering(test_toxic, model, tokenizer, steering_tensor, alpha=suggested_alpha)\n",
        "\n",
        "print(f\"\\nSentence: {test_toxic}\")\n",
        "print(f\"Original (Alpha 0):       {score_original:.4f}\")\n",
        "print(f\"Detoxified (Alpha -{suggested_alpha}): {score_detoxified:.4f} (expect decrease)\")\n",
        "print(f\"Amplified (Alpha +{suggested_alpha}):  {score_amplified:.4f} (expect increase)\")\n",
        "\n",
        "# Plot steering effects across alpha range\n",
        "alphas = np.linspace(-suggested_alpha * 2, suggested_alpha * 2, 10)\n",
        "scores = [predict_with_steering(test_toxic, model, tokenizer, steering_tensor, alpha=a) for a in alphas]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(alphas, scores, marker='o', color='green', linewidth=2)\n",
        "plt.axhline(0.5, color='gray', linestyle='--', label='Classification Threshold')\n",
        "plt.title(f\"Mean Difference Steering (Layer 5)\\nSentence: '{test_toxic[:30]}...'\")\n",
        "plt.xlabel(\"Alpha (Negative = Detoxification)\")\n",
        "plt.ylabel(\"Toxic Probability\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Demonstrate classification flip\n",
        "print(\"\\n=== Classification Change Demo ===\")\n",
        "base_score = predict_with_steering(test_toxic, model, tokenizer, steering_tensor, alpha=0)\n",
        "detox_score = predict_with_steering(test_toxic, model, tokenizer, steering_tensor, alpha=-15)\n",
        "\n",
        "print(f\"Sentence: {test_toxic}\")\n",
        "print(f\"Original:  {base_score:.4f} (Toxic? {base_score > 0.5})\")\n",
        "print(f\"Steered:   {detox_score:.4f} (Toxic? {detox_score > 0.5})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Side Effects Testing and Safety Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5-sUYzAER7k"
      },
      "outputs": [],
      "source": [
        "# Set optimal alpha based on steering experiments\n",
        "optimal_alpha = -3.0\n",
        "\n",
        "# Test sentences from different categories\n",
        "test_sentences = [\n",
        "    # Toxic sentences (should be detoxified)\n",
        "    (\"You are a complete idiot and a waste of time.\", \"Toxic\"),\n",
        "    (\"I hope you fail miserably, you loser.\", \"Toxic\"),\n",
        "    \n",
        "    # Safe positive sentences (should remain unchanged)\n",
        "    (\"I really love the weather today, it is beautiful.\", \"Safe_Positive\"),\n",
        "    (\"Thank you for your help, I appreciate it.\", \"Safe_Positive\"),\n",
        "    \n",
        "    # Safe negative sentences (important edge case - should not be flagged)\n",
        "    (\"I am feeling very sad and tired today.\", \"Safe_Negative\"),\n",
        "    (\"The movie was boring and too long.\", \"Safe_Negative\"),\n",
        "    (\"I disagree with your opinion regarding the tax policy.\", \"Safe_Negative\")\n",
        "]\n",
        "\n",
        "print(f\"=== Side Effects Test (Alpha = {optimal_alpha}) ===\\n\")\n",
        "print(f\"{'Category':<20} | {'Original':<12} | {'Steered':<12} | {'Status'}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for text, category in test_sentences:\n",
        "    # Prediction without intervention\n",
        "    probability_original = predict_with_steering(text, model, tokenizer, steering_tensor, alpha=0)\n",
        "    \n",
        "    # Prediction with detoxification\n",
        "    probability_steered = predict_with_steering(text, model, tokenizer, steering_tensor, alpha=optimal_alpha)\n",
        "    \n",
        "    # Evaluate results\n",
        "    if category == \"Toxic\":\n",
        "        # For toxic: success if drops below 0.5\n",
        "        if probability_steered < 0.1:\n",
        "            status = \"✅ Fixed\"\n",
        "        elif probability_steered < 0.5:\n",
        "            status = \"⚠️  Improved\"\n",
        "        else:\n",
        "            status = \"❌ Failed\"\n",
        "    else:\n",
        "        # For safe: check that it doesn't break (shouldn't become toxic)\n",
        "        change = abs(probability_original - probability_steered)\n",
        "        if probability_steered > 0.5:\n",
        "            status = \"❌ BROKEN (False Positive)\"\n",
        "        elif change < 0.2:\n",
        "            status = \"✅ Stable\"\n",
        "        else:\n",
        "            status = \"⚠️  Shifted\"\n",
        "    \n",
        "    print(f\"{category:<20} | {probability_original:.4f}       | {probability_steered:.4f}       | {status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Production Deployment - Saving and Loading Steering Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "# Create steering artifact for production use\n",
        "steering_artifact = {\n",
        "    \"steering_vector\": steering_tensor.cpu(),  # Move to CPU for saving\n",
        "    \"layer_index\": 5,                          # Target layer\n",
        "    \"alpha\": optimal_alpha,                    # Optimal steering strength (-3.0)\n",
        "    \"method\": \"mean_difference\",\n",
        "    \"model_name\": \"distilbert-base-uncased\",\n",
        "    \"description\": \"Vector for removing toxicity concept from layer 5\"\n",
        "}\n",
        "\n",
        "# Save with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "save_path = f\"/drive/MyDrive/msc-project/vectors/toxicity_steering_controller_{timestamp}.pt\"\n",
        "\n",
        "torch.save(steering_artifact, save_path)\n",
        "print(f\"✅ Steering artifact saved to: {save_path}\")\n",
        "\n",
        "# ============================================================\n",
        "# PRODUCTION SIMULATION - Clean session without training data\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== Production Environment Simulation ===\")\n",
        "\n",
        "# Load artifact\n",
        "artifact = torch.load(save_path)\n",
        "loaded_vector = artifact[\"steering_vector\"].to(device)\n",
        "loaded_layer = artifact[\"layer_index\"]\n",
        "loaded_alpha = artifact[\"alpha\"]\n",
        "\n",
        "print(f\"Loaded controller: {artifact['description']}\")\n",
        "print(f\"Configuration: Layer {loaded_layer}, Alpha {loaded_alpha}\")\n",
        "\n",
        "# Define production hook class\n",
        "class ProductionSteeringHook:\n",
        "    \"\"\"Hook for applying steering vector in production inference.\"\"\"\n",
        "    def __init__(self, vector, coefficient):\n",
        "        self.vector = vector\n",
        "        self.coefficient = coefficient\n",
        "    \n",
        "    def __call__(self, module, inputs, output):\n",
        "        hidden_states = output[0]\n",
        "        steered_states = hidden_states + (self.coefficient * self.vector)\n",
        "        return (steered_states,) + output[1:]\n",
        "\n",
        "# Production inference function\n",
        "def generate_safe_prediction(text, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Run inference with built-in detoxification.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text to classify\n",
        "        model: Classification model\n",
        "        tokenizer: Tokenizer for the model\n",
        "        \n",
        "    Returns:\n",
        "        Toxic probability after steering\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    \n",
        "    # Register steering hook\n",
        "    hook = model.distilbert.transformer.layer[loaded_layer].register_forward_hook(\n",
        "        ProductionSteeringHook(loaded_vector, loaded_alpha)\n",
        "    )\n",
        "    \n",
        "    # Run prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probability = torch.sigmoid(outputs.logits)[0][0].item()\n",
        "    \n",
        "    # Clean up hook\n",
        "    hook.remove()\n",
        "    \n",
        "    return probability\n",
        "\n",
        "# Live test\n",
        "live_test_text = \"You are completely useless and stupid.\"\n",
        "safety_score = generate_safe_prediction(live_test_text, model, tokenizer)\n",
        "\n",
        "print(f\"\\nLive Test Input: '{live_test_text}'\")\n",
        "print(f\"Model Toxic Probability (Steered): {safety_score:.4f}\")\n",
        "print(f\"Decision: {'🔴 BLOCK' if safety_score > 0.5 else '🟢 ALLOW'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment Summary: Representation Engineering for Model Detoxification\n",
        "\n",
        "This project demonstrates comprehensive analysis and modification of internal representations in a DistilBERT model (fine-tuned on Jigsaw Toxicity) to control its behavior without retraining.\n",
        "\n",
        "### Completed Stages:\n",
        "\n",
        "#### 1. Layer-wise Analysis\n",
        "- Investigated linear separability of the \"toxicity\" concept throughout the network\n",
        "- Identified **Layer 5** as the optimal intervention point (F1 Score = 0.80)\n",
        "- This layer surpassed the final layer in representation quality, indicating the \"sweet spot\" for semantic understanding\n",
        "\n",
        "#### 2. Stability Analysis\n",
        "- Used T5 paraphrase generator to test representation robustness\n",
        "- Demonstrated that Layer 5 activations are extremely semantically stable (Cosine Similarity > 0.99)\n",
        "- Confirmed that representations remain consistent even when sentence structure changes, validating intervention feasibility\n",
        "\n",
        "#### 3. Steering Vector Extraction\n",
        "- Applied **Difference of Means** method to compute directional vector between toxic and safe activation centroids in Layer 5\n",
        "- This approach proved more effective than logistic regression weights, providing appropriate signal scaling\n",
        "- Successfully captured the toxicity concept direction in representation space\n",
        "\n",
        "#### 4. Model Steering and Intervention\n",
        "- Implemented PyTorch Forward Hook mechanism for real-time steering vector injection\n",
        "- Applied intervention with strength Alpha = -3.0 for effective model \"detoxification\"\n",
        "- Achieved controllable reduction in toxicity detection without model retraining\n",
        "\n",
        "#### 5. Evaluation and Quality Assurance\n",
        "\n",
        "**Effectiveness:**\n",
        "- Toxicity probability for offensive phrases dropped from ~92% to ~1-4%\n",
        "\n",
        "**Safety:**\n",
        "- Model maintained correct behavior for neutral and positive sentences (no \"lobotomy\" effect)\n",
        "\n",
        "**False Positive Reduction:**\n",
        "- Eliminated incorrect flagging of negative sentiment sentences (e.g., complaints) as toxic\n",
        "- False positive rate dropped from 10% to 0%\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This project confirms that **Representation Engineering (RepE)** is a powerful, low-cost method for controlling LLM/BERT model behavior. Through precise operations on activation vectors in Layer 5, we successfully eliminated undesired model behavior (toxicity detection) while preserving its general linguistic capabilities.\n",
        "\n",
        "The approach demonstrates that internal representation manipulation offers a viable alternative to expensive retraining, enabling fine-grained control over model outputs through targeted interventions in the activation space."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPsX7S8R1dubpjAsshSJLte",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
