{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPsX7S8R1dubpjAsshSJLte",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-project/blob/main/test_notebooks/msc_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N9kkURRXxcaG"
      },
      "outputs": [],
      "source": [
        "!uv pip install --upgrade transformers datasets captum quantus accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "metadata": {
        "id": "rnzL_6SCy5qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7bbe3ea"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_path = '/drive/MyDrive/msc-project/jigsaw-toxic-comment/train.csv'\n",
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(\"CSV file loaded successfully!\")\n",
        "    display(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file was not found at {csv_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(example):\n",
        "    \"\"\"Applies all cleaning steps to the 'comment_text' field.\"\"\"\n",
        "\n",
        "    # 1. Get the text\n",
        "    text = example['comment_text']\n",
        "\n",
        "    # 2. Lowercasing\n",
        "    # This is crucial for \"uncased\" BERT models\n",
        "    text = text.lower()\n",
        "\n",
        "    # 3. Remove URLs\n",
        "    # re.sub finds a pattern and replaces it\n",
        "    # r'http\\S+' finds 'http' followed by any non-space characters\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # 4. Remove IP Addresses\n",
        "    # \\d{1,3} means \"a digit, 1-to-3 times\". \\. means \"a literal dot\".\n",
        "    text = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', '', text)\n",
        "\n",
        "    # 5. Remove Wikipedia metadata like (talk), timestamps, etc.\n",
        "    # This is a simple regex to find things like (talk)\n",
        "    # You could make this more complex, but this is a good start.\n",
        "    text = re.sub(r'\\(talk\\)', '', text)\n",
        "    text = re.sub(r'\\d{2}:\\d{2}, \\w+ \\d{1,2}, \\d{4} \\(utc\\)', '', text)\n",
        "\n",
        "    # 6. Remove newlines and other special characters\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = text.replace('\\xa0', ' ')\n",
        "\n",
        "    # 7. Remove any text inside double quotes at the start/end\n",
        "    # This removes things like '\"\\n\\n ' from the beginning\n",
        "    text = text.strip(' \"')\n",
        "\n",
        "    # 8. Clean up whitespace\n",
        "    # \\s+ means \"one or more space characters\"\n",
        "    # We replace any group of spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # 9. Update the example\n",
        "    example['comment_text'] = text\n",
        "    return example"
      ],
      "metadata": {
        "id": "Baob8jOTYaBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "train_df = df.head(2000)\n",
        "data = datasets.Dataset.from_pandas(train_df)"
      ],
      "metadata": {
        "id": "XMcBMwOZZOYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCleaning data...\")\n",
        "cleaned_data = data.map(clean_text)\n",
        "print(\"Data cleaned!\")"
      ],
      "metadata": {
        "id": "1XSauFnTY4GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- BEFORE CLEANING ---\")\n",
        "print(data[1]['comment_text'])\n",
        "print(\"\\n\" + data[6]['comment_text'])\n",
        "print(\"\\n\" + data[0]['comment_text'])\n",
        "\n",
        "print(\"\\n\\n--- AFTER CLEANING ---\")\n",
        "print(cleaned_data[1]['comment_text'])\n",
        "print(\"\\n\" + cleaned_data[6]['comment_text'])\n",
        "print(\"\\n\" + cleaned_data[0]['comment_text'])"
      ],
      "metadata": {
        "id": "3Eij-Oi0ZTkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# \"model card\"\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "\n",
        "try:\n",
        "    # Download and cache the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    print(\"Tokenizer loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")"
      ],
      "metadata": {
        "id": "IdmXlYxUczCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Applies the tokenizer to a batch of text.\"\"\"\n",
        "\n",
        "    # The main tokenization step.\n",
        "    # padding=\"max_length\" fills short comments with [PAD] tokens.\n",
        "    # truncation=True cuts off comments that are too long.\n",
        "    # max_length=256 is a good balance of speed and context for comments.\n",
        "    # Could use 512 (DistilBERT's max) but it's slower.\n",
        "    return tokenizer(\n",
        "        examples[\"comment_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "# Apply the function with .map()\n",
        "# batched=True makes it MUCH faster by tokenizing many texts at once.\n",
        "print(\"\\nTokenizing data...\")\n",
        "tokenized_data = cleaned_data.map(tokenize_function, batched=True)\n",
        "print(\"Data tokenized!\")"
      ],
      "metadata": {
        "id": "7di3UMZHdB1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Example of a Tokenized Entry ---\")\n",
        "print(tokenized_data[0])"
      ],
      "metadata": {
        "id": "Mn-sppLedLSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Define label columns in the correct order\n",
        "label_columns = [\n",
        "    'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'\n",
        "]\n",
        "\n",
        "def create_labels_column(example):\n",
        "    \"\"\"\n",
        "    Creates a new 'labels' column by combining the 6 label columns.\n",
        "    We convert them to float32, which is what ML models expect.\n",
        "    \"\"\"\n",
        "    # For each example, build a list of its label values\n",
        "    labels_list = [float(example[col]) for col in label_columns]\n",
        "    example['labels'] = labels_list\n",
        "    return example\n",
        "\n",
        "# 2. Apply the function\n",
        "print(\"\\nConsolidating labels...\")\n",
        "final_data = tokenized_data.map(create_labels_column)\n",
        "print(\"Labels consolidated!\")\n",
        "\n",
        "# 3. Let's see the result for a toxic comment\n",
        "print(\"\\n--- Example of a Processed Entry ---\")\n",
        "print(final_data[6])"
      ],
      "metadata": {
        "id": "c8F9I2V2d-I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. List all columns to be removed\n",
        "columns_to_remove = [\n",
        "    'id', 'comment_text', 'toxic', 'severe_toxic',\n",
        "    'obscene', 'threat', 'insult', 'identity_hate'\n",
        "]\n",
        "\n",
        "print(f\"\\nOriginal columns: {final_data.column_names}\")\n",
        "final_data = final_data.remove_columns(columns_to_remove)\n",
        "print(f\"Cleaned columns: {final_data.column_names}\")\n",
        "\n",
        "# 2. Set the dataset format to \"torch\" (for PyTorch)\n",
        "try:\n",
        "    final_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "    print(\"\\nDataset format set to 'torch'!\")\n",
        "except ImportError:\n",
        "    print(\"\\nPyTorch not installed. Skipping .set_format('torch').\")\n",
        "    print(\"Please install with: pip install torch\")\n",
        "\n",
        "print(\"\\n--- Final, Model-Ready Item ---\")\n",
        "print(final_data[6])"
      ],
      "metadata": {
        "id": "GdvRwT_leOM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "num_labels = 6 # 6 toxic categories\n",
        "\n",
        "# Load the model, configuring it for multi-label classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(\"Model configured for multi-label classification.\")"
      ],
      "metadata": {
        "id": "RY2kk5asekT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_splits = final_data.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "train_dataset = data_splits['train']\n",
        "eval_dataset = data_splits['test']\n",
        "\n",
        "print(f\"\\nData split complete:\")\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Evaluation samples: {len(eval_dataset)}\")"
      ],
      "metadata": {
        "id": "7EUdT7kYeyof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "import torch\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    # p.predictions are the raw logit outputs\n",
        "    # p.label_ids are the true labels\n",
        "\n",
        "    # Apply sigmoid to logits to get probabilities\n",
        "    logits = p.predictions\n",
        "    # Sigmoid function\n",
        "    probs = 1 / (1 + np.exp(-logits))\n",
        "\n",
        "    # Set a threshold (0.5) to get binary predictions\n",
        "    threshold = 0.5\n",
        "    predictions = (probs > threshold).astype(int)\n",
        "\n",
        "    # Compute the metrics\n",
        "    labels = p.label_ids\n",
        "\n",
        "    # Use 'micro' averaging, which is good for imbalanced labels\n",
        "    f1_micro = f1_score(labels, predictions, average='micro')\n",
        "\n",
        "    # This measures how many individual labels (out of 6*num_samples) were correct\n",
        "    overall_accuracy = accuracy_score(labels.flatten(), predictions.flatten())\n",
        "\n",
        "    # Return metrics as a dictionary\n",
        "    return {\n",
        "        'f1_micro': f1_micro,\n",
        "        'accuracy': overall_accuracy\n",
        "    }"
      ],
      "metadata": {
        "id": "NX52g5ElfBO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "model_output_dir = \"/drive/MyDrive/msc-project/models/distilbert-jigsaw-finetuned\"\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_output_dir,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    # helps prevent overfitting\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=5,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_micro\",\n",
        "    # DISABLE WANDB\n",
        "    report_to=\"none\",\n",
        ")"
      ],
      "metadata": {
        "id": "Gd4r8HXrfO8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"\\n--- Starting Training ---\")\n",
        "trainer.train()\n",
        "print(\"--- Training Complete ---\")"
      ],
      "metadata": {
        "id": "hgN5hlo3fowE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "base_path = \"/drive/MyDrive/msc-project/models/final_distilbert_jigsaw\"\n",
        "save_directory = f\"{base_path}_{timestamp}\"\n",
        "\n",
        "trainer.save_model(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "print(f\"Model and tokenizer saved in: {save_directory}\")"
      ],
      "metadata": {
        "id": "BlksX5w2b_-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "text = \"you are a fucking moron, who should die in hell but I love your lovely kitten\"\n",
        "\n",
        "# Tokenization\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "# Inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    # Use SIGMOID dla multi-label\n",
        "    probs = torch.sigmoid(logits)\n",
        "\n",
        "# Display results\n",
        "labels_list = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "print(f\"Text: '{text}'\\n\")\n",
        "print(\"Probabilities:\")\n",
        "for label, prob in zip(labels_list, probs[0]):\n",
        "    print(f\"{label}: {prob:.4f}\")"
      ],
      "metadata": {
        "id": "bjKLCf6bcgqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "# 1. Captum wrapper\n",
        "def predict_func(inputs_embeds, attention_mask=None):\n",
        "    output = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "    return output.logits\n",
        "\n",
        "# 2. Simple Integrated Gradients init\n",
        "ig = IntegratedGradients(predict_func)\n",
        "\n",
        "# 3. Label selection\n",
        "# 0=toxic, 1=severe_toxic, 2=obscene, 3=threat, 4=insult, 5=identity_hate\n",
        "TARGET_LABEL_INDEX = 0\n",
        "target_name = labels_list[TARGET_LABEL_INDEX]\n",
        "\n",
        "# A. Text vectors\n",
        "input_ids = inputs.input_ids\n",
        "# Take vectors (floats) from embedding layer\n",
        "input_embeddings = model.distilbert.embeddings(input_ids)\n",
        "\n",
        "# B. Background vectors (Baseline - padding)\n",
        "# Create tensor ID padding with the same length as input\n",
        "ref_input_ids = torch.tensor([tokenizer.pad_token_id] * input_ids.size(1), device=device).unsqueeze(0)\n",
        "# Change to vectors\n",
        "ref_input_embeddings = model.distilbert.embeddings(ref_input_ids)\n",
        "\n",
        "# C. Attention mask (model must know what is padding)\n",
        "attention_mask = inputs.attention_mask\n",
        "\n",
        "# 5. Attribution calculation\n",
        "print(f\"Attribution calculation: {target_name}...\")\n",
        "\n",
        "attributions, delta = ig.attribute(\n",
        "    inputs=input_embeddings,         # Pass prepared vectors\n",
        "    baselines=ref_input_embeddings,  # Pass background vectors\n",
        "    target=TARGET_LABEL_INDEX,\n",
        "    additional_forward_args=(attention_mask,), # Pass attention mask\n",
        "    return_convergence_delta=True\n",
        ")"
      ],
      "metadata": {
        "id": "UVYwRbBVdZB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import visualization\n",
        "\n",
        "# Results processing for visualisation\n",
        "attributions_sum = attributions.sum(dim=-1).squeeze(0)\n",
        "attributions_sum = attributions_sum / torch.norm(attributions_sum)\n",
        "attributions_np = attributions_sum.cpu().detach().numpy()\n",
        "\n",
        "# Get probability for given label\n",
        "prob_score = probs[0][TARGET_LABEL_INDEX].item()\n",
        "pred_class_label = \"True\" if prob_score > 0.5 else \"False\"\n",
        "\n",
        "# Get tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "vis_data = visualization.VisualizationDataRecord(\n",
        "    word_attributions=attributions_np,\n",
        "    pred_prob=prob_score,       # Label probability\n",
        "    pred_class=pred_class_label, # Did it pass the threshold?\n",
        "    true_class=1,               # Assume that text is toxic\n",
        "    attr_class=target_name,     # Label name (np. 'toxic')\n",
        "    attr_score=attributions_np.sum(),\n",
        "    raw_input_ids=tokens,\n",
        "    convergence_score=delta\n",
        ")\n",
        "\n",
        "print(f\"\\nLabel explaination: {target_name}\")\n",
        "visualization.visualize_text([vis_data])"
      ],
      "metadata": {
        "id": "k0ur6jyFd7E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Model congi\n",
        "model.config.output_hidden_states = True\n",
        "\n",
        "# 2. Extraction function\n",
        "def extract_hidden_states(data_subset, layer_index=4):\n",
        "    model.eval()\n",
        "    all_hidden_states = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(f\"Extract data from layer: {layer_index}...\")\n",
        "\n",
        "    for i in tqdm(range(len(data_subset))):\n",
        "        entry = data_subset[i]\n",
        "\n",
        "        text = entry['input_ids'].unsqueeze(0).to(device)\n",
        "        mask = entry['attention_mask'].unsqueeze(0).to(device)\n",
        "        label = entry['labels'][0].item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(text, attention_mask=mask)\n",
        "            hidden_state = outputs.hidden_states[layer_index]\n",
        "            cls_embedding = hidden_state[0, 0, :].cpu().numpy()\n",
        "\n",
        "            all_hidden_states.append(cls_embedding)\n",
        "            all_labels.append(label)\n",
        "\n",
        "    return np.array(all_hidden_states), np.array(all_labels)\n",
        "\n",
        "# Check data size and take max\n",
        "total_eval_samples = len(eval_dataset)\n",
        "target_size = 500\n",
        "subset_size = min(target_size, total_eval_samples)\n",
        "\n",
        "print(f\"Dostępnych próbek: {total_eval_samples}. Używam: {subset_size}\")\n",
        "\n",
        "test_subset = eval_dataset.select(range(subset_size))\n",
        "\n",
        "# Extraction\n",
        "X_hidden, y_labels = extract_hidden_states(test_subset, layer_index=4)\n",
        "\n",
        "print(f\"\\nKształt danych X: {X_hidden.shape}\")\n",
        "print(f\"Kształt danych y: {y_labels.shape}\")"
      ],
      "metadata": {
        "id": "exduABBQeocg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# 1. Split extracted data for training and test sets\n",
        "X_train_probe, X_test_probe, y_train_probe, y_test_probe = train_test_split(\n",
        "    X_hidden, y_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 2. Create and test simple probe\n",
        "# Increase the max_iter to make it happen\n",
        "probe = LogisticRegression(max_iter=1000)\n",
        "probe.fit(X_train_probe, y_train_probe)\n",
        "\n",
        "# 3. Check how probe sees the toxicity in the layer\n",
        "y_pred_probe = probe.predict(X_test_probe)\n",
        "\n",
        "acc = accuracy_score(y_test_probe, y_pred_probe)\n",
        "f1 = f1_score(y_test_probe, y_pred_probe)\n",
        "\n",
        "print(f\"--- Probe results (Layer 4) ---\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Interpretacja\n",
        "if acc > 0.80:\n",
        "    print(\"Layer 4 has strong representation of toxicity\")\n",
        "else:\n",
        "    print(\"Layer 4 does not have strong representation of toxicity\")"
      ],
      "metadata": {
        "id": "1GAOZapZe5tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Take CAV vector from trained probe\n",
        "# Logistic regression weights [1, 768]\n",
        "cav_vector = probe.coef_[0]\n",
        "intercept = probe.intercept_[0]\n",
        "\n",
        "# 2. We project the data onto this vector (dot product)\n",
        "# This will tell us how much each sentence lies “along” the direction of toxicity\n",
        "# We multiply the representation matrix (X_test_probe) by the CAV vector\n",
        "projected_scores = np.dot(X_test_probe, cav_vector) + intercept\n",
        "\n",
        "# 3. Preparing the data for the plot\n",
        "# We split the results into the toxic group (1) and the safe group (0) based on the true labels\n",
        "scores_toxic = projected_scores[y_test_probe == 1]\n",
        "scores_safe = projected_scores[y_test_probe == 0]\n",
        "\n",
        "# 4. Histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "sns.histplot(scores_safe, color=\"green\", label=\"Non-Toxic\", kde=True, alpha=0.5)\n",
        "\n",
        "sns.histplot(scores_toxic, color=\"red\", label=\"Toxic\", kde=True, alpha=0.5)\n",
        "\n",
        "plt.axvline(0, color='black', linestyle='--', label=\"Decision Boundary (Probe)\")\n",
        "plt.title(f\"Distribution of activations along the CAV vector (Layer 4)\\nAccuracy: {acc:.2f}, F1: {f1:.2f}\")\n",
        "plt.xlabel(\"Projection score (The further to the right, the more 'toxic' according to the layer)\")\n",
        "plt.ylabel(\"Number of examples\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e6x3w5-0fO3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import quantus\n",
        "import numpy as np\n",
        "\n",
        "# 1. Prediction function for Quantus\n",
        "# Quantus provides data as a numpy array, so we need to convert it into Tensors\n",
        "def model_predict_numpy(model, inputs, **kwargs):\n",
        "    model.eval()\n",
        "    # 'inputs' here is a matrix of token IDs [batch_size, seq_len]\n",
        "    input_tensor = torch.tensor(inputs, device=device).long()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        # We return probabilities (Softmax/Sigmoid) as numpy\n",
        "        return torch.sigmoid(outputs.logits).cpu().numpy()\n",
        "\n",
        "# 2. Explanation function for Quantus (Integrated Gradients)\n",
        "def explain_func_numpy(model, inputs, targets, **kwargs):\n",
        "    # Wrapper that runs your IG code inside Quantus\n",
        "    model.eval()\n",
        "    input_tensor = torch.tensor(inputs, device=device).long()\n",
        "\n",
        "    # Create embeddings (as we fixed earlier)\n",
        "    input_embeddings = model.distilbert.embeddings(input_tensor)\n",
        "\n",
        "    # Baseline (padding)\n",
        "    ref_input_ids = torch.tensor([tokenizer.pad_token_id] * inputs.shape[1], device=device).unsqueeze(0)\n",
        "    ref_input_embeddings = model.distilbert.embeddings(ref_input_ids)\n",
        "\n",
        "    # IG\n",
        "    ig = IntegratedGradients(lambda x: model(inputs_embeds=x).logits)\n",
        "\n",
        "    # Important: loop over the batch (Quantus sometimes provides multiple examples at once)\n",
        "    attributions_list = []\n",
        "    for i in range(len(inputs)):\n",
        "        # Target (which class?)\n",
        "        target_idx = int(targets[i])\n",
        "\n",
        "        attr = ig.attribute(\n",
        "            inputs=input_embeddings[i].unsqueeze(0),\n",
        "            baselines=ref_input_embeddings,\n",
        "            target=target_idx,\n",
        "            n_steps=20 # Fewer steps for faster testing\n",
        "        )\n",
        "        # Sum attributions into a single value per token\n",
        "        attr_sum = attr.sum(dim=-1).squeeze(0).cpu().detach().numpy()\n",
        "        attributions_list.append(attr_sum)\n",
        "\n",
        "    return np.array(attributions_list)"
      ],
      "metadata": {
        "id": "AGCEVlF6fqPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# --- PREPARE DATA ---\n",
        "# We need to filter the dataset to find examples that are actually toxic (label=1)\n",
        "\n",
        "# Find indices where the label is 1 (Toxic)\n",
        "toxic_indices = np.where(y_labels == 1)[0]\n",
        "\n",
        "# Select up to 16 examples (or fewer if we don't have 16)\n",
        "batch_size = 16\n",
        "selected_indices = toxic_indices[:batch_size]\n",
        "\n",
        "# Extract the Input IDs for these specific indices\n",
        "x_batch_toxic = [test_subset[int(i)]['input_ids'] for i in selected_indices]\n",
        "y_batch_targets = y_labels[selected_indices]\n",
        "\n",
        "print(f\"Selected {len(x_batch_toxic)} toxic examples for evaluation.\")\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TOP_K_TOKENS = 5   # How many most important words do we remove?\n",
        "dataset_samples = x_batch_toxic  # We take our 16 toxic sentences\n",
        "targets = y_batch_targets        # Our labels\n",
        "\n",
        "print(f\"--- Manual Faithfulness Evaluation (Comprehensiveness) ---\")\n",
        "print(f\"Test on {len(dataset_samples)} examples.\")\n",
        "print(f\"Removing {TOP_K_TOKENS} most important words from each sentence.\\n\")\n",
        "\n",
        "scores = []\n",
        "\n",
        "# Loop over each example\n",
        "for i in range(len(dataset_samples)):\n",
        "    # 1. Prepare a single input\n",
        "    input_id = dataset_samples[i].unsqueeze(0).to(device) # Shape becomes [1, seq_len]\n",
        "\n",
        "    # 2. Original prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        orig_output = model(input_id)\n",
        "        orig_prob = torch.sigmoid(orig_output.logits)[0][0].item()  # Probability of class 'Toxic'\n",
        "\n",
        "    # 3. Compute attributions (IG) for this example\n",
        "    # (Using your existing IG object; assuming 'ig' is defined earlier)\n",
        "    # If not, uncomment the line below:\n",
        "    ig = IntegratedGradients(predict_func)\n",
        "\n",
        "    # Prepare embeddings\n",
        "    input_emb = model.distilbert.embeddings(input_id)\n",
        "    baseline_emb = model.distilbert.embeddings(\n",
        "        torch.tensor([tokenizer.pad_token_id] * input_id.size(1), device=device).unsqueeze(0)\n",
        "    )\n",
        "\n",
        "    # Compute attributions\n",
        "    attributions, _ = ig.attribute(\n",
        "        inputs=input_emb,\n",
        "        baselines=baseline_emb,\n",
        "        target=0,  # Targeting the Toxic class\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "\n",
        "    # Sum attributions to token level\n",
        "    attr_sum = attributions.sum(dim=-1).squeeze(0)  # [seq_len]\n",
        "\n",
        "    # 4. Find TOP-K most important tokens\n",
        "    # torch.topk returns values and indices\n",
        "    _, top_indices = torch.topk(attr_sum, k=TOP_K_TOKENS)\n",
        "\n",
        "    # 5. PERTURBATION (Remove words)\n",
        "    # Copy the input and replace important words with padding (or mask)\n",
        "    perturbed_input_id = input_id.clone()\n",
        "    # Insert PAD (id: 0) in the positions of the most important words\n",
        "    perturbed_input_id[0, top_indices] = tokenizer.pad_token_id\n",
        "\n",
        "    # 6. New prediction on the \"censored\" text\n",
        "    with torch.no_grad():\n",
        "        pert_output = model(perturbed_input_id)\n",
        "        pert_prob = torch.sigmoid(pert_output.logits)[0][0].item()\n",
        "\n",
        "    # 7. Compute the score (Comprehensiveness)\n",
        "    # How much did the model confidence drop?\n",
        "    drop = orig_prob - pert_prob\n",
        "    scores.append(drop)\n",
        "\n",
        "    # Optional: print preview for the first element\n",
        "    if i == 0:\n",
        "        print(f\"Example 1 - Original confidence: {orig_prob:.4f}\")\n",
        "        print(f\"Example 1 - After removing top-{TOP_K_TOKENS} words: {pert_prob:.4f}\")\n",
        "        print(f\"Example 1 - Drop (Score): {drop:.4f}\")\n",
        "        removed_words = tokenizer.convert_ids_to_tokens(input_id[0, top_indices])\n",
        "        print(f\"Removed words: {removed_words}\\n\")\n",
        "\n",
        "# --- FINAL RESULTS ---\n",
        "avg_score = np.mean(scores)\n",
        "std_score = np.std(scores)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Average Comprehensiveness score: {avg_score:.4f}\")\n",
        "print(f\"Standard deviation: {std_score:.4f}\")\n",
        "\n",
        "if avg_score > 0.1:\n",
        "    print(\"\\n✅ CONCLUSION: IG works! Removing the identified words significantly reduces toxicity.\")\n",
        "else:\n",
        "    print(\"\\n❌ CONCLUSION: IG poorly identifies important words (the model still thinks it's toxic).\")\n"
      ],
      "metadata": {
        "id": "ar39KJjKftNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import InputXGradient\n",
        "\n",
        "# 1. Inicjalizacja InputXGradient\n",
        "# Używamy tej samej funkcji 'predict_func', która została zdefiniowana przy Integrated Gradients\n",
        "ixg = InputXGradient(predict_func)\n",
        "\n",
        "print(f\"Obliczanie atrybucji metodą Input X Gradient dla klasy: {target_name}...\")\n",
        "\n",
        "# 2. Wykonanie atrybucji (InputXGradient)\n",
        "# WAŻNE: InputXGradient oblicza gradient * wejście. Nie wymaga baseline'u (tła).\n",
        "attributions_ixg = ixg.attribute(\n",
        "    inputs=input_embeddings,                   # Te same embeddingi wejściowe co przy IG\n",
        "    target=TARGET_LABEL_INDEX,                 # Ta sama etykieta docelowa\n",
        "    additional_forward_args=(attention_mask,)  # Przekazujemy maskę uwagi, aby model działał poprawnie\n",
        ")\n",
        "\n",
        "# 3. Przetwarzanie wyników\n",
        "# Wynik ma kształt [batch, seq_len, hidden_dim]. Sumujemy po ostatnim wymiarze (hidden_dim),\n",
        "# aby uzyskać jedną wartość ważności dla każdego tokenu.\n",
        "attributions_ixg_sum = attributions_ixg.sum(dim=-1).squeeze(0)\n",
        "\n",
        "# Normalizacja (norma Euklidesowa) - identyczna jak w Twoim kodzie dla IG,\n",
        "# co pozwala na uczciwe porównanie \"siły\" atrybucji.\n",
        "attributions_ixg_sum = attributions_ixg_sum / torch.norm(attributions_ixg_sum)\n",
        "attributions_ixg_np = attributions_ixg_sum.cpu().detach().numpy()\n",
        "\n",
        "# 4. Wizualizacja i Porównanie\n",
        "# Tworzymy rekord wizualizacji specyficzny dla Input X Gradient\n",
        "vis_data_ixg = visualization.VisualizationDataRecord(\n",
        "    word_attributions=attributions_ixg_np,\n",
        "    pred_prob=prob_score,           # Korzystamy z obliczonego wcześniej prawdopodobieństwa\n",
        "    pred_class=pred_class_label,    # Etykieta (True/False)\n",
        "    true_class=1,                   # Zakładana prawdziwa klasa (Toxic)\n",
        "    attr_class=f\"{target_name} (InpxGrad)\", # Zmieniamy nazwę, aby odróżnić od IG\n",
        "    attr_score=attributions_ixg_np.sum(),\n",
        "    raw_input_ids=tokens,           # Te same tokeny\n",
        "    convergence_score=None          # InputXGradient nie oblicza błędu konwergencji (delta)\n",
        ")\n",
        "\n",
        "print(\"\\n--- Porównanie wizualne metod XAI ---\")\n",
        "print(\"1. Wiersz: Integrated Gradients (IG)\")\n",
        "print(\"2. Wiersz: Input X Gradient (Baseline)\")\n",
        "\n",
        "# Funkcja visualize_text przyjmuje listę rekordów.\n",
        "# Przekazujemy 'vis_data' (z poprzedniej komórki - IG) oraz 'vis_data_ixg' (nowy wynik).\n",
        "visualization.visualize_text([vis_data, vis_data_ixg])"
      ],
      "metadata": {
        "id": "Mz8ZxFLy-j23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- KROK 1: Wydajna ekstrakcja aktywacji ze wszystkich warstw ---\n",
        "\n",
        "def extract_all_layers(dataset, model, device, batch_size=32):\n",
        "    \"\"\"\n",
        "    Ekstrahuje embeddingi [CLS] ze wszystkich warstw modelu w jednym przebiegu.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # DataLoader przyspiesza proces dzięki batchowaniu (zamiast pojedynczych próbek)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Słownik do przechowywania list aktywacji dla każdej warstwy\n",
        "    # DistilBERT ma zazwyczaj: 1 warstwę embeddingów + 6 warstw transformera = 7 stanów\n",
        "    layers_data = {}\n",
        "    all_labels = []\n",
        "\n",
        "    print(f\"Rozpoczynam ekstrakcję z {len(dataset)} próbek...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Extraction\"):\n",
        "            # Przeniesienie danych na GPU\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels']\n",
        "\n",
        "            # --- Single Pass Extraction ---\n",
        "            # Uruchamiamy model raz, ale prosimy o zwrot stanów ukrytych wszystkich warstw\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "\n",
        "            # outputs.hidden_states to krotka (tuple) tensorów\n",
        "            # Indeks 0 = Word Embeddings, Indeks 1..6 = Warstwy Encodera\n",
        "            for layer_idx, hidden_state in enumerate(outputs.hidden_states):\n",
        "                if layer_idx not in layers_data:\n",
        "                    layers_data[layer_idx] = []\n",
        "\n",
        "                # Wyciągamy token [CLS] (indeks 0 w sekwencji) -> [batch_size, hidden_dim]\n",
        "                cls_embeddings = hidden_state[:, 0, :].cpu().numpy()\n",
        "                layers_data[layer_idx].append(cls_embeddings)\n",
        "\n",
        "            # Zbieramy etykiety (zakładamy, że labels są one-hot lub listą, bierzemy 'toxic' czyli kolumnę 0)\n",
        "            # Jeśli labels to tensor float [batch, 6], bierzemy pierwszą kolumnę (toxic)\n",
        "            # Jeśli labels to LongTensor (klasyfikacja binarna), bierzemy bezpośrednio\n",
        "            if labels.dim() > 1:\n",
        "                toxic_labels = labels[:, 0].cpu().numpy() # Zakładamy, że indeks 0 to 'toxic'\n",
        "            else:\n",
        "                toxic_labels = labels.cpu().numpy()\n",
        "\n",
        "            all_labels.extend(toxic_labels)\n",
        "\n",
        "    # Konwersja list na macierze numpy\n",
        "    final_layer_activations = {\n",
        "        layer: np.concatenate(data, axis=0)\n",
        "        for layer, data in layers_data.items()\n",
        "    }\n",
        "    final_labels = np.array(all_labels)\n",
        "\n",
        "    return final_layer_activations, final_labels\n",
        "\n",
        "# Ustalenie wielkości podzbioru do analizy (np. 1000 próbek dla szybkości lub cały zbiór)\n",
        "# Używamy wcześniej zdefiniowanego 'test_subset' z Twojego notebooka lub tworzymy nowy\n",
        "eval_subset_size = 1000\n",
        "if len(eval_dataset) > eval_subset_size:\n",
        "    analysis_dataset = eval_dataset.select(range(eval_subset_size))\n",
        "else:\n",
        "    analysis_dataset = eval_dataset\n",
        "\n",
        "# Uruchomienie ekstrakcji\n",
        "X_layers_dict, y_all = extract_all_layers(analysis_dataset, model, device, batch_size=32)\n",
        "\n",
        "print(f\"\\nEkstrakcja zakończona. Pobrane warstwy: {list(X_layers_dict.keys())}\")\n",
        "print(f\"Kształt aktywacji dla warstwy 0: {X_layers_dict[0].shape}\")\n",
        "\n",
        "\n",
        "# --- KROK 2: Pętla treningowa sond (Probing Loop) ---\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"\\nRozpoczynam trening sond liniowych (Linear Probes)...\")\n",
        "\n",
        "for layer_idx in sorted(X_layers_dict.keys()):\n",
        "    X = X_layers_dict[layer_idx]\n",
        "    y = y_all\n",
        "\n",
        "    # Binaryzacja etykiet (na wszelki wypadek, gdyby były floatami)\n",
        "    y = (y > 0.5).astype(int)\n",
        "\n",
        "    # Podział na train/test dla sondy\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Trening LogReg\n",
        "    clf = LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Ewaluacja\n",
        "    y_pred = clf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    results.append({\n",
        "        'layer': layer_idx,\n",
        "        'accuracy': acc,\n",
        "        'f1': f1\n",
        "    })\n",
        "\n",
        "    print(f\"Layer {layer_idx}: Acc={acc:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "# Konwersja wyników do DataFrame dla łatwiejszego rysowania\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "# --- KROK 3: Wizualizacja (Line Plot) ---\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Rysowanie linii\n",
        "sns.lineplot(data=df_results, x='layer', y='accuracy', marker='o', label='Accuracy', linewidth=2.5)\n",
        "sns.lineplot(data=df_results, x='layer', y='f1', marker='s', label='F1 Score', linewidth=2.5)\n",
        "\n",
        "# Formatowanie wykresu\n",
        "plt.title(\"Liniowa separowalność toksyczności w warstwach modelu (DistilBERT)\", fontsize=14, pad=15)\n",
        "plt.xlabel(\"Numer Warstwy (0 = Embeddings, 1-6 = Transformer Layers)\", fontsize=12)\n",
        "plt.ylabel(\"Wartość Metryki\", fontsize=12)\n",
        "plt.ylim(0.0, 1.05)  # Skala Y od 0 do nieco powyżej 1\n",
        "plt.xticks(df_results['layer']) # Wymuś pokazanie wszystkich numerów warstw\n",
        "plt.legend(fontsize=11)\n",
        "\n",
        "# Dodanie wartości nad punktami (opcjonalnie, dla czytelności)\n",
        "for index, row in df_results.iterrows():\n",
        "    plt.text(row['layer'], row['accuracy'] + 0.01, f\"{row['accuracy']:.2f}\",\n",
        "             ha='center', color='blue', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sQQkWggCADaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "# --- CZĘŚĆ 1: Generator Parafraz (T5) ---\n",
        "\n",
        "print(\"Ładowanie modelu do parafrazowania (T5)...\")\n",
        "para_model_name = \"Vamsi/T5_Paraphrase_Paws\"\n",
        "para_tokenizer = AutoTokenizer.from_pretrained(para_model_name)\n",
        "para_model = AutoModelForSeq2SeqLM.from_pretrained(para_model_name).to(device)\n",
        "print(\"Model T5 załadowany!\")\n",
        "\n",
        "def generate_paraphrase(text, num_return_sequences=1):\n",
        "    \"\"\"\n",
        "    Generuje parafrazę dla podanego tekstu używając modelu T5.\n",
        "    \"\"\"\n",
        "    para_model.eval()\n",
        "\n",
        "    # T5 wymaga prefiksu dla tego konkretnego zadania\n",
        "    text = \"paraphrase: \" + text + \" </s>\"\n",
        "\n",
        "    encoding = para_tokenizer.encode_plus(\n",
        "        text,\n",
        "        padding=\"longest\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    attention_masks = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = para_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_masks,\n",
        "            max_length=256,\n",
        "            do_sample=True, # Sampling pozwala na większą różnorodność\n",
        "            top_k=120,\n",
        "            top_p=0.95,\n",
        "            early_stopping=True,\n",
        "            num_return_sequences=num_return_sequences\n",
        "        )\n",
        "\n",
        "    # Dekodowanie wyniku\n",
        "    paraphrase = para_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return paraphrase\n",
        "\n",
        "# --- CZĘŚĆ 2: Funkcje pomocnicze do Stabilności ---\n",
        "\n",
        "def get_top_k_tokens(text_input, model, tokenizer, k=5):\n",
        "    \"\"\"\n",
        "    Oblicza atrybucje IG i zwraca zbiór k najważniejszych słów (stringów).\n",
        "    \"\"\"\n",
        "    # 1. Przygotowanie inputu dla DistilBERT\n",
        "    inputs = tokenizer(text_input, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    input_ids = inputs.input_ids\n",
        "    attention_mask = inputs.attention_mask\n",
        "\n",
        "    # Funkcja predykcji dla IG\n",
        "    def predict_func(inputs_embeds):\n",
        "        out = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "        return out.logits\n",
        "\n",
        "    ig = IntegratedGradients(predict_func)\n",
        "\n",
        "    # Embeddings\n",
        "    input_embeddings = model.distilbert.embeddings(input_ids)\n",
        "    ref_input_ids = torch.tensor([tokenizer.pad_token_id] * input_ids.size(1), device=device).unsqueeze(0)\n",
        "    ref_input_embeddings = model.distilbert.embeddings(ref_input_ids)\n",
        "\n",
        "    # Atrybucja (Target=0 -> 'Toxic' w Twoim modelu, sprawdź czy to właściwy indeks!)\n",
        "    # Zakładam, że index 0 to klasa, którą badamy (np. Toxic). Jeśli Toxic to 1, zmień target=1.\n",
        "    # W Twoim poprzednim kodzie labels_list[0] to 'toxic'.\n",
        "    target_idx = 0\n",
        "\n",
        "    attributions, _ = ig.attribute(\n",
        "        inputs=input_embeddings,\n",
        "        baselines=ref_input_embeddings,\n",
        "        target=target_idx,\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "\n",
        "    # Sumowanie i wybór Top-K\n",
        "    attr_sum = attributions.sum(dim=-1).squeeze(0)\n",
        "    _, top_indices = torch.topk(attr_sum, k=min(k, len(attr_sum)))\n",
        "\n",
        "    # Konwersja ID na Tokeny (Stringi)\n",
        "    top_tokens = tokenizer.convert_ids_to_tokens(input_ids[0][top_indices])\n",
        "\n",
        "    # Czyszczenie tokenów (usuwanie '##' z subwords i lowercase)\n",
        "    clean_tokens = set([t.replace(\"##\", \"\").lower() for t in top_tokens if t not in ['[CLS]', '[SEP]', '[PAD]']])\n",
        "\n",
        "    return clean_tokens\n",
        "\n",
        "def evaluate_stability(original_text, layer_index, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Główna funkcja obliczająca 3 metryki stabilności.\n",
        "    \"\"\"\n",
        "    # 1. Generowanie parafrazy\n",
        "    paraphrase_text = generate_paraphrase(original_text)\n",
        "\n",
        "    # 2. Przygotowanie obu tekstów\n",
        "    inputs_orig = tokenizer(original_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "    inputs_para = tokenizer(paraphrase_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # --- A & B: Predykcja i Reprezentacja Warstwy ---\n",
        "    # Uruchamiamy model z output_hidden_states=True\n",
        "    with torch.no_grad():\n",
        "        out_orig = model(**inputs_orig, output_hidden_states=True)\n",
        "        out_para = model(**inputs_para, output_hidden_states=True)\n",
        "\n",
        "    # A. Stabilność Predykcji (Output Stability)\n",
        "    # Bierzemy prawdopodobieństwo klasy Toxic (indeks 0 lub 1 zależnie od Twojej konfiguracji)\n",
        "    # Zakładam sigmoid dla multilabel, bierzemy pierwszy label 'toxic'\n",
        "    prob_orig = torch.sigmoid(out_orig.logits)[0][0].item()\n",
        "    prob_para = torch.sigmoid(out_para.logits)[0][0].item()\n",
        "    pred_diff = abs(prob_orig - prob_para)\n",
        "\n",
        "    # B. Stabilność Reprezentacji (Layer Stability - Cosine Sim)\n",
        "    # Pobieramy hidden state z wybranej warstwy.\n",
        "    # Tuple ma (embeddings, layer1, ... layer6). Layer index 0 w kodzie to embeddings.\n",
        "    # Jeśli layer_index=5, bierzemy outputs.hidden_states[5]\n",
        "\n",
        "    # Wyciągamy wektor [CLS] (indeks 0 w sekwencji)\n",
        "    cls_orig = out_orig.hidden_states[layer_index][:, 0, :] # [1, 768]\n",
        "    cls_para = out_para.hidden_states[layer_index][:, 0, :] # [1, 768]\n",
        "\n",
        "    cosine_sim = F.cosine_similarity(cls_orig, cls_para).item()\n",
        "\n",
        "    # --- C: Stabilność Atrybucji (Jaccard) ---\n",
        "    # Obliczamy tylko jeśli mamy wystarczająco zasobów (IG jest kosztowne)\n",
        "    tokens_orig = get_top_k_tokens(original_text, model, tokenizer, k=5)\n",
        "    tokens_para = get_top_k_tokens(paraphrase_text, model, tokenizer, k=5)\n",
        "\n",
        "    # Jaccard Index\n",
        "    intersection = len(tokens_orig.intersection(tokens_para))\n",
        "    union = len(tokens_orig.union(tokens_para))\n",
        "    jaccard_score = intersection / union if union > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"Original Text\": original_text,\n",
        "        \"Paraphrase\": paraphrase_text,\n",
        "        \"Prob Original\": round(prob_orig, 4),\n",
        "        \"Prob Paraphrase\": round(prob_para, 4),\n",
        "        \"Pred Diff (Output)\": round(pred_diff, 4),\n",
        "        \"Layer Cosine Sim\": round(cosine_sim, 4),\n",
        "        \"Attribution Jaccard\": round(jaccard_score, 4),\n",
        "        \"Top Tokens Orig\": list(tokens_orig),\n",
        "        \"Top Tokens Para\": list(tokens_para)\n",
        "    }\n",
        "\n",
        "# --- CZĘŚĆ 3: Eksperyment i Raportowanie ---\n",
        "\n",
        "# Wybór toksycznych przykładów ze zbioru testowego (tam gdzie label=1)\n",
        "# Zakładam, że masz 'test_subset' i 'y_labels' z poprzednich kroków.\n",
        "# Jeśli nie, pobieramy nowe z eval_dataset.\n",
        "\n",
        "# Szukamy indeksów toksycznych\n",
        "toxic_indices = [i for i, x in enumerate(y_test_probe) if x == 1][:15] # Bierzemy 15 sztuk\n",
        "if len(toxic_indices) == 0:\n",
        "    print(\"Brak toksycznych próbek w podręcznym zbiorze, dobieram losowe...\")\n",
        "    toxic_indices = range(10)\n",
        "\n",
        "print(f\"\\nRozpoczynam analizę stabilności dla {len(toxic_indices)} przykładów...\")\n",
        "print(f\"Badana warstwa: {5} (Zgodnie z wynikami poprzedniej analizy)\")\n",
        "\n",
        "results_stability = []\n",
        "\n",
        "# Iteracja\n",
        "for idx in toxic_indices:\n",
        "    # Pobierz tekst (odkoduj z input_ids jeśli trzeba, lub weź z datasetu raw)\n",
        "    # Tutaj zakładam, że wyciągamy surowy tekst z datasetu\n",
        "    # (Jeśli korzystasz z 'test_subset', musimy odkodować tokeny)\n",
        "    input_ids_raw = test_subset[idx]['input_ids']\n",
        "    orig_text = tokenizer.decode(input_ids_raw, skip_special_tokens=True)\n",
        "\n",
        "    # Uruchomienie ewaluacji dla Warstwy 5 (najlepszej wg wykresu)\n",
        "    metrics = evaluate_stability(orig_text, layer_index=5, model=model, tokenizer=tokenizer)\n",
        "    results_stability.append(metrics)\n",
        "\n",
        "# Tworzenie DataFrame\n",
        "df_stability = pd.DataFrame(results_stability)\n",
        "\n",
        "# Wyświetlanie wyników\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "display(df_stability[[\n",
        "    \"Original Text\", \"Paraphrase\",\n",
        "    \"Pred Diff (Output)\", \"Layer Cosine Sim\", \"Attribution Jaccard\"\n",
        "]])\n",
        "\n",
        "# Podsumowanie średnie\n",
        "print(\"\\n--- PODSUMOWANIE STABILNOŚCI (Średnie) ---\")\n",
        "print(f\"Mean Prediction Stability (Diff): {df_stability['Pred Diff (Output)'].mean():.4f} (Im mniej tym lepiej)\")\n",
        "print(f\"Mean Layer Stability (Cosine):    {df_stability['Layer Cosine Sim'].mean():.4f} (Im bliżej 1.0 tym lepiej)\")\n",
        "print(f\"Mean Attribution Stability (Jacc):{df_stability['Attribution Jaccard'].mean():.4f} (Im bliżej 1.0 tym lepiej)\")"
      ],
      "metadata": {
        "id": "GsUmG5UsBrc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- POPRAWIONY KROK 1: Obliczanie wektora metodą Difference of Means ---\n",
        "\n",
        "# 1. Rozdzielamy dane z Warstwy 5 na toksyczne i bezpieczne\n",
        "# Używamy X_layers_dict[5] (z wcześniejszej ekstrakcji) i y_all\n",
        "X_layer_5 = X_layers_dict[5]\n",
        "y_bool = (y_all > 0.5) # True dla Toxic, False dla Safe\n",
        "\n",
        "# 2. Obliczamy średnie (centroids) dla obu grup\n",
        "mean_toxic = np.mean(X_layer_5[y_bool], axis=0)\n",
        "mean_safe = np.mean(X_layer_5[~y_bool], axis=0)\n",
        "\n",
        "# 3. Wektor kierunkowy: Od Safe do Toxic\n",
        "# To jest wektor, który mówi: \"Co dodać do bezpiecznego zdania, żeby stało się toksyczne?\"\n",
        "direction_vector = mean_toxic - mean_safe\n",
        "\n",
        "# --- DEBUGGING SKALI (Kluczowe dla dobrania Alphy) ---\n",
        "vec_norm = np.linalg.norm(direction_vector)\n",
        "hidden_state_norm = np.linalg.norm(mean_safe) # Średnia \"siła\" aktywacji modelu\n",
        "\n",
        "print(f\"Norma wektora sterującego (Diff of Means): {vec_norm:.4f}\")\n",
        "print(f\"Średnia norma aktywacji w modelu: {hidden_state_norm:.4f}\")\n",
        "print(f\"Stosunek sił: {vec_norm / hidden_state_norm:.4f}\")\n",
        "\n",
        "# 4. Konwersja do Torch (BEZ NORMALIZACJI DO 1!)\n",
        "# Nie normalizujemy wektora, bo chcemy zachować naturalną \"siłę\" różnicy między klasami.\n",
        "steering_tensor = torch.tensor(direction_vector, dtype=torch.float32).to(device)\n",
        "\n",
        "print(\"\\nNowy wektor sterujący (Mean Diff) gotowy!\")\n",
        "\n",
        "# --- POWTÓRZENIE TESTU (Kopiuj-wklej z poprzedniego kodu, ale z większym zakresem alpha) ---\n",
        "\n",
        "# Zwiększamy zakres alpha, ponieważ nie znormalizowaliśmy wektora (lub jeśli jest mały)\n",
        "# Jeśli vec_norm jest duży (np. 10), używamy mniejszych alpha (np. -2, 2).\n",
        "# Jeśli vec_norm jest mały (np. 0.5), używamy dużych alpha (np. -20, 20).\n",
        "# Poniżej dynamiczny dobór alpha na podstawie normy:\n",
        "\n",
        "scale_factor = 5.0  # Mnożnik eksperymentalny\n",
        "suggested_alpha = scale_factor\n",
        "\n",
        "print(f\"\\nSugerowana siła alpha: +/- {suggested_alpha}\")\n",
        "\n",
        "# Test na tym samym zdaniu\n",
        "text_toxic = \"You are a complete idiot and a waste of time.\"\n",
        "\n",
        "# Funkcja predict_with_steering musi być zdefiniowana (z poprzedniego kroku)\n",
        "score_orig = predict_with_steering(text_toxic, model, tokenizer, steering_tensor, alpha=0)\n",
        "score_detox = predict_with_steering(text_toxic, model, tokenizer, steering_tensor, alpha=-suggested_alpha) # Odejmujemy toksyczność\n",
        "score_toxic = predict_with_steering(text_toxic, model, tokenizer, steering_tensor, alpha=suggested_alpha)  # Dodajemy toksyczność\n",
        "\n",
        "print(f\"\\nZdanie: {text_toxic}\")\n",
        "print(f\"Oryginał (Alpha 0):      {score_orig:.4f}\")\n",
        "print(f\"Detoksykacja (Alpha -{suggested_alpha}): {score_detox:.4f} (Oczekujemy spadku)\")\n",
        "print(f\"Toksyfikacja (Alpha +{suggested_alpha}): {score_toxic:.4f} (Oczekujemy wzrostu)\")\n",
        "\n",
        "# Rysowanie wykresu dla szerszego zakresu\n",
        "alphas = np.linspace(-suggested_alpha * 2, suggested_alpha * 2, 10)\n",
        "scores = [predict_with_steering(text_toxic, model, tokenizer, steering_tensor, alpha=a) for a in alphas]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(alphas, scores, marker='o', color='green', linewidth=2)\n",
        "plt.axhline(0.5, color='gray', linestyle='--')\n",
        "plt.title(f\"Mean Difference Steering (Layer 5)\\nZdanie: '{text_toxic[:30]}...'\")\n",
        "plt.xlabel(\"Alpha (Ujemna = Detoksykacja)\")\n",
        "plt.ylabel(\"Prawdopodobieństwo Toxic\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# --- Bonus: Przykład zmiany klasyfikacji ---\n",
        "print(\"\\n--- DEMO ZMIANY KLASYFIKACJI ---\")\n",
        "text_toxic = \"You are a complete idiot and a waste of time.\"\n",
        "base_score = predict_with_steering(text_toxic, model, tokenizer, steering_tensor, alpha=0)\n",
        "detox_score = predict_with_steering(text_toxic, model, tokenizer, steering_tensor, alpha=-15)\n",
        "\n",
        "print(f\"Zdanie: {text_toxic}\")\n",
        "print(f\"Oryginał:     {base_score:.4f} (Toxic? {base_score>0.5})\")\n",
        "print(f\"Po 'Leczeniu': {detox_score:.4f} (Toxic? {detox_score>0.5})\")"
      ],
      "metadata": {
        "id": "-oshWrc9Cjd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TEST BEZPIECZEŃSTWA (SIDE EFFECTS) ---\n",
        "\n",
        "# Ustawiamy optymalną Alphę na podstawie Twojego wykresu (między -2.5 a -5)\n",
        "OPTIMAL_ALPHA = -3.0\n",
        "\n",
        "# Lista zdań testowych różnych kategorii\n",
        "test_sentences = [\n",
        "    # 1. TOKSYCZNE (Tu chcemy zmiany na Safe)\n",
        "    (\"You are a complete idiot and a waste of time.\", \"Toxic\"),\n",
        "    (\"I hope you fail miserably, you loser.\", \"Toxic\"),\n",
        "\n",
        "    # 2. BEZPIECZNE - POZYTYWNE (Tu nie chcemy zmian)\n",
        "    (\"I really love the weather today, it is beautiful.\", \"Safe\"),\n",
        "    (\"Thank you for your help, I appreciate it.\", \"Safe\"),\n",
        "\n",
        "    # 3. BEZPIECZNE - NEGATYWNE (To jest PUŁAPKA! Model nie powinien tego ruszać)\n",
        "    (\"I am feeling very sad and tired today.\", \"Safe_Negative\"),\n",
        "    (\"The movie was boring and too long.\", \"Safe_Negative\"),\n",
        "    (\"I disagree with your opinion regarding the tax policy.\", \"Safe_Negative\")\n",
        "]\n",
        "\n",
        "print(f\"--- TEST SKUTKÓW UBOCZNYCH (Alpha = {OPTIMAL_ALPHA}) ---\\n\")\n",
        "print(f\"{'Sentence Category':<15} | {'Original Prob':<15} | {'Detox Prob':<15} | {'Status'}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Upewnij się, że steering_tensor jest zdefiniowany (ten z metody Difference of Means)\n",
        "for text, category in test_sentences:\n",
        "    # Wynik bez interwencji\n",
        "    prob_orig = predict_with_steering(text, model, tokenizer, steering_tensor, alpha=0)\n",
        "\n",
        "    # Wynik z detoksykacją\n",
        "    prob_detox = predict_with_steering(text, model, tokenizer, steering_tensor, alpha=OPTIMAL_ALPHA)\n",
        "\n",
        "    # Ocena\n",
        "    # Dla Toxic: Sukces, jeśli spadnie < 0.5\n",
        "    # Dla Safe: Sukces, jeśli pozostanie < 0.5 (i nie wzrośnie drastycznie)\n",
        "\n",
        "    status = \"\"\n",
        "    if category == \"Toxic\":\n",
        "        if prob_detox < 0.1: status = \"✅ Fixed\"\n",
        "        elif prob_detox < 0.5: status = \"⚠️ Improved\"\n",
        "        else: status = \"❌ Failed\"\n",
        "    else:\n",
        "        # Dla zdań bezpiecznych sprawdzamy, czy model nie \"zwariował\" (np. nagle 0.9)\n",
        "        # Ale ważniejsze: czy wynik się drastycznie nie zmienił?\n",
        "        # W idealnym świecie Safe nie powinno się ruszać.\n",
        "        change = abs(prob_orig - prob_detox)\n",
        "        if prob_detox > 0.5: status = \"❌ BROKEN (Flagged Safe)\"\n",
        "        elif change < 0.2: status = \"✅ Stable\"\n",
        "        else: status = \"⚠️ Shifted\"\n",
        "\n",
        "    print(f\"{category:<15} | {prob_orig:.4f}          | {prob_detox:.4f}          | {status}\")"
      ],
      "metadata": {
        "id": "-TudqH85Dkqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "# --- BLOK 1: ZAPIS (EKSPORT ARTEFAKTU) ---\n",
        "\n",
        "# Tworzymy słownik ze wszystkimi potrzebnymi danymi\n",
        "steering_artifact = {\n",
        "    \"steering_vector\": steering_tensor.cpu(),  # Przenosimy na CPU do zapisu\n",
        "    \"layer_index\": 5,                          # Twoja znaleziona warstwa\n",
        "    \"alpha\": OPTIMAL_ALPHA,                    # Twoja optymalna siła (-3.0)\n",
        "    \"method\": \"mean_difference\",\n",
        "    \"model_name\": \"distilbert-base-uncased\",\n",
        "    \"description\": \"Vector removing toxicity concept from layer 5\"\n",
        "}\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "save_path = f\"/drive/MyDrive/msc-project/vectors/toxicity_steering_controller_{timestamp}.pt\"\n",
        "\n",
        "torch.save(steering_artifact, save_path)\n",
        "print(f\"✅ Artefakt sterujący zapisany w: {save_path}\")\n",
        "\n",
        "\n",
        "# --- BLOK 2: SYMULACJA PRODUKCJI (CZYSTA SESJA) ---\n",
        "# Wyobraź sobie, że to jest oddzielny skrypt na serwerze, który nie widzi danych treningowych.\n",
        "\n",
        "print(\"\\n--- SYMULACJA ŚRODOWISKA PRODUKCYJNEGO ---\")\n",
        "\n",
        "# 1. Ładowanie artefaktu\n",
        "artifact = torch.load(save_path)\n",
        "loaded_vector = artifact[\"steering_vector\"].to(device)\n",
        "loaded_layer = artifact[\"layer_index\"]\n",
        "loaded_alpha = artifact[\"alpha\"]\n",
        "\n",
        "print(f\"Wczytano sterownik: {artifact['description']}\")\n",
        "print(f\"Konfiguracja: Layer {loaded_layer}, Alpha {loaded_alpha}\")\n",
        "\n",
        "# 2. Definicja klasy Hooka (Musi być dostępna w kodzie produkcyjnym)\n",
        "class ProductionSteeringHook:\n",
        "    def __init__(self, vector, coeff):\n",
        "        self.vector = vector\n",
        "        self.coeff = coeff\n",
        "\n",
        "    def __call__(self, module, inputs, output):\n",
        "        # output[0] to hidden_states\n",
        "        hidden_states = output[0]\n",
        "        shifted_states = hidden_states + (self.coeff * self.vector)\n",
        "        return (shifted_states,) + output[1:]\n",
        "\n",
        "# 3. Funkcja Inference z \"Detoksem\"\n",
        "def generate_safe_prediction(text, model, tokenizer):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "\n",
        "    # Rejestracja Hooka (Wstrzyknięcie \"szczepionki\")\n",
        "    hook = model.distilbert.transformer.layer[loaded_layer].register_forward_hook(\n",
        "        ProductionSteeringHook(loaded_vector, loaded_alpha)\n",
        "    )\n",
        "\n",
        "    # Predykcja\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.sigmoid(outputs.logits)[0][0].item()\n",
        "\n",
        "    # Usunięcie Hooka (Sprzątanie)\n",
        "    hook.remove()\n",
        "\n",
        "    return probs\n",
        "\n",
        "# 4. Test na żywo\n",
        "live_test_text = \"You are completely useless and stupid.\"\n",
        "safety_score = generate_safe_prediction(live_test_text, model, tokenizer)\n",
        "\n",
        "print(f\"\\nLive Test Input: '{live_test_text}'\")\n",
        "print(f\"Model Toxic Probability (Steered): {safety_score:.4f}\")\n",
        "print(f\"Decyzja: {'🔴 BLOKUJ' if safety_score > 0.5 else '🟢 PRZEPUŚĆ'}\")"
      ],
      "metadata": {
        "id": "n5-sUYzAER7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podsumowanie Eksperymentu: Representation Engineering w Detoksykacji Modelu\n",
        "W ramach tego projektu przeprowadzono kompleksową analizę i modyfikację wewnętrznych reprezentacji modelu DistilBERT (fine-tuned na Jigsaw Toxicity) w celu sterowania jego zachowaniem bez konieczności ponownego trenowania (fine-tuning).\n",
        "\n",
        "Zrealizowane etapy:\n",
        "\n",
        "Analiza Warstwowa (Layer-wise Analysis):\n",
        "\n",
        "Zbadano liniową separowalność konceptu \"toksyczności\" w głąb sieci.\n",
        "\n",
        "Zidentyfikowano Warstwę 5 jako kluczowy punkt (tzw. sweet spot), gdzie reprezentacja semantyczna jest najsilniejsza (F1 Score = 0.80), przewyższając warstwę ostatnią.\n",
        "\n",
        "Badanie Stabilności (Stability Analysis):\n",
        "\n",
        "Wykorzystano generator parafraz (T5) do sprawdzenia odporności reprezentacji.\n",
        "\n",
        "Wykazano, że wektory aktywacji w Warstwie 5 są niezwykle stabilne semantycznie (Cosine Similarity > 0.99) nawet przy zmianie struktury zdania, co potwierdziło zasadność interwencji w tym miejscu.\n",
        "\n",
        "Ekstrakcja Wektora Sterującego (Concept Extraction):\n",
        "\n",
        "Zastosowano metodę Difference of Means (Różnica Średnich), obliczając wektor różnicowy między centroidami aktywacji dla przykładów toksycznych i bezpiecznych w Warstwie 5.\n",
        "\n",
        "Metoda ta okazała się skuteczniejsza od wag regresji logistycznej, zapewniając odpowiednią skalę sygnału.\n",
        "\n",
        "Interwencja i Sterowanie (Model Steering):\n",
        "\n",
        "Zaimplementowano mechanizm PyTorch Forward Hook, umożliwiający wstrzykiwanie wektora sterującego w czasie rzeczywistym.\n",
        "\n",
        "Zastosowano interwencję z siłą Alpha = -3.0, co pozwoliło na skuteczną \"detoksykację\" modelu.\n",
        "\n",
        "Ewaluacja i Quality Assurance:\n",
        "\n",
        "Skuteczność: Prawdopodobieństwo wykrycia toksyczności dla fraz obraźliwych spadło z ~92% do ~1-4%.\n",
        "\n",
        "Bezpieczeństwo: Model zachował poprawne działanie dla zdań neutralnych i pozytywnych (brak efektu \"lobotomii\" modelu).\n",
        "\n",
        "Redukcja False Positives: Interwencja wyeliminowała błędne oznaczanie zdań o negatywnym sentymencie (np. narzekanie) jako toksyczne (spadek z 10% do 0%).\n",
        "\n",
        "Wniosek końcowy: Projekt potwierdził, że Representation Engineering (RepE) jest potężną, niskokosztową metodą kontroli modeli LLM/BERT. Poprzez precyzyjną operację na wektorach aktywacji w Warstwie 5 udało się wyeliminować niepożądane zachowanie modelu (wykrywanie toksyczności) przy zachowaniu jego ogólnych zdolności językowych."
      ],
      "metadata": {
        "id": "o_g4mzO2Ec9K"
      }
    }
  ]
}