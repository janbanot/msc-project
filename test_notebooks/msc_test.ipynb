{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-project/blob/main/test_notebooks/msc_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Klasyfikacja Toksycznych Komentarzy z DistilBERT i Analiza XAI\n",
        "\n",
        "Ten notebook demonstruje:\n",
        "1. Fine-tuning modelu DistilBERT dla wieloklasowej klasyfikacji toksyczności\n",
        "2. Metody wyjaśnialności (Integrated Gradients, Input×Gradient)\n",
        "3. Analiza warstwowa metodą sondowania liniowego\n",
        "4. Inżynieria reprezentacji do sterowania zachowaniem modelu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Konfiguracja Środowiska i Zależności\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N9kkURRXxcaG"
      },
      "outputs": [],
      "source": [
        "!uv pip install transformers datasets captum quantus accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnzL_6SCy5qj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Wczytywanie i Eksploracja Danych\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7bbe3ea"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ścieżka do pliku CSV z danymi Jigsaw Toxic Comment\n",
        "csv_path = '/drive/MyDrive/msc-project/jigsaw-toxic-comment/train.csv'\n",
        "\n",
        "try:\n",
        "    dataframe = pd.read_csv(csv_path)\n",
        "    print(\"Plik CSV wczytany pomyślnie!\")\n",
        "    display(dataframe.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Błąd: Nie znaleziono pliku pod ścieżką {csv_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Wystąpił błąd: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocessing i Czyszczenie Danych\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(example):\n",
        "    \"\"\"\n",
        "    Stosuje kompleksowe operacje czyszczenia na polu 'comment_text'.\n",
        "    \n",
        "    Funkcja jest kluczowa dla zapewnienia spójności danych między treningiem a ewaluacją.\n",
        "    Usuwa szum i normalizuje format tekstów komentarzy.\n",
        "    \n",
        "    Argumenty:\n",
        "        example: Słownik zawierający klucz 'comment_text' z tekstem do oczyszczenia\n",
        "        \n",
        "    Zwraca:\n",
        "        Zaktualizowany słownik example z oczyszczonym tekstem\n",
        "        \n",
        "    Operacje czyszczenia:\n",
        "        - Konwersja na małe litery (wymagane dla modeli BERT typu uncased)\n",
        "        - Usunięcie URL (wzorce http/https/www)\n",
        "        - Usunięcie adresów IP (np. 192.168.1.1)\n",
        "        - Usunięcie metadanych Wikipedii (talk pages, timestampy UTC)\n",
        "        - Usunięcie znaków specjalnych i normalizacja białych znaków\n",
        "    \"\"\"\n",
        "    text = example['comment_text']\n",
        "    \n",
        "    # Konwersja na małe litery dla modeli uncased BERT\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Usunięcie URL (wzorce http/https i www)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    \n",
        "    # Usunięcie adresów IP (np. 192.168.1.1)\n",
        "    text = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', '', text)\n",
        "    \n",
        "    # Usunięcie metadanych specyficznych dla Wikipedii\n",
        "    text = re.sub(r'\\(talk\\)', '', text)\n",
        "    text = re.sub(r'\\d{2}:\\d{2}, \\w+ \\d{1,2}, \\d{4} \\(utc\\)', '', text)\n",
        "    \n",
        "    # Usunięcie znaków nowej linii i spacji niepodzielnych\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = text.replace('\\xa0', ' ')\n",
        "    \n",
        "    # Usunięcie cudzysłowów z początku/końca\n",
        "    text = text.strip(' \"')\n",
        "    \n",
        "    # Normalizacja białych znaków (zamiana wielokrotnych spacji na jedną)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()    return example\n",
        "\n",
        "        example['comment_text'] = text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMcBMwOZZOYj"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "# Ograniczenie do pierwszych 2000 próbek dla szybszego eksperymentowania\n",
        "# Wartość 2000 zapewnia wystarczającą różnorodność danych przy krótkim czasie przetwarzania\n",
        "train_dataframe = dataframe.head(2000)\n",
        "dataset = datasets.Dataset.from_pandas(train_dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XSauFnTY4GY"
      },
      "outputs": [],
      "source": [
        "print(\"Czyszczenie danych...\")\n",
        "cleaned_dataset = dataset.map(clean_text)\n",
        "print(\"Czyszczenie danych zakończone!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Eij-Oi0ZTkh"
      },
      "outputs": [],
      "source": [
        "# Porównanie danych przed i po czyszczeniu\n",
        "print(\"=== PRZED CZYSZCZENIEM ===\")\n",
        "print(dataset[1]['comment_text'])\n",
        "print(\"\\n\" + dataset[6]['comment_text'])\n",
        "print(\"\\n\" + dataset[0]['comment_text'])\n",
        "\n",
        "print(\"\\n\\n=== PO CZYSZCZENIU ===\")\n",
        "print(cleaned_dataset[1]['comment_text'])\n",
        "print(\"\\n\" + cleaned_dataset[6]['comment_text'])\n",
        "print(\"\\n\" + cleaned_dataset[0]['comment_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Tokenizacja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdmXlYxUczCB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Załadowanie tokenizera dla modelu DistilBERT uncased\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    print(\"Tokenizer załadowany pomyślnie!\")\n",
        "except Exception as e:\n",
        "    print(f\"Błąd ładowania tokenizera: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7di3UMZHdB1U"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"\n",
        "    Tokenizuje batch tekstów dla modeli BERT.\n",
        "    \n",
        "    Parametry tokenizacji zoptymalizowane dla DistilBERT i klasyfikacji komentarzy.\n",
        "    \n",
        "    Argumenty:\n",
        "        examples: Batch przykładów zawierających pole 'comment_text'\n",
        "        \n",
        "    Zwraca:\n",
        "        Słownik z kluczami: input_ids (ID tokenów), attention_mask (maska paddingu)\n",
        "        \n",
        "    Parametry:\n",
        "        - padding=\"max_length\": Wyrównuje krótkie komentarze do jednolitej długości\n",
        "        - truncation=True: Obcina komentarze przekraczające maksymalną długość\n",
        "        - max_length=256: Równowaga między kontekstem a szybkością\n",
        "                         (DistilBERT obsługuje do 512, ale 256 wystarczy dla komentarzy)\n",
        "    \n",
        "    Uwagi:\n",
        "        256 tokenów to ~150-200 słów, co pokrywa większość komentarzy w zbiorze Jigsaw.\n",
        "        Większa wartość zwiększa zużycie pamięci GPU bez znaczącej poprawy jakości.\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"comment_text\"],\n",
        "        padding=\"max_length\",\n",
        "\n",
        "        truncation=True,print(\"Tokenizacja zakończona!\")\n",
        "\n",
        "        max_length=256  # Magic number: 256 tokenów = optymalna długość dla komentarzytokenized_dataset = cleaned_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    )print(\"Tokenizacja zbioru danych...\")\n",
        "\n",
        "# Zastosowanie tokenizacji z batchingiem dla wydajności"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn-sppLedLSN"
      },
      "outputs": [],
      "source": [
        "# Przykład zetokenizowanego wpisu\n",
        "print(\"=== Przykład Zetokenizowanego Wpisu ===\")\n",
        "print(tokenized_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Przygotowanie Etykiet dla Klasyfikacji Multi-Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8F9I2V2d-I0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definicja 6 kolumn etykiet toksyczności w kolejności\n",
        "label_columns = [\n",
        "    'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'\n",
        "]\n",
        "\n",
        "def create_labels_column(example):\n",
        "    \"\"\"\n",
        "    Konsoliduje 6 osobnych etykiet binarnych w jedną tablicę 'labels'.\n",
        "    \n",
        "    Przekształca format danych z wielu kolumn binarnych (0/1) na jedną listę,\n",
        "    która jest wymagana przez modele PyTorch dla klasyfikacji multi-label.\n",
        "    \n",
        "    Argumenty:\n",
        "        example: Słownik zawierający pojedyncze kolumny etykiet\n",
        "        \n",
        "    Zwraca:\n",
        "        Zaktualizowany example z dodatkową listą 'labels'\n",
        "        \n",
        "    Format wyjściowy:\n",
        "        example['labels'] = [toxic, severe_toxic, obscene, threat, insult, identity_hate]\n",
        "        Każda wartość to float (0.0 lub 1.0) dla kompatybilności z PyTorch\n",
        "        \n",
        "    Uwagi:\n",
        "        Konwersja na float jest kluczowa - PyTorch wymaga float32 dla BCEWithLogitsLoss\n",
        "    \"\"\"\n",
        "    labels_list = [float(example[col]) for col in label_columns]\n",
        "    example['labels'] = labels_list\n",
        "\n",
        "    return exampleprint(final_dataset[6])\n",
        "\n",
        "print(\"\\n=== Przykładowy Przetworzony Wpis ===\")\n",
        "\n",
        "print(\"Konsolidacja etykiet...\")# Wyświetlenie przykładowego przetworzono wpisu\n",
        "\n",
        "final_dataset = tokenized_dataset.map(create_labels_column)\n",
        "print(\"Etykiety skonsolidowane!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdvRwT_leOM6"
      },
      "outputs": [],
      "source": [
        "# Usunięcie niepotrzebnych kolumn w celu przygotowania do treningu\n",
        "columns_to_remove = [\n",
        "    'id', 'comment_text', 'toxic', 'severe_toxic',\n",
        "    'obscene', 'threat', 'insult', 'identity_hate'\n",
        "]\n",
        "\n",
        "print(f\"Oryginalne kolumny: {final_dataset.column_names}\")\n",
        "final_dataset = final_dataset.remove_columns(columns_to_remove)\n",
        "print(f\"Pozostałe kolumny: {final_dataset.column_names}\")\n",
        "\n",
        "# Ustawienie formatu zbioru danych na tensory PyTorch\n",
        "try:\n",
        "    final_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "    print(\"\\nFormat zbioru danych ustawiony na 'torch'!\")\n",
        "except ImportError:\n",
        "    print(\"\\nPyTorch nie jest zainstalowany. Pominięto .set_format('torch').\")\n",
        "    print(\"Zainstaluj komendą: pip install torch\")\n",
        "\n",
        "print(\"\\n=== Ostateczny Element Gotowy dla Modelu ===\")\n",
        "print(final_dataset[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Konfiguracja i Trening Modelu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RY2kk5asekT8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "num_labels = 6  # Sześć kategorii toksyczności (toxic, severe_toxic, obscene, threat, insult, identity_hate)\n",
        "\n",
        "# Załadowanie wstępnie wytrenowanego modelu i konfiguracja dla klasyfikacji multi-label\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels,\n",
        "    problem_type=\"multi_label_classification\"  # Multi-label: komentarz może mieć wiele etykiet jednocześnie\n",
        ")\n",
        "\n",
        "print(\"Model załadowany pomyślnie!\")\n",
        "print(\"Skonfigurowany dla klasyfikacji multi-label z 6 wyjściami.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EUdT7kYeyof"
      },
      "outputs": [],
      "source": [
        "# Podział zbioru danych na zestawy treningowy i ewaluacyjny\n",
        "data_splits = final_dataset.train_test_split(test_size=0.2, seed=42)  # 20% na ewaluację, 80% na trening\n",
        "\n",
        "train_dataset = data_splits['train']\n",
        "evaluation_dataset = data_splits['test']\n",
        "\n",
        "print(\"Podział danych zakończony:\")\n",
        "print(f\"  Próbki treningowe: {len(train_dataset)}\")\n",
        "print(f\"  Próbki ewaluacyjne: {len(evaluation_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX52g5ElfBO7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "import torch\n",
        "\n",
        "def compute_metrics(prediction: EvalPrediction):\n",
        "    \"\"\"\n",
        "    Oblicza metryki ewaluacyjne dla klasyfikacji multi-label.\n",
        "    \n",
        "    Funkcja konwertuje logity na prawdopodobieństwa (sigmoid), a następnie\n",
        "    binaryzuje przez próg 0.5 i oblicza kluczowe metryki.\n",
        "    \n",
        "    Argumenty:\n",
        "        prediction: Obiekt EvalPrediction z polami predictions i label_ids\n",
        "        \n",
        "    Zwraca:\n",
        "        Słownik z metrykami: f1_micro, accuracy\n",
        "        \n",
        "    Metryki:\n",
        "        - f1_micro: Mikro-uśredniony F1 (dobre dla niezbalansowanych etykiet;\n",
        "                   traktuje każdą próbkę-etykietę jako oddzielny przypadek)\n",
        "        - accuracy: Ogólna dokładność (procent poprawnie sklasyfikowanych elementów)\n",
        "        \n",
        "    Próg klasyfikacji (threshold = 0.5):\n",
        "        Wartość 0.5 jest standardem dla klasyfikacji binarnej.\n",
        "        Prawdopodobieństwo > 0.5 => etykieta pozytywna (1)\n",
        "        Prawdopodobieństwo <= 0.5 => etykieta negatywna (0)\n",
        "        Można dostosować próg dla różnej równowagi precyzja/recall.\n",
        "    \"\"\"\n",
        "    # Zastosowanie sigmoidu do logitów w celu uzyskania prawdopodobieństw\n",
        "    logits = prediction.predictions\n",
        "    probabilities = 1 / (1 + np.exp(-logits))  # Sigmoid: logits -> [0, 1]\n",
        "    \n",
        "    # Konwersja prawdopodobieństw na predykcje binarne (próg = 0.5)\n",
        "    threshold = 0.5  # Magic number: standardowy próg decyzyjny dla klasyfikacji binarnej\n",
        "\n",
        "    predictions = (probabilities > threshold).astype(int)    }\n",
        "\n",
        "            'accuracy': overall_accuracy\n",
        "\n",
        "    labels = prediction.label_ids        'f1_micro': f1_micro,\n",
        "\n",
        "        return {\n",
        "\n",
        "    # Mikro-uśredniony F1 (dobry dla niezbalansowanych etykiet)    \n",
        "\n",
        "    f1_micro = f1_score(labels, predictions, average='micro')    overall_accuracy = accuracy_score(labels.flatten(), predictions.flatten())\n",
        "\n",
        "        # Ogólna dokładność dla wszystkich etykiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd4r8HXrfO8K"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "model_output_dir = \"/drive/MyDrive/msc-project/models/distilbert-jigsaw-finetuned\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_output_dir,\n",
        "    learning_rate=2e-5,  # 2e-5 to standardowy LR dla fine-tuningu BERT (zbyt wysoki destabilizuje trening)\n",
        "    num_train_epochs=3,  # 3 epoki to standard dla BERT (więcej = ryzyko overfittingu)\n",
        "    per_device_train_batch_size=8,  # Batch size 8 bezpieczny dla GPU T4 w Colab (zwiększ do 16-32 dla silniejszych GPU)\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,  # Regularyzacja L2 zapobiegająca overfittingowi\n",
        "    eval_strategy=\"epoch\",  # Ewaluacja po każdej epoce (monitorowanie postępów)\n",
        "    save_strategy=\"epoch\",  # Zapis checkpointu po każdej epoce\n",
        "    logging_steps=5,  # Logowanie co 5 kroków treningowych\n",
        "    load_best_model_at_end=True,  # Wczytanie najlepszego modelu po zakończeniu (według f1_micro)\n",
        "    metric_for_best_model=\"f1_micro\",  # Kryterium wyboru najlepszego modelu\n",
        "    report_to=\"none\",  # Wyłączenie integracji z wandb\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgN5hlo3fowE"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=evaluation_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"=== Rozpoczęcie Treningu ===\")\n",
        "trainer.train()\n",
        "print(\"=== Trening Zakończony ===\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlksX5w2b_-H"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Utworzenie katalogu zapisu z timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "base_path = \"/drive/MyDrive/msc-project/models/final_distilbert_jigsaw\"\n",
        "save_directory = f\"{base_path}_{timestamp}\"\n",
        "\n",
        "# Zapis zarówno modelu jak i tokenizera\n",
        "trainer.save_model(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "print(f\"Model i tokenizer zapisane w: {save_directory}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Wnioskowanie i Testowanie Modelu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjKLCf6bcgqw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Przełączenie modelu w tryb ewaluacji\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Testowe zdanie z mieszaną treścią toksyczną i pozytywną\n",
        "test_text = \"you are a fucking moron, who should die in hell but I love your lovely kitten\"\n",
        "\n",
        "# Tokenizacja wejścia\n",
        "inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "# Uruchomienie wnioskowania\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    # Użycie sigmoid dla klasyfikacji multi-label (każda etykieta niezależnie)\n",
        "    probabilities = torch.sigmoid(logits)\n",
        "\n",
        "# Wyświetlenie wyników dla każdej etykiety\n",
        "labels_list = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "print(f\"Tekst: '{test_text}'\\n\")\n",
        "print(\"Prawdopodobieństwa Toksyczności:\")\n",
        "for label, probability in zip(labels_list, probabilities[0]):\n",
        "    print(f\"  {label}: {probability:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Analiza Wyjaśnianości z Integrated Gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVYwRbBVdZB0"
      },
      "outputs": [],
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "# Define prediction function wrapper for Captum\n",
        "def predict_function(inputs_embeds, attention_mask=None):\n",
        "    \"\"\"Wrapper function that returns model logits for Captum.\"\"\"\n",
        "    output = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "    return output.logits\n",
        "\n",
        "# Initialize Integrated Gradients\n",
        "integrated_gradients = IntegratedGradients(predict_function)\n",
        "\n",
        "# Select target label for attribution\n",
        "# 0=toxic, 1=severe_toxic, 2=obscene, 3=threat, 4=insult, 5=identity_hate\n",
        "target_label_index = 0\n",
        "target_name = labels_list[target_label_index]\n",
        "\n",
        "# Prepare input embeddings\n",
        "input_ids = inputs.input_ids\n",
        "input_embeddings = model.distilbert.embeddings(input_ids)\n",
        "\n",
        "# Create baseline (padding tokens as reference)\n",
        "reference_input_ids = torch.tensor(\n",
        "    [tokenizer.pad_token_id] * input_ids.size(1), \n",
        "    device=device\n",
        ").unsqueeze(0)\n",
        "reference_embeddings = model.distilbert.embeddings(reference_input_ids)\n",
        "\n",
        "# Prepare attention mask\n",
        "attention_mask = inputs.attention_mask\n",
        "\n",
        "# Compute attributions\n",
        "print(f\"Computing attributions for: {target_name}...\")\n",
        "\n",
        "attributions, delta = integrated_gradients.attribute(\n",
        "    inputs=input_embeddings,\n",
        "    baselines=reference_embeddings,\n",
        "    target=target_label_index,\n",
        "    additional_forward_args=(attention_mask,),\n",
        "    return_convergence_delta=True\n",
        ")\n",
        "\n",
        "print(f\"Attribution complete. Convergence delta: {delta.item():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0ur6jyFd7E3"
      },
      "outputs": [],
      "source": [
        "from captum.attr import visualization\n",
        "\n",
        "# Process attributions for visualization\n",
        "attributions_sum = attributions.sum(dim=-1).squeeze(0)\n",
        "attributions_sum = attributions_sum / torch.norm(attributions_sum)\n",
        "attributions_numpy = attributions_sum.cpu().detach().numpy()\n",
        "\n",
        "# Get probability for target label\n",
        "probability_score = probabilities[0][target_label_index].item()\n",
        "predicted_class_label = \"True\" if probability_score > 0.5 else \"False\"\n",
        "\n",
        "# Convert input IDs to tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "# Create visualization data record\n",
        "visualization_data = visualization.VisualizationDataRecord(\n",
        "    word_attributions=attributions_numpy,\n",
        "    pred_prob=probability_score,\n",
        "    pred_class=predicted_class_label,\n",
        "    true_class=1,  # Assume text is toxic\n",
        "    attr_class=target_name,\n",
        "    attr_score=attributions_numpy.sum(),\n",
        "    raw_input_ids=tokens,\n",
        "    convergence_score=delta\n",
        ")\n",
        "\n",
        "print(f\"\\nLabel explanation: {target_name}\")\n",
        "visualization.visualize_text([visualization_data])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Analiza Warstwowa Metodą Sondowania"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exduABBQeocg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Enable hidden states output in model configuration\n",
        "model.config.output_hidden_states = True\n",
        "\n",
        "def extract_hidden_states(data_subset, layer_index=4):\n",
        "    \"\"\"\n",
        "    Extracts hidden state representations from a specific layer.\n",
        "    \n",
        "    Args:\n",
        "        data_subset: Dataset subset to extract from\n",
        "        layer_index: Which transformer layer to extract (0-6 for DistilBERT)\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (hidden_states_array, labels_array)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_hidden_states = []\n",
        "    all_labels = []\n",
        "    \n",
        "    print(f\"Extracting representations from layer {layer_index}...\")\n",
        "    \n",
        "    for i in tqdm(range(len(data_subset))):\n",
        "        entry = data_subset[i]\n",
        "        \n",
        "        text_tensor = entry['input_ids'].unsqueeze(0).to(device)\n",
        "        mask_tensor = entry['attention_mask'].unsqueeze(0).to(device)\n",
        "        label = entry['labels'][0].item()  # Extract first label (toxic)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(text_tensor, attention_mask=mask_tensor)\n",
        "            hidden_state = outputs.hidden_states[layer_index]\n",
        "            # Extract CLS token embedding (first token)\n",
        "            cls_embedding = hidden_state[0, 0, :].cpu().numpy()\n",
        "            \n",
        "            all_hidden_states.append(cls_embedding)\n",
        "            all_labels.append(label)\n",
        "    \n",
        "    return np.array(all_hidden_states), np.array(all_labels)\n",
        "\n",
        "# Determine subset size for analysis\n",
        "total_evaluation_samples = len(evaluation_dataset)\n",
        "target_size = 500\n",
        "subset_size = min(target_size, total_evaluation_samples)\n",
        "\n",
        "print(f\"Available samples: {total_evaluation_samples}. Using: {subset_size}\")\n",
        "\n",
        "test_subset = evaluation_dataset.select(range(subset_size))\n",
        "\n",
        "# Extract hidden states from layer 4\n",
        "hidden_states_matrix, labels_array = extract_hidden_states(test_subset, layer_index=4)\n",
        "\n",
        "print(f\"\\nHidden states shape: {hidden_states_matrix.shape}\")\n",
        "print(f\"Labels shape: {labels_array.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GAOZapZe5tn"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Split extracted representations into train/test sets for probe\n",
        "X_train_probe, X_test_probe, y_train_probe, y_test_probe = train_test_split(\n",
        "    hidden_states_matrix, labels_array, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train linear probe classifier\n",
        "probe_classifier = LogisticRegression(max_iter=1000)\n",
        "probe_classifier.fit(X_train_probe, y_train_probe)\n",
        "\n",
        "# Evaluate probe performance\n",
        "y_predictions_probe = probe_classifier.predict(X_test_probe)\n",
        "\n",
        "accuracy = accuracy_score(y_test_probe, y_predictions_probe)\n",
        "f1 = f1_score(y_test_probe, y_predictions_probe)\n",
        "\n",
        "print(\"=== Probe Results (Layer 4) ===\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "if accuracy > 0.80:\n",
        "    print(\"\\n✓ Layer 4 has strong toxicity representation\")\n",
        "else:\n",
        "    print(\"\\n✗ Layer 4 does not have strong toxicity representation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6x3w5-0fO3e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Extract CAV (Concept Activation Vector) from trained probe\n",
        "concept_vector = probe_classifier.coef_[0]\n",
        "intercept = probe_classifier.intercept_[0]\n",
        "\n",
        "# Project test data onto the toxicity concept vector\n",
        "projected_scores = np.dot(X_test_probe, concept_vector) + intercept\n",
        "\n",
        "# Separate scores by true label (toxic vs non-toxic)\n",
        "scores_toxic = projected_scores[y_test_probe == 1]\n",
        "scores_safe = projected_scores[y_test_probe == 0]\n",
        "\n",
        "# Create histogram visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "sns.histplot(scores_safe, color=\"green\", label=\"Non-Toxic\", kde=True, alpha=0.5)\n",
        "sns.histplot(scores_toxic, color=\"red\", label=\"Toxic\", kde=True, alpha=0.5)\n",
        "\n",
        "plt.axvline(0, color='black', linestyle='--', label=\"Decision Boundary\")\n",
        "plt.title(f\"Activation Distribution Along CAV (Layer 4)\\nAccuracy: {accuracy:.2f}, F1: {f1:.2f}\")\n",
        "plt.xlabel(\"Projection Score (higher = more toxic)\")\n",
        "plt.ylabel(\"Number of Examples\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Porównanie Metod XAI i Ewaluacja Wierności"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGCEVlF6fqPC"
      },
      "outputs": [],
      "source": [
        "import quantus\n",
        "import numpy as np\n",
        "\n",
        "def model_predict_numpy(model, inputs, **kwargs):\n",
        "    \"\"\"\n",
        "    Prediction function wrapper for Quantus (accepts numpy arrays).\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        inputs: Numpy array of token IDs [batch_size, seq_len]\n",
        "        \n",
        "    Returns:\n",
        "        Numpy array of probabilities\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    input_tensor = torch.tensor(inputs, device=device).long()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        return torch.sigmoid(outputs.logits).cpu().numpy()\n",
        "\n",
        "def explain_function_numpy(model, inputs, targets, **kwargs):\n",
        "    \"\"\"\n",
        "    Explanation function wrapper for Quantus using Integrated Gradients.\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        inputs: Numpy array of token IDs\n",
        "        targets: Array of target class indices\n",
        "        \n",
        "    Returns:\n",
        "        Numpy array of attribution scores per token\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    input_tensor = torch.tensor(inputs, device=device).long()\n",
        "    \n",
        "    # Create embeddings\n",
        "    input_embeddings = model.distilbert.embeddings(input_tensor)\n",
        "    \n",
        "    # Create baseline (padding)\n",
        "    reference_input_ids = torch.tensor(\n",
        "        [tokenizer.pad_token_id] * inputs.shape[1], \n",
        "        device=device\n",
        "    ).unsqueeze(0)\n",
        "    reference_embeddings = model.distilbert.embeddings(reference_input_ids)\n",
        "    \n",
        "    # Initialize Integrated Gradients\n",
        "    integrated_gradients = IntegratedGradients(lambda x: model(inputs_embeds=x).logits)\n",
        "    \n",
        "    # Process each example in batch\n",
        "    attributions_list = []\n",
        "    for i in range(len(inputs)):\n",
        "        target_index = int(targets[i])\n",
        "        \n",
        "        attribution = integrated_gradients.attribute(\n",
        "            inputs=input_embeddings[i].unsqueeze(0),\n",
        "            baselines=reference_embeddings,\n",
        "            target=target_index,\n",
        "            n_steps=20  # Fewer steps for faster computation\n",
        "        )\n",
        "        \n",
        "        # Sum over embedding dimension to get per-token importance\n",
        "        attribution_sum = attribution.sum(dim=-1).squeeze(0).cpu().detach().numpy()\n",
        "        attributions_list.append(attribution_sum)\n",
        "    \n",
        "    return np.array(attributions_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar39KJjKftNJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Find toxic examples in dataset\n",
        "toxic_indices = np.where(labels_array == 1)[0]\n",
        "\n",
        "# Select batch of examples for evaluation\n",
        "batch_size = 16\n",
        "selected_indices = toxic_indices[:batch_size]\n",
        "\n",
        "# Extract input IDs for selected examples\n",
        "input_batch_toxic = [test_subset[int(i)]['input_ids'] for i in selected_indices]\n",
        "target_batch = labels_array[selected_indices]\n",
        "\n",
        "print(f\"Selected {len(input_batch_toxic)} toxic examples for evaluation.\")\n",
        "\n",
        "# Configuration for faithfulness test\n",
        "top_k_tokens = 5  # Number of most important tokens to remove\n",
        "dataset_samples = input_batch_toxic\n",
        "targets = target_batch\n",
        "\n",
        "print(f\"\\n=== Faithfulness Evaluation (Comprehensiveness) ===\")\n",
        "print(f\"Testing on {len(dataset_samples)} examples\")\n",
        "print(f\"Removing {top_k_tokens} most important tokens from each sentence\\n\")\n",
        "\n",
        "comprehensiveness_scores = []\n",
        "\n",
        "# Evaluate each example\n",
        "for i in range(len(dataset_samples)):\n",
        "    # Prepare single input\n",
        "    input_id = dataset_samples[i].unsqueeze(0).to(device)\n",
        "    \n",
        "    # Get original prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        original_output = model(input_id)\n",
        "        original_probability = torch.sigmoid(original_output.logits)[0][0].item()\n",
        "    \n",
        "    # Compute attributions using Integrated Gradients\n",
        "    integrated_gradients = IntegratedGradients(predict_function)\n",
        "    \n",
        "    # Prepare embeddings\n",
        "    input_embedding = model.distilbert.embeddings(input_id)\n",
        "    baseline_embedding = model.distilbert.embeddings(\n",
        "        torch.tensor([tokenizer.pad_token_id] * input_id.size(1), device=device).unsqueeze(0)\n",
        "    )\n",
        "    \n",
        "    # Compute attributions\n",
        "    attribution, _ = integrated_gradients.attribute(\n",
        "        inputs=input_embedding,\n",
        "        baselines=baseline_embedding,\n",
        "        target=0,  # Targeting toxic class\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "    \n",
        "    # Sum attributions to token level\n",
        "    attribution_sum = attribution.sum(dim=-1).squeeze(0)\n",
        "    \n",
        "    # Find top-K most important tokens\n",
        "    _, top_indices = torch.topk(attribution_sum, k=top_k_tokens)\n",
        "    \n",
        "    # Perturb input by masking important tokens\n",
        "    perturbed_input_id = input_id.clone()\n",
        "    perturbed_input_id[0, top_indices] = tokenizer.pad_token_id\n",
        "    \n",
        "    # Get prediction on perturbed input\n",
        "    with torch.no_grad():\n",
        "        perturbed_output = model(perturbed_input_id)\n",
        "        perturbed_probability = torch.sigmoid(perturbed_output.logits)[0][0].item()\n",
        "    \n",
        "    # Compute comprehensiveness score (confidence drop)\n",
        "    confidence_drop = original_probability - perturbed_probability\n",
        "    comprehensiveness_scores.append(confidence_drop)\n",
        "    \n",
        "    # Display first example details\n",
        "    if i == 0:\n",
        "        print(f\"Example 1 - Original confidence: {original_probability:.4f}\")\n",
        "        print(f\"Example 1 - After removing top-{top_k_tokens} tokens: {perturbed_probability:.4f}\")\n",
        "        print(f\"Example 1 - Confidence drop: {confidence_drop:.4f}\")\n",
        "        removed_words = tokenizer.convert_ids_to_tokens(input_id[0, top_indices])\n",
        "        print(f\"Removed tokens: {removed_words}\\n\")\n",
        "\n",
        "# Display final results\n",
        "average_score = np.mean(comprehensiveness_scores)\n",
        "std_score = np.std(comprehensiveness_scores)\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(f\"Average Comprehensiveness Score: {average_score:.4f}\")\n",
        "print(f\"Standard Deviation: {std_score:.4f}\")\n",
        "\n",
        "if average_score > 0.1:\n",
        "    print(\"\\n✅ Integrated Gradients works well!\")\n",
        "    print(\"   Removing identified tokens significantly reduces toxicity.\")\n",
        "else:\n",
        "    print(\"\\n❌ Integrated Gradients poorly identifies important tokens.\")\n",
        "    print(\"   Model still detects toxicity after perturbation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz8ZxFLy-j23"
      },
      "outputs": [],
      "source": [
        "from captum.attr import InputXGradient\n",
        "\n",
        "# Initialize Input × Gradient method\n",
        "input_x_gradient = InputXGradient(predict_function)\n",
        "\n",
        "print(f\"Computing attributions using Input×Gradient for: {target_name}...\")\n",
        "\n",
        "# Compute attributions (no baseline needed for Input×Gradient)\n",
        "attributions_ixg = input_x_gradient.attribute(\n",
        "    inputs=input_embeddings,\n",
        "    target=target_label_index,\n",
        "    additional_forward_args=(attention_mask,)\n",
        ")\n",
        "\n",
        "# Process results for visualization\n",
        "attributions_ixg_sum = attributions_ixg.sum(dim=-1).squeeze(0)\n",
        "attributions_ixg_sum = attributions_ixg_sum / torch.norm(attributions_ixg_sum)\n",
        "attributions_ixg_numpy = attributions_ixg_sum.cpu().detach().numpy()\n",
        "\n",
        "# Create visualization data record for Input×Gradient\n",
        "visualization_data_ixg = visualization.VisualizationDataRecord(\n",
        "    word_attributions=attributions_ixg_numpy,\n",
        "    pred_prob=probability_score,\n",
        "    pred_class=predicted_class_label,\n",
        "    true_class=1,\n",
        "    attr_class=f\"{target_name} (Input×Gradient)\",\n",
        "    attr_score=attributions_ixg_numpy.sum(),\n",
        "    raw_input_ids=tokens,\n",
        "    convergence_score=None  # Input×Gradient doesn't compute convergence delta\n",
        ")\n",
        "\n",
        "print(\"\\n=== XAI Methods Comparison ===\")\n",
        "print(\"Row 1: Integrated Gradients\")\n",
        "print(\"Row 2: Input × Gradient\")\n",
        "\n",
        "# Visualize both methods side by side\n",
        "visualization.visualize_text([visualization_data, visualization_data_ixg])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Kompleksowa Analiza Wszystkich Warstw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQQkWggCADaq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_all_layers(dataset, model, device, batch_size=32):\n",
        "    \"\"\"\n",
        "    Efficiently extracts CLS embeddings from all transformer layers in a single pass.\n",
        "    \n",
        "    Args:\n",
        "        dataset: HuggingFace dataset to extract from\n",
        "        model: Transformer model with output_hidden_states enabled\n",
        "        device: PyTorch device (cuda/cpu)\n",
        "        batch_size: Batch size for processing\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (layers_dict, labels_array)\n",
        "        - layers_dict: Dictionary mapping layer_index -> numpy array of embeddings\n",
        "        - labels_array: Numpy array of labels\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Create DataLoader for efficient batching\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    # Dictionary to store activations per layer\n",
        "    # DistilBERT has: 1 embedding layer + 6 transformer layers = 7 hidden states\n",
        "    layers_data = {}\n",
        "    all_labels = []\n",
        "    \n",
        "    print(f\"Extracting from {len(dataset)} samples...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Layer Extraction\"):\n",
        "            # Move batch to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels']\n",
        "            \n",
        "            # Single forward pass retrieves all hidden states\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "            \n",
        "            # Extract CLS token (index 0) from each layer\n",
        "            for layer_index, hidden_state in enumerate(outputs.hidden_states):\n",
        "                if layer_index not in layers_data:\n",
        "                    layers_data[layer_index] = []\n",
        "                \n",
        "                # Extract CLS embeddings [batch_size, hidden_dim]\n",
        "                cls_embeddings = hidden_state[:, 0, :].cpu().numpy()\n",
        "                layers_data[layer_index].append(cls_embeddings)\n",
        "            \n",
        "            # Extract toxic label (first column)\n",
        "            if labels.dim() > 1:\n",
        "                toxic_labels = labels[:, 0].cpu().numpy()\n",
        "            else:\n",
        "                toxic_labels = labels.cpu().numpy()\n",
        "            \n",
        "            all_labels.extend(toxic_labels)\n",
        "    \n",
        "    # Concatenate all batches\n",
        "    final_layer_activations = {\n",
        "        layer: np.concatenate(data, axis=0)\n",
        "        for layer, data in layers_data.items()\n",
        "    }\n",
        "    final_labels = np.array(all_labels)\n",
        "    \n",
        "    return final_layer_activations, final_labels\n",
        "\n",
        "# Determine analysis subset size\n",
        "evaluation_subset_size = 1000\n",
        "if len(evaluation_dataset) > evaluation_subset_size:\n",
        "    analysis_dataset = evaluation_dataset.select(range(evaluation_subset_size))\n",
        "else:\n",
        "    analysis_dataset = evaluation_dataset\n",
        "\n",
        "# Extract representations from all layers\n",
        "layers_activations_dict, all_labels = extract_all_layers(\n",
        "    analysis_dataset, model, device, batch_size=32\n",
        ")\n",
        "\n",
        "print(f\"\\nExtraction complete. Layers extracted: {list(layers_activations_dict.keys())}\")\n",
        "print(f\"Layer 0 shape: {layers_activations_dict[0].shape}\")\n",
        "\n",
        "# Train probes for each layer\n",
        "probe_results = []\n",
        "\n",
        "print(\"\\nTraining linear probes for each layer...\")\n",
        "\n",
        "for layer_index in sorted(layers_activations_dict.keys()):\n",
        "    X = layers_activations_dict[layer_index]\n",
        "    y = all_labels\n",
        "    \n",
        "    # Binarize labels\n",
        "    y = (y > 0.5).astype(int)\n",
        "    \n",
        "    # Split into train/test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    # Train logistic regression probe\n",
        "    classifier = LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')\n",
        "    classifier.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate\n",
        "    y_pred = classifier.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    \n",
        "    probe_results.append({\n",
        "        'layer': layer_index,\n",
        "        'accuracy': acc,\n",
        "        'f1': f1\n",
        "    })\n",
        "    \n",
        "    print(f\"Layer {layer_index}: Accuracy={acc:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_dataframe = pd.DataFrame(probe_results)\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Plot lines for accuracy and F1\n",
        "sns.lineplot(data=results_dataframe, x='layer', y='accuracy', \n",
        "             marker='o', label='Accuracy', linewidth=2.5)\n",
        "sns.lineplot(data=results_dataframe, x='layer', y='f1', \n",
        "             marker='s', label='F1 Score', linewidth=2.5)\n",
        "\n",
        "# Format plot\n",
        "plt.title(\"Linear Separability of Toxicity Across Layers (DistilBERT)\", fontsize=14, pad=15)\n",
        "plt.xlabel(\"Layer Number (0=Embeddings, 1-6=Transformer Layers)\", fontsize=12)\n",
        "plt.ylabel(\"Metric Value\", fontsize=12)\n",
        "plt.ylim(0.0, 1.05)\n",
        "plt.xticks(results_dataframe['layer'])\n",
        "plt.legend(fontsize=11)\n",
        "\n",
        "# Add value labels above points\n",
        "for index, row in results_dataframe.iterrows():\n",
        "    plt.text(row['layer'], row['accuracy'] + 0.01, f\"{row['accuracy']:.2f}\",\n",
        "             ha='center', color='blue', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Analiza Stabilności z Generowaniem Parafraz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsUmG5UsBrc9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "# Load T5 paraphrase generation model\n",
        "print(\"Loading T5 paraphrase model...\")\n",
        "paraphrase_model_name = \"Vamsi/T5_Paraphrase_Paws\"\n",
        "paraphrase_tokenizer = AutoTokenizer.from_pretrained(paraphrase_model_name)\n",
        "paraphrase_model = AutoModelForSeq2SeqLM.from_pretrained(paraphrase_model_name).to(device)\n",
        "print(\"T5 model loaded!\")\n",
        "\n",
        "def generate_paraphrase(text, num_return_sequences=1):\n",
        "    \"\"\"\n",
        "    Generates paraphrase for given text using T5 model.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text to paraphrase\n",
        "        num_return_sequences: Number of paraphrases to generate\n",
        "        \n",
        "    Returns:\n",
        "        Paraphrased text string\n",
        "    \"\"\"\n",
        "    paraphrase_model.eval()\n",
        "    \n",
        "    # T5 requires task prefix for this specific model\n",
        "    text = \"paraphrase: \" + text + \" </s>\"\n",
        "    \n",
        "    encoding = paraphrase_tokenizer.encode_plus(\n",
        "        text,\n",
        "        padding=\"longest\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    \n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    attention_masks = encoding[\"attention_mask\"].to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = paraphrase_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_masks,\n",
        "            max_length=256,\n",
        "            do_sample=True,  # Sampling allows greater diversity\n",
        "            top_k=120,\n",
        "            top_p=0.95,\n",
        "            early_stopping=True,\n",
        "            num_return_sequences=num_return_sequences\n",
        "        )\n",
        "    \n",
        "    # Decode output\n",
        "    paraphrase = paraphrase_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return paraphrase\n",
        "\n",
        "def get_top_k_tokens(text_input, model, tokenizer, k=5):\n",
        "    \"\"\"\n",
        "    Computes Integrated Gradients attributions and returns set of k most important tokens.\n",
        "    \n",
        "    Args:\n",
        "        text_input: Input text to analyze\n",
        "        model: Classification model\n",
        "        tokenizer: Tokenizer for the model\n",
        "        k: Number of top tokens to return\n",
        "        \n",
        "    Returns:\n",
        "        Set of most important token strings\n",
        "    \"\"\"\n",
        "    # Prepare input for DistilBERT\n",
        "    inputs = tokenizer(text_input, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    input_ids = inputs.input_ids\n",
        "    attention_mask = inputs.attention_mask\n",
        "    \n",
        "    # Define prediction function for IG\n",
        "    def predict_func(inputs_embeds):\n",
        "        out = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "        return out.logits\n",
        "    \n",
        "    integrated_gradients = IntegratedGradients(predict_func)\n",
        "    \n",
        "    # Prepare embeddings\n",
        "    input_embeddings = model.distilbert.embeddings(input_ids)\n",
        "    reference_input_ids = torch.tensor(\n",
        "        [tokenizer.pad_token_id] * input_ids.size(1), \n",
        "        device=device\n",
        "    ).unsqueeze(0)\n",
        "    reference_embeddings = model.distilbert.embeddings(reference_input_ids)\n",
        "    \n",
        "    # Compute attributions (target=0 for 'toxic' class)\n",
        "    target_index = 0\n",
        "    \n",
        "    attributions, _ = integrated_gradients.attribute(\n",
        "        inputs=input_embeddings,\n",
        "        baselines=reference_embeddings,\n",
        "        target=target_index,\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "    \n",
        "    # Sum and select top-K\n",
        "    attribution_sum = attributions.sum(dim=-1).squeeze(0)\n",
        "    _, top_indices = torch.topk(attribution_sum, k=min(k, len(attribution_sum)))\n",
        "    \n",
        "    # Convert IDs to tokens\n",
        "    top_tokens = tokenizer.convert_ids_to_tokens(input_ids[0][top_indices])\n",
        "    \n",
        "    # Clean tokens (remove '##' from subwords and convert to lowercase)\n",
        "    clean_tokens = set([\n",
        "        token.replace(\"##\", \"\").lower() \n",
        "        for token in top_tokens \n",
        "        if token not in ['[CLS]', '[SEP]', '[PAD]']\n",
        "    ])\n",
        "    \n",
        "    return clean_tokens\n",
        "\n",
        "def evaluate_stability(original_text, layer_index, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Computes three stability metrics:\n",
        "    1. Output Stability - How much prediction changes\n",
        "    2. Layer Stability - Cosine similarity of representations\n",
        "    3. Attribution Stability - Jaccard similarity of important tokens\n",
        "    \n",
        "    Args:\n",
        "        original_text: Original input text\n",
        "        layer_index: Which layer to analyze\n",
        "        model: Classification model\n",
        "        tokenizer: Tokenizer for the model\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with stability metrics\n",
        "    \"\"\"\n",
        "    # Generate paraphrase\n",
        "    paraphrase_text = generate_paraphrase(original_text)\n",
        "    \n",
        "    # Prepare both texts\n",
        "    inputs_original = tokenizer(\n",
        "        original_text, return_tensors=\"pt\", truncation=True, max_length=512\n",
        "    ).to(device)\n",
        "    inputs_paraphrase = tokenizer(\n",
        "        paraphrase_text, return_tensors=\"pt\", truncation=True, max_length=512\n",
        "    ).to(device)\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    # Run model with hidden states output\n",
        "    with torch.no_grad():\n",
        "        output_original = model(**inputs_original, output_hidden_states=True)\n",
        "        output_paraphrase = model(**inputs_paraphrase, output_hidden_states=True)\n",
        "    \n",
        "    # A. Output Stability (prediction difference)\n",
        "    probability_original = torch.sigmoid(output_original.logits)[0][0].item()\n",
        "    probability_paraphrase = torch.sigmoid(output_paraphrase.logits)[0][0].item()\n",
        "    prediction_difference = abs(probability_original - probability_paraphrase)\n",
        "    \n",
        "    # B. Layer Stability (cosine similarity of CLS representations)\n",
        "    cls_original = output_original.hidden_states[layer_index][:, 0, :]  # [1, 768]\n",
        "    cls_paraphrase = output_paraphrase.hidden_states[layer_index][:, 0, :]  # [1, 768]\n",
        "    \n",
        "    cosine_similarity = F.cosine_similarity(cls_original, cls_paraphrase).item()\n",
        "    \n",
        "    # C. Attribution Stability (Jaccard Index of top tokens)\n",
        "    tokens_original = get_top_k_tokens(original_text, model, tokenizer, k=5)\n",
        "    tokens_paraphrase = get_top_k_tokens(paraphrase_text, model, tokenizer, k=5)\n",
        "    \n",
        "    # Compute Jaccard Index\n",
        "    intersection = len(tokens_original.intersection(tokens_paraphrase))\n",
        "    union = len(tokens_original.union(tokens_paraphrase))\n",
        "    jaccard_score = intersection / union if union > 0 else 0.0\n",
        "    \n",
        "    return {\n",
        "        \"Original Text\": original_text,\n",
        "        \"Paraphrase\": paraphrase_text,\n",
        "        \"Prob Original\": round(probability_original, 4),\n",
        "        \"Prob Paraphrase\": round(probability_paraphrase, 4),\n",
        "        \"Pred Diff (Output)\": round(prediction_difference, 4),\n",
        "        \"Layer Cosine Sim\": round(cosine_similarity, 4),\n",
        "        \"Attribution Jaccard\": round(jaccard_score, 4),\n",
        "        \"Top Tokens Orig\": list(tokens_original),\n",
        "        \"Top Tokens Para\": list(tokens_paraphrase)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oshWrc9Cjd5"
      },
      "outputs": [],
      "source": [
        "# Find toxic examples for stability testing\n",
        "toxic_indices = [i for i, x in enumerate(y_test_probe) if x == 1][:15]\n",
        "if len(toxic_indices) == 0:\n",
        "    print(\"No toxic samples found, using random selection...\")\n",
        "    toxic_indices = range(10)\n",
        "\n",
        "print(f\"\\nStarting stability analysis for {len(toxic_indices)} examples...\")\n",
        "print(f\"Analyzing layer: 5 (based on previous analysis results)\")\n",
        "\n",
        "stability_results = []\n",
        "\n",
        "# Evaluate stability for each example\n",
        "for idx in toxic_indices:\n",
        "    # Decode tokenized text\n",
        "    input_ids_raw = test_subset[idx]['input_ids']\n",
        "    original_text = tokenizer.decode(input_ids_raw, skip_special_tokens=True)\n",
        "    \n",
        "    # Run stability evaluation on Layer 5\n",
        "    metrics = evaluate_stability(original_text, layer_index=5, model=model, tokenizer=tokenizer)\n",
        "    stability_results.append(metrics)\n",
        "\n",
        "# Create DataFrame with results\n",
        "stability_dataframe = pd.DataFrame(stability_results)\n",
        "\n",
        "# Display results\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "display(stability_dataframe[[\n",
        "    \"Original Text\", \"Paraphrase\",\n",
        "    \"Pred Diff (Output)\", \"Layer Cosine Sim\", \"Attribution Jaccard\"\n",
        "]])\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n=== Stability Summary (Averages) ===\")\n",
        "print(f\"Mean Prediction Stability (Diff): {stability_dataframe['Pred Diff (Output)'].mean():.4f}\")\n",
        "print(f\"  (Lower is better - less variation in predictions)\")\n",
        "print(f\"Mean Layer Stability (Cosine):    {stability_dataframe['Layer Cosine Sim'].mean():.4f}\")\n",
        "print(f\"  (Higher is better - closer to 1.0 indicates more stable representations)\")\n",
        "print(f\"Mean Attribution Stability (Jacc): {stability_dataframe['Attribution Jaccard'].mean():.4f}\")\n",
        "print(f\"  (Higher is better - closer to 1.0 indicates consistent explanations)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Inżynieria Reprezentacji - Budowanie Wektorów Sterujących"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TudqH85Dkqm"
      },
      "outputs": [],
      "source": [
        "# Extract steering vector using Difference of Means method\n",
        "\n",
        "# Separate Layer 5 activations by toxic/safe labels\n",
        "layer_5_activations = layers_activations_dict[5]\n",
        "is_toxic = (all_labels > 0.5)  # Boolean mask for toxic examples\n",
        "\n",
        "# Compute centroids for both groups\n",
        "mean_toxic = np.mean(layer_5_activations[is_toxic], axis=0)\n",
        "mean_safe = np.mean(layer_5_activations[~is_toxic], axis=0)\n",
        "\n",
        "# Compute direction vector (from safe to toxic)\n",
        "direction_vector = mean_toxic - mean_safe\n",
        "\n",
        "# Analyze vector properties for debugging\n",
        "vector_norm = np.linalg.norm(direction_vector)\n",
        "hidden_state_norm = np.linalg.norm(mean_safe)\n",
        "\n",
        "print(f\"Steering vector norm (Difference of Means): {vector_norm:.4f}\")\n",
        "print(f\"Average activation norm in model: {hidden_state_norm:.4f}\")\n",
        "print(f\"Ratio: {vector_norm / hidden_state_norm:.4f}\")\n",
        "\n",
        "# Convert to PyTorch tensor (preserve natural scale)\n",
        "steering_tensor = torch.tensor(direction_vector, dtype=torch.float32).to(device)\n",
        "\n",
        "print(\"\\nSteering vector (Mean Difference) ready!\")\n",
        "\n",
        "# Determine optimal alpha scaling based on vector norm\n",
        "scale_factor = 5.0\n",
        "suggested_alpha = scale_factor\n",
        "\n",
        "print(f\"\\nSuggested alpha strength: +/- {suggested_alpha}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_with_steering(text, model, tokenizer, steering_vector, alpha=0):\n",
        "    \"\"\"\n",
        "    Makes prediction with optional steering vector intervention.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text to classify\n",
        "        model: Classification model\n",
        "        tokenizer: Tokenizer for the model\n",
        "        steering_vector: Direction vector for intervention\n",
        "        alpha: Steering strength (negative detoxifies, positive amplifies)\n",
        "        \n",
        "    Returns:\n",
        "        Probability of toxic class\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    \n",
        "    # Define hook for intervention\n",
        "    class SteeringHook:\n",
        "        def __init__(self, vector, coefficient):\n",
        "            self.vector = vector\n",
        "            self.coefficient = coefficient\n",
        "        \n",
        "        def __call__(self, module, inputs, output):\n",
        "            hidden_states = output[0]\n",
        "            steered_states = hidden_states + (self.coefficient * self.vector)\n",
        "            return (steered_states,) + output[1:]\n",
        "    \n",
        "    # Register hook on Layer 5 if alpha != 0\n",
        "    if alpha != 0:\n",
        "        hook_handle = model.distilbert.transformer.layer[5].register_forward_hook(\n",
        "            SteeringHook(steering_vector, alpha)\n",
        "        )\n",
        "    \n",
        "    # Run prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probability = torch.sigmoid(outputs.logits)[0][0].item()\n",
        "    \n",
        "    # Remove hook\n",
        "    if alpha != 0:\n",
        "        hook_handle.remove()\n",
        "    \n",
        "    return probability\n",
        "\n",
        "# Test steering on toxic sentence\n",
        "test_toxic = \"You are a complete idiot and a waste of time.\"\n",
        "\n",
        "score_original = predict_with_steering(test_toxic, model, tokenizer, steering_tensor, alpha=0)\n",
        "score_detoxified = predict_with_steering(test_toxic, model, tokenizer, steering_tensor, alpha=-suggested_alpha)\n",
        "score_amplified = predict_with_steering(test_toxic, model, tokenizer, steering_tensor, alpha=suggested_alpha)\n",
        "\n",
        "print(f\"\\nSentence: {test_toxic}\")\n",
        "print(f\"Original (Alpha 0):       {score_original:.4f}\")\n",
        "print(f\"Detoxified (Alpha -{suggested_alpha}): {score_detoxified:.4f} (expect decrease)\")\n",
        "print(f\"Amplified (Alpha +{suggested_alpha}):  {score_amplified:.4f} (expect increase)\")\n",
        "\n",
        "# Plot steering effects across alpha range\n",
        "alphas = np.linspace(-suggested_alpha * 2, suggested_alpha * 2, 10)\n",
        "scores = [predict_with_steering(test_toxic, model, tokenizer, steering_tensor, alpha=a) for a in alphas]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(alphas, scores, marker='o', color='green', linewidth=2)\n",
        "plt.axhline(0.5, color='gray', linestyle='--', label='Classification Threshold')\n",
        "plt.title(f\"Mean Difference Steering (Layer 5)\\nSentence: '{test_toxic[:30]}...'\")\n",
        "plt.xlabel(\"Alpha (Negative = Detoxification)\")\n",
        "plt.ylabel(\"Toxic Probability\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Demonstrate classification flip\n",
        "print(\"\\n=== Classification Change Demo ===\")\n",
        "base_score = predict_with_steering(test_toxic, model, tokenizer, steering_tensor, alpha=0)\n",
        "detox_score = predict_with_steering(test_toxic, model, tokenizer, steering_tensor, alpha=-15)\n",
        "\n",
        "print(f\"Sentence: {test_toxic}\")\n",
        "print(f\"Original:  {base_score:.4f} (Toxic? {base_score > 0.5})\")\n",
        "print(f\"Steered:   {detox_score:.4f} (Toxic? {detox_score > 0.5})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Testowanie Efektów Ubocznych i Ocena Bezpieczeństwa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5-sUYzAER7k"
      },
      "outputs": [],
      "source": [
        "# Set optimal alpha based on steering experiments\n",
        "optimal_alpha = -3.0\n",
        "\n",
        "# Test sentences from different categories\n",
        "test_sentences = [\n",
        "    # Toxic sentences (should be detoxified)\n",
        "    (\"You are a complete idiot and a waste of time.\", \"Toxic\"),\n",
        "    (\"I hope you fail miserably, you loser.\", \"Toxic\"),\n",
        "    \n",
        "    # Safe positive sentences (should remain unchanged)\n",
        "    (\"I really love the weather today, it is beautiful.\", \"Safe_Positive\"),\n",
        "    (\"Thank you for your help, I appreciate it.\", \"Safe_Positive\"),\n",
        "    \n",
        "    # Safe negative sentences (important edge case - should not be flagged)\n",
        "    (\"I am feeling very sad and tired today.\", \"Safe_Negative\"),\n",
        "    (\"The movie was boring and too long.\", \"Safe_Negative\"),\n",
        "    (\"I disagree with your opinion regarding the tax policy.\", \"Safe_Negative\")\n",
        "]\n",
        "\n",
        "print(f\"=== Side Effects Test (Alpha = {optimal_alpha}) ===\\n\")\n",
        "print(f\"{'Category':<20} | {'Original':<12} | {'Steered':<12} | {'Status'}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for text, category in test_sentences:\n",
        "    # Prediction without intervention\n",
        "    probability_original = predict_with_steering(text, model, tokenizer, steering_tensor, alpha=0)\n",
        "    \n",
        "    # Prediction with detoxification\n",
        "    probability_steered = predict_with_steering(text, model, tokenizer, steering_tensor, alpha=optimal_alpha)\n",
        "    \n",
        "    # Evaluate results\n",
        "    if category == \"Toxic\":\n",
        "        # For toxic: success if drops below 0.5\n",
        "        if probability_steered < 0.1:\n",
        "            status = \"✅ Fixed\"\n",
        "        elif probability_steered < 0.5:\n",
        "            status = \"⚠️  Improved\"\n",
        "        else:\n",
        "            status = \"❌ Failed\"\n",
        "    else:\n",
        "        # For safe: check that it doesn't break (shouldn't become toxic)\n",
        "        change = abs(probability_original - probability_steered)\n",
        "        if probability_steered > 0.5:\n",
        "            status = \"❌ BROKEN (False Positive)\"\n",
        "        elif change < 0.2:\n",
        "            status = \"✅ Stable\"\n",
        "        else:\n",
        "            status = \"⚠️  Shifted\"\n",
        "    \n",
        "    print(f\"{category:<20} | {probability_original:.4f}       | {probability_steered:.4f}       | {status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Wdrożenie Produkcyjne - Zapisywanie i Ładowanie Artefaktów Sterowania"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "# Create steering artifact for production use\n",
        "steering_artifact = {\n",
        "    \"steering_vector\": steering_tensor.cpu(),  # Move to CPU for saving\n",
        "    \"layer_index\": 5,                          # Target layer\n",
        "    \"alpha\": optimal_alpha,                    # Optimal steering strength (-3.0)\n",
        "    \"method\": \"mean_difference\",\n",
        "    \"model_name\": \"distilbert-base-uncased\",\n",
        "    \"description\": \"Vector for removing toxicity concept from layer 5\"\n",
        "}\n",
        "\n",
        "# Save with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "save_path = f\"/drive/MyDrive/msc-project/vectors/toxicity_steering_controller_{timestamp}.pt\"\n",
        "\n",
        "torch.save(steering_artifact, save_path)\n",
        "print(f\"✅ Steering artifact saved to: {save_path}\")\n",
        "\n",
        "# ============================================================\n",
        "# PRODUCTION SIMULATION - Clean session without training data\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n=== Production Environment Simulation ===\")\n",
        "\n",
        "# Load artifact\n",
        "artifact = torch.load(save_path)\n",
        "loaded_vector = artifact[\"steering_vector\"].to(device)\n",
        "loaded_layer = artifact[\"layer_index\"]\n",
        "loaded_alpha = artifact[\"alpha\"]\n",
        "\n",
        "print(f\"Loaded controller: {artifact['description']}\")\n",
        "print(f\"Configuration: Layer {loaded_layer}, Alpha {loaded_alpha}\")\n",
        "\n",
        "# Define production hook class\n",
        "class ProductionSteeringHook:\n",
        "    \"\"\"Hook for applying steering vector in production inference.\"\"\"\n",
        "    def __init__(self, vector, coefficient):\n",
        "        self.vector = vector\n",
        "        self.coefficient = coefficient\n",
        "    \n",
        "    def __call__(self, module, inputs, output):\n",
        "        hidden_states = output[0]\n",
        "        steered_states = hidden_states + (self.coefficient * self.vector)\n",
        "        return (steered_states,) + output[1:]\n",
        "\n",
        "# Production inference function\n",
        "def generate_safe_prediction(text, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Run inference with built-in detoxification.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text to classify\n",
        "        model: Classification model\n",
        "        tokenizer: Tokenizer for the model\n",
        "        \n",
        "    Returns:\n",
        "        Toxic probability after steering\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "    \n",
        "    # Register steering hook\n",
        "    hook = model.distilbert.transformer.layer[loaded_layer].register_forward_hook(\n",
        "        ProductionSteeringHook(loaded_vector, loaded_alpha)\n",
        "    )\n",
        "    \n",
        "    # Run prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probability = torch.sigmoid(outputs.logits)[0][0].item()\n",
        "    \n",
        "    # Clean up hook\n",
        "    hook.remove()\n",
        "    \n",
        "    return probability\n",
        "\n",
        "# Live test\n",
        "live_test_text = \"You are completely useless and stupid.\"\n",
        "safety_score = generate_safe_prediction(live_test_text, model, tokenizer)\n",
        "\n",
        "print(f\"\\nLive Test Input: '{live_test_text}'\")\n",
        "print(f\"Model Toxic Probability (Steered): {safety_score:.4f}\")\n",
        "print(f\"Decision: {'🔴 BLOCK' if safety_score > 0.5 else '🟢 ALLOW'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Podsumowanie Eksperymentu: Inżynieria Reprezentacji dla Detoksykacji Modelu\n",
        "\n",
        "Ten projekt demonstruje kompleksową analizę i modyfikację wewnętrznych reprezentacji w modelu DistilBERT (wytrenowanym na danych Jigsaw Toxicity) w celu kontrolowania jego zachowania bez ponownego treningu.\n",
        "\n",
        "### Zrealizowane Etapy:\n",
        "\n",
        "#### 1. Analiza Warstwowa\n",
        "- Zbadano liniową separowalność koncepcji \"toksyczność\" w całej sieci neuronowej\n",
        "- Zidentyfikowano **Warstwę 5** jako optymalny punkt interwencji (F1 Score = 0.80)\n",
        "- Ta warstwa przewyższyła warstwę końcową pod względem jakości reprezentacji, wskazując \"sweet spot\" dla rozumienia semantycznego\n",
        "\n",
        "#### 2. Analiza Stabilności\n",
        "- Użyto generatora parafraz T5 do testowania robustności reprezentacji\n",
        "- Wykazano, że aktywacje Warstwy 5 są niezwykle stabilne semantycznie (Podobieństwo Cosine > 0.99)\n",
        "- Potwierdzono, że reprezentacje pozostają spójne nawet gdy struktura zdania się zmienia, walidując wykonalność interwencji\n",
        "\n",
        "#### 3. Ekstrakcja Wektora Sterującego\n",
        "- Zastosowano metodę **Difference of Means** do obliczenia wektora kierunkowego między centroidami aktywacji toksycznych i bezpiecznych w Warstwie 5\n",
        "- To podejście okazało się bardziej skuteczne niż wagi regresji logistycznej, zapewniając odpowiednie skalowanie sygnału\n",
        "- Pomyślnie przechwycono kierunek koncepcji toksyczności w przestrzeni reprezentacji\n",
        "\n",
        "#### 4. Sterowanie Modelem i Interwencja\n",
        "- Zaimplementowano mechanizm PyTorch Forward Hook dla wstrzykiwania wektora sterującego w czasie rzeczywistym\n",
        "- Zastosowano interwencję z siłą Alpha = -3.0 dla efektywnej \"detoksykacji\" modelu\n",
        "- Osiągnięto kontrolowaną redukcję detekcji toksyczności bez ponownego treningu modelu\n",
        "\n",
        "#### 5. Ewaluacja i Zapewnienie Jakości\n",
        "\n",
        "**Skuteczność:**\n",
        "- Prawdopodobieństwo toksyczności dla obraźliwych fraz spadło z ~92% do ~1-4%\n",
        "\n",
        "**Bezpieczeństwo:**\n",
        "- Model zachował poprawne zachowanie dla zdań neutralnych i pozytywnych (brak efektu \"lobotomii\")\n",
        "\n",
        "**Redukcja Fałszywych Pozytywów:**\n",
        "- Wyeliminowano niepoprawne oznaczanie zdań o negatywnym sentymencie (np. skargi) jako toksyczne\n",
        "- Wskaźnik fałszywych pozytywów spadł z 10% do 0%\n",
        "\n",
        "### Wnioski\n",
        "\n",
        "Ten projekt potwierdza, że **Inżynieria Reprezentacji (RepE)** jest potężną, niskokosztową metodą kontrolowania zachowania modeli LLM/BERT. Poprzez precyzyjne operacje na wektorach aktywacji w Warstwie 5, pomyślnie wyeliminowaliśmy niepożądane zachowanie modelu (detekcję toksyczności) zachowując jego ogólne możliwości językowe.\n",
        "\n",
        "Podejście demonstruje, że manipulacja wewnętrznymi reprezentacjami oferuje realną alternatywę dla kosztownego ponownego treningu, umożliwiając precyzyjną kontrolę nad wyjściami modelu poprzez celowane interwencje w przestrzeni aktywacji.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPsX7S8R1dubpjAsshSJLte",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
