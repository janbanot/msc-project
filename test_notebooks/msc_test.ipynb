{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZjeLrnSaFPVw3FELL/nqa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-project/blob/main/test_notebooks/msc_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N9kkURRXxcaG"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers datasets captum quantus accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "metadata": {
        "id": "rnzL_6SCy5qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7bbe3ea"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_path = '/drive/MyDrive/msc-project/jigsaw-toxic-comment/train.csv'\n",
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(\"CSV file loaded successfully!\")\n",
        "    display(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file was not found at {csv_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(example):\n",
        "    \"\"\"Applies all cleaning steps to the 'comment_text' field.\"\"\"\n",
        "\n",
        "    # 1. Get the text\n",
        "    text = example['comment_text']\n",
        "\n",
        "    # 2. Lowercasing\n",
        "    # This is crucial for \"uncased\" BERT models\n",
        "    text = text.lower()\n",
        "\n",
        "    # 3. Remove URLs\n",
        "    # re.sub finds a pattern and replaces it\n",
        "    # r'http\\S+' finds 'http' followed by any non-space characters\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # 4. Remove IP Addresses\n",
        "    # \\d{1,3} means \"a digit, 1-to-3 times\". \\. means \"a literal dot\".\n",
        "    text = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', '', text)\n",
        "\n",
        "    # 5. Remove Wikipedia metadata like (talk), timestamps, etc.\n",
        "    # This is a simple regex to find things like (talk)\n",
        "    # You could make this more complex, but this is a good start.\n",
        "    text = re.sub(r'\\(talk\\)', '', text)\n",
        "    text = re.sub(r'\\d{2}:\\d{2}, \\w+ \\d{1,2}, \\d{4} \\(utc\\)', '', text)\n",
        "\n",
        "    # 6. Remove newlines and other special characters\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = text.replace('\\xa0', ' ')\n",
        "\n",
        "    # 7. Remove any text inside double quotes at the start/end\n",
        "    # This removes things like '\"\\n\\n ' from the beginning\n",
        "    text = text.strip(' \"')\n",
        "\n",
        "    # 8. Clean up whitespace\n",
        "    # \\s+ means \"one or more space characters\"\n",
        "    # We replace any group of spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # 9. Update the example\n",
        "    example['comment_text'] = text\n",
        "    return example"
      ],
      "metadata": {
        "id": "Baob8jOTYaBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "train_df = df.head(2000)\n",
        "data = datasets.Dataset.from_pandas(train_df)"
      ],
      "metadata": {
        "id": "XMcBMwOZZOYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCleaning data...\")\n",
        "cleaned_data = data.map(clean_text)\n",
        "print(\"Data cleaned!\")"
      ],
      "metadata": {
        "id": "1XSauFnTY4GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- BEFORE CLEANING ---\")\n",
        "print(data[1]['comment_text'])\n",
        "print(\"\\n\" + data[6]['comment_text'])\n",
        "print(\"\\n\" + data[0]['comment_text'])\n",
        "\n",
        "print(\"\\n\\n--- AFTER CLEANING ---\")\n",
        "print(cleaned_data[1]['comment_text'])\n",
        "print(\"\\n\" + cleaned_data[6]['comment_text'])\n",
        "print(\"\\n\" + cleaned_data[0]['comment_text'])"
      ],
      "metadata": {
        "id": "3Eij-Oi0ZTkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# \"model card\"\n",
        "# 'uncased' matches the .lower() step we did earlier.\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "\n",
        "try:\n",
        "    # This downloads and caches the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "    print(\"Tokenizer loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer: {e}\")"
      ],
      "metadata": {
        "id": "IdmXlYxUczCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Applies the tokenizer to a batch of text.\"\"\"\n",
        "\n",
        "    # This is the main tokenization step.\n",
        "    # padding=\"max_length\" fills short comments with [PAD] tokens.\n",
        "    # truncation=True cuts off comments that are too long.\n",
        "    # max_length=256 is a good balance of speed and context for comments.\n",
        "    # Could use 512 (DistilBERT's max) but it's slower.\n",
        "    return tokenizer(\n",
        "        examples[\"comment_text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "# Apply the function with .map()\n",
        "# batched=True makes it MUCH faster by tokenizing many texts at once.\n",
        "print(\"\\nTokenizing data...\")\n",
        "tokenized_data = cleaned_data.map(tokenize_function, batched=True)\n",
        "print(\"Data tokenized!\")"
      ],
      "metadata": {
        "id": "7di3UMZHdB1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Example of a Tokenized Entry ---\")\n",
        "print(tokenized_data[0])"
      ],
      "metadata": {
        "id": "Mn-sppLedLSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Define label columns in the correct order\n",
        "label_columns = [\n",
        "    'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'\n",
        "]\n",
        "\n",
        "def create_labels_column(example):\n",
        "    \"\"\"\n",
        "    Creates a new 'labels' column by combining the 6 label columns.\n",
        "    We convert them to float32, which is what ML models expect.\n",
        "    \"\"\"\n",
        "    # For each example, build a list of its label values\n",
        "    labels_list = [float(example[col]) for col in label_columns]\n",
        "    example['labels'] = labels_list\n",
        "    return example\n",
        "\n",
        "# 2. Apply the function\n",
        "print(\"\\nConsolidating labels...\")\n",
        "final_data = tokenized_data.map(create_labels_column)\n",
        "print(\"Labels consolidated!\")\n",
        "\n",
        "# 3. Let's see the result for a toxic comment\n",
        "print(\"\\n--- Example of a Processed Entry ---\")\n",
        "print(final_data[6])"
      ],
      "metadata": {
        "id": "c8F9I2V2d-I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. List all columns to be removed\n",
        "columns_to_remove = [\n",
        "    'id', 'comment_text', 'toxic', 'severe_toxic',\n",
        "    'obscene', 'threat', 'insult', 'identity_hate'\n",
        "]\n",
        "\n",
        "print(f\"\\nOriginal columns: {final_data.column_names}\")\n",
        "final_data = final_data.remove_columns(columns_to_remove)\n",
        "print(f\"Cleaned columns: {final_data.column_names}\")\n",
        "\n",
        "# 2. Set the dataset format to \"torch\" (for PyTorch)\n",
        "try:\n",
        "    final_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "    print(\"\\nDataset format set to 'torch'!\")\n",
        "except ImportError:\n",
        "    print(\"\\nPyTorch not installed. Skipping .set_format('torch').\")\n",
        "    print(\"Please install with: pip install torch\")\n",
        "\n",
        "print(\"\\n--- Final, Model-Ready Item ---\")\n",
        "print(final_data[6])"
      ],
      "metadata": {
        "id": "GdvRwT_leOM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "num_labels = 6 # 6 toxic categories\n",
        "\n",
        "# Load the model, configuring it for multi-label classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels,\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(\"Model configured for multi-label classification.\")"
      ],
      "metadata": {
        "id": "RY2kk5asekT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_splits = final_data.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "train_dataset = data_splits['train']\n",
        "eval_dataset = data_splits['test']\n",
        "\n",
        "print(f\"\\nData split complete:\")\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Evaluation samples: {len(eval_dataset)}\")"
      ],
      "metadata": {
        "id": "7EUdT7kYeyof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "from transformers import EvalPrediction\n",
        "import torch\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    # p.predictions are the raw logit outputs\n",
        "    # p.label_ids are the true labels\n",
        "\n",
        "    # Apply sigmoid to logits to get probabilities\n",
        "    logits = p.predictions\n",
        "    # Sigmoid function\n",
        "    probs = 1 / (1 + np.exp(-logits))\n",
        "\n",
        "    # Set a threshold (0.5) to get binary predictions\n",
        "    threshold = 0.5\n",
        "    predictions = (probs > threshold).astype(int)\n",
        "\n",
        "    # Compute the metrics\n",
        "    labels = p.label_ids\n",
        "\n",
        "    # Use 'micro' averaging, which is good for imbalanced labels\n",
        "    f1_micro = f1_score(labels, predictions, average='micro')\n",
        "\n",
        "    # This measures how many individual labels (out of 6*num_samples) were correct\n",
        "    overall_accuracy = accuracy_score(labels.flatten(), predictions.flatten())\n",
        "\n",
        "    # Return metrics as a dictionary\n",
        "    return {\n",
        "        'f1_micro': f1_micro,\n",
        "        'accuracy': overall_accuracy\n",
        "    }"
      ],
      "metadata": {
        "id": "NX52g5ElfBO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "model_output_dir = \"/drive/MyDrive/msc-project/models/distilbert-jigsaw-finetuned\"\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_output_dir,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    # helps prevent overfitting\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=5,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_micro\",\n",
        "    # DISABLE WANDB\n",
        "    report_to=\"none\",\n",
        ")"
      ],
      "metadata": {
        "id": "Gd4r8HXrfO8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"\\n--- Starting Training ---\")\n",
        "trainer.train()\n",
        "print(\"--- Training Complete ---\")"
      ],
      "metadata": {
        "id": "hgN5hlo3fowE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_directory = \"/drive/MyDrive/msc-project/models/final_distilbert_jigsaw\"\n",
        "trainer.save_model(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "print(f\"Model and tokenizer saved in: {save_directory}\")"
      ],
      "metadata": {
        "id": "BlksX5w2b_-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "text = \"you are a fucking moron, who should die in hell but I love your lovely kitten\"\n",
        "\n",
        "# Tokenization\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "# Inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    # Use SIGMOID dla multi-label\n",
        "    probs = torch.sigmoid(logits)\n",
        "\n",
        "# Display results\n",
        "labels_list = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "print(f\"Text: '{text}'\\n\")\n",
        "print(\"Probabilities:\")\n",
        "for label, prob in zip(labels_list, probs[0]):\n",
        "    print(f\"{label}: {prob:.4f}\")"
      ],
      "metadata": {
        "id": "bjKLCf6bcgqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from captum.attr import IntegratedGradients\n",
        "\n",
        "# 1. Captum wrapper\n",
        "def predict_func(inputs_embeds, attention_mask=None):\n",
        "    output = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "    return output.logits\n",
        "\n",
        "# 2. Simple Integrated Gradients init\n",
        "ig = IntegratedGradients(predict_func)\n",
        "\n",
        "# 3. Label selection\n",
        "# 0=toxic, 1=severe_toxic, 2=obscene, 3=threat, 4=insult, 5=identity_hate\n",
        "TARGET_LABEL_INDEX = 0\n",
        "target_name = labels_list[TARGET_LABEL_INDEX]\n",
        "\n",
        "# A. Text vectors\n",
        "input_ids = inputs.input_ids\n",
        "# Take vectors (floats) from embedding layer\n",
        "input_embeddings = model.distilbert.embeddings(input_ids)\n",
        "\n",
        "# B. Background vectors (Baseline - padding)\n",
        "# Create tensor ID padding with the same length as input\n",
        "ref_input_ids = torch.tensor([tokenizer.pad_token_id] * input_ids.size(1), device=device).unsqueeze(0)\n",
        "# Change to vectors\n",
        "ref_input_embeddings = model.distilbert.embeddings(ref_input_ids)\n",
        "\n",
        "# C. Attention mask (model must know what is padding)\n",
        "attention_mask = inputs.attention_mask\n",
        "\n",
        "# 5. Attribution calculation\n",
        "print(f\"Attribution calculation: {target_name}...\")\n",
        "\n",
        "attributions, delta = ig.attribute(\n",
        "    inputs=input_embeddings,         # Pass prepared vectors\n",
        "    baselines=ref_input_embeddings,  # Pass background vectors\n",
        "    target=TARGET_LABEL_INDEX,\n",
        "    additional_forward_args=(attention_mask,), # Pass attention mask\n",
        "    return_convergence_delta=True\n",
        ")"
      ],
      "metadata": {
        "id": "UVYwRbBVdZB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Results processing for visualisation\n",
        "attributions_sum = attributions.sum(dim=-1).squeeze(0)\n",
        "attributions_sum = attributions_sum / torch.norm(attributions_sum)\n",
        "attributions_np = attributions_sum.cpu().detach().numpy()\n",
        "\n",
        "# Get probability for given label\n",
        "prob_score = probs[0][TARGET_LABEL_INDEX].item()\n",
        "pred_class_label = \"True\" if prob_score > 0.5 else \"False\"\n",
        "\n",
        "# Get tokens\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "vis_data = visualization.VisualizationDataRecord(\n",
        "    word_attributions=attributions_np,\n",
        "    pred_prob=prob_score,       # Label probability\n",
        "    pred_class=pred_class_label, # Did it pass the threshold?\n",
        "    true_class=1,               # Assume that text is toxic\n",
        "    attr_class=target_name,     # Label name (np. 'toxic')\n",
        "    attr_score=attributions_np.sum(),\n",
        "    raw_input_ids=tokens,\n",
        "    convergence_score=delta\n",
        ")\n",
        "\n",
        "print(f\"\\nLabel explaination: {target_name}\")\n",
        "visualization.visualize_text([vis_data])"
      ],
      "metadata": {
        "id": "k0ur6jyFd7E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Model congi\n",
        "model.config.output_hidden_states = True\n",
        "\n",
        "# 2. Extraction function\n",
        "def extract_hidden_states(data_subset, layer_index=4):\n",
        "    model.eval()\n",
        "    all_hidden_states = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(f\"Extract data from layer: {layer_index}...\")\n",
        "\n",
        "    for i in tqdm(range(len(data_subset))):\n",
        "        entry = data_subset[i]\n",
        "\n",
        "        text = entry['input_ids'].unsqueeze(0).to(device)\n",
        "        mask = entry['attention_mask'].unsqueeze(0).to(device)\n",
        "        label = entry['labels'][0].item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(text, attention_mask=mask)\n",
        "            hidden_state = outputs.hidden_states[layer_index]\n",
        "            cls_embedding = hidden_state[0, 0, :].cpu().numpy()\n",
        "\n",
        "            all_hidden_states.append(cls_embedding)\n",
        "            all_labels.append(label)\n",
        "\n",
        "    return np.array(all_hidden_states), np.array(all_labels)\n",
        "\n",
        "# Check data size and take max\n",
        "total_eval_samples = len(eval_dataset)\n",
        "target_size = 500\n",
        "subset_size = min(target_size, total_eval_samples)\n",
        "\n",
        "print(f\"Dostępnych próbek: {total_eval_samples}. Używam: {subset_size}\")\n",
        "\n",
        "test_subset = eval_dataset.select(range(subset_size))\n",
        "\n",
        "# Extraction\n",
        "X_hidden, y_labels = extract_hidden_states(test_subset, layer_index=4)\n",
        "\n",
        "print(f\"\\nKształt danych X: {X_hidden.shape}\")\n",
        "print(f\"Kształt danych y: {y_labels.shape}\")"
      ],
      "metadata": {
        "id": "exduABBQeocg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# 1. Split extracted data for training and test sets\n",
        "X_train_probe, X_test_probe, y_train_probe, y_test_probe = train_test_split(\n",
        "    X_hidden, y_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 2. Create and test simple probe\n",
        "# Increase the max_iter to make it happen\n",
        "probe = LogisticRegression(max_iter=1000)\n",
        "probe.fit(X_train_probe, y_train_probe)\n",
        "\n",
        "# 3. Check how probe sees the toxicity in the layer\n",
        "y_pred_probe = probe.predict(X_test_probe)\n",
        "\n",
        "acc = accuracy_score(y_test_probe, y_pred_probe)\n",
        "f1 = f1_score(y_test_probe, y_pred_probe)\n",
        "\n",
        "print(f\"--- Probe results (Layer 4) ---\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Interpretacja\n",
        "if acc > 0.80:\n",
        "    print(\"Layer 4 has strong representation of toxicity\")\n",
        "else:\n",
        "    print(\"Layer 4 does not have strong representation of toxicity\")"
      ],
      "metadata": {
        "id": "1GAOZapZe5tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Take CAV vector from trained probe\n",
        "# Logistic regression weights [1, 768]\n",
        "cav_vector = probe.coef_[0]\n",
        "intercept = probe.intercept_[0]\n",
        "\n",
        "# 2. We project the data onto this vector (dot product)\n",
        "# This will tell us how much each sentence lies “along” the direction of toxicity\n",
        "# We multiply the representation matrix (X_test_probe) by the CAV vector\n",
        "projected_scores = np.dot(X_test_probe, cav_vector) + intercept\n",
        "\n",
        "# 3. Preparing the data for the plot\n",
        "# We split the results into the toxic group (1) and the safe group (0) based on the true labels\n",
        "scores_toxic = projected_scores[y_test_probe == 1]\n",
        "scores_safe = projected_scores[y_test_probe == 0]\n",
        "\n",
        "# 4. Histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "sns.histplot(scores_safe, color=\"green\", label=\"Non-Toxic\", kde=True, alpha=0.5)\n",
        "\n",
        "sns.histplot(scores_toxic, color=\"red\", label=\"Toxic\", kde=True, alpha=0.5)\n",
        "\n",
        "plt.axvline(0, color='black', linestyle='--', label=\"Decision Boundary (Probe)\")\n",
        "plt.title(f\"Distribution of activations along the CAV vector (Layer 4)\\nAccuracy: {acc:.2f}, F1: {f1:.2f}\")\n",
        "plt.xlabel(\"Projection score (The further to the right, the more 'toxic' according to the layer)\")\n",
        "plt.ylabel(\"Number of examples\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e6x3w5-0fO3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import quantus\n",
        "import numpy as np\n",
        "\n",
        "# 1. Prediction function for Quantus\n",
        "# Quantus provides data as a numpy array, so we need to convert it into Tensors\n",
        "def model_predict_numpy(model, inputs, **kwargs):\n",
        "    model.eval()\n",
        "    # 'inputs' here is a matrix of token IDs [batch_size, seq_len]\n",
        "    input_tensor = torch.tensor(inputs, device=device).long()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        # We return probabilities (Softmax/Sigmoid) as numpy\n",
        "        return torch.sigmoid(outputs.logits).cpu().numpy()\n",
        "\n",
        "# 2. Explanation function for Quantus (Integrated Gradients)\n",
        "def explain_func_numpy(model, inputs, targets, **kwargs):\n",
        "    # Wrapper that runs your IG code inside Quantus\n",
        "    model.eval()\n",
        "    input_tensor = torch.tensor(inputs, device=device).long()\n",
        "\n",
        "    # Create embeddings (as we fixed earlier)\n",
        "    input_embeddings = model.distilbert.embeddings(input_tensor)\n",
        "\n",
        "    # Baseline (padding)\n",
        "    ref_input_ids = torch.tensor([tokenizer.pad_token_id] * inputs.shape[1], device=device).unsqueeze(0)\n",
        "    ref_input_embeddings = model.distilbert.embeddings(ref_input_ids)\n",
        "\n",
        "    # IG\n",
        "    ig = IntegratedGradients(lambda x: model(inputs_embeds=x).logits)\n",
        "\n",
        "    # Important: loop over the batch (Quantus sometimes provides multiple examples at once)\n",
        "    attributions_list = []\n",
        "    for i in range(len(inputs)):\n",
        "        # Target (which class?)\n",
        "        target_idx = int(targets[i])\n",
        "\n",
        "        attr = ig.attribute(\n",
        "            inputs=input_embeddings[i].unsqueeze(0),\n",
        "            baselines=ref_input_embeddings,\n",
        "            target=target_idx,\n",
        "            n_steps=20 # Fewer steps for faster testing\n",
        "        )\n",
        "        # Sum attributions into a single value per token\n",
        "        attr_sum = attr.sum(dim=-1).squeeze(0).cpu().detach().numpy()\n",
        "        attributions_list.append(attr_sum)\n",
        "\n",
        "    return np.array(attributions_list)"
      ],
      "metadata": {
        "id": "AGCEVlF6fqPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TOP_K_TOKENS = 5   # How many most important words do we remove?\n",
        "dataset_samples = x_batch_toxic  # We take our 16 toxic sentences\n",
        "targets = y_batch_targets        # Our labels\n",
        "\n",
        "print(f\"--- Manual Faithfulness Evaluation (Comprehensiveness) ---\")\n",
        "print(f\"Test on {len(dataset_samples)} examples.\")\n",
        "print(f\"Removing {TOP_K_TOKENS} most important words from each sentence.\\n\")\n",
        "\n",
        "scores = []\n",
        "\n",
        "# Loop over each example\n",
        "for i in range(len(dataset_samples)):\n",
        "    # 1. Prepare a single input\n",
        "    input_id = torch.tensor([dataset_samples[i]], device=device) # Shape [1, seq_len]\n",
        "\n",
        "    # 2. Original prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        orig_output = model(input_id)\n",
        "        orig_prob = torch.sigmoid(orig_output.logits)[0][0].item()  # Probability of class 'Toxic'\n",
        "\n",
        "    # 3. Compute attributions (IG) for this example\n",
        "    # (Using your existing IG object; assuming 'ig' is defined earlier)\n",
        "    # If not, uncomment the line below:\n",
        "    ig = IntegratedGradients(predict_func)\n",
        "\n",
        "    # Prepare embeddings\n",
        "    input_emb = model.distilbert.embeddings(input_id)\n",
        "    baseline_emb = model.distilbert.embeddings(\n",
        "        torch.tensor([tokenizer.pad_token_id] * input_id.size(1), device=device).unsqueeze(0)\n",
        "    )\n",
        "\n",
        "    # Compute attributions\n",
        "    attributions, _ = ig.attribute(\n",
        "        inputs=input_emb,\n",
        "        baselines=baseline_emb,\n",
        "        target=0,  # Targeting the Toxic class\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "\n",
        "    # Sum attributions to token level\n",
        "    attr_sum = attributions.sum(dim=-1).squeeze(0)  # [seq_len]\n",
        "\n",
        "    # 4. Find TOP-K most important tokens\n",
        "    # torch.topk returns values and indices\n",
        "    _, top_indices = torch.topk(attr_sum, k=TOP_K_TOKENS)\n",
        "\n",
        "    # 5. PERTURBATION (Remove words)\n",
        "    # Copy the input and replace important words with padding (or mask)\n",
        "    perturbed_input_id = input_id.clone()\n",
        "    # Insert PAD (id: 0) in the positions of the most important words\n",
        "    perturbed_input_id[0, top_indices] = tokenizer.pad_token_id\n",
        "\n",
        "    # 6. New prediction on the \"censored\" text\n",
        "    with torch.no_grad():\n",
        "        pert_output = model(perturbed_input_id)\n",
        "        pert_prob = torch.sigmoid(pert_output.logits)[0][0].item()\n",
        "\n",
        "    # 7. Compute the score (Comprehensiveness)\n",
        "    # How much did the model confidence drop?\n",
        "    drop = orig_prob - pert_prob\n",
        "    scores.append(drop)\n",
        "\n",
        "    # Optional: print preview for the first element\n",
        "    if i == 0:\n",
        "        print(f\"Example 1 - Original confidence: {orig_prob:.4f}\")\n",
        "        print(f\"Example 1 - After removing top-{TOP_K_TOKENS} words: {pert_prob:.4f}\")\n",
        "        print(f\"Example 1 - Drop (Score): {drop:.4f}\")\n",
        "        removed_words = tokenizer.convert_ids_to_tokens(input_id[0, top_indices])\n",
        "        print(f\"Removed words: {removed_words}\\n\")\n",
        "\n",
        "# --- FINAL RESULTS ---\n",
        "avg_score = np.mean(scores)\n",
        "std_score = np.std(scores)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Average Comprehensiveness score: {avg_score:.4f}\")\n",
        "print(f\"Standard deviation: {std_score:.4f}\")\n",
        "\n",
        "if avg_score > 0.1:\n",
        "    print(\"\\n✅ CONCLUSION: IG works! Removing the identified words significantly reduces toxicity.\")\n",
        "else:\n",
        "    print(\"\\n❌ CONCLUSION: IG poorly identifies important words (the model still thinks it's toxic).\")\n"
      ],
      "metadata": {
        "id": "ar39KJjKftNJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}