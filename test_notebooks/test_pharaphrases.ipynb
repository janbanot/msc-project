{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "727c9b36",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-project/blob/main/test_notebooks/test_pharaphrases.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87cb3184",
      "metadata": {},
      "source": [
        "# Test Parafraz T5\n",
        "\n",
        "Ten notatnik testuje jakość parafraz generowanych przez model T5 (Vamsi/T5_Paraphrase_Paws).\n",
        "\n",
        "**Cel:**\n",
        "- Wczytać 20 toksycznych przykładów z datasetu\n",
        "- Wygenerować parafrazy używając T5\n",
        "- Porównać oryginał vs parafraza\n",
        "- Wyświetlić metryki jakości (cosine similarity, zachowanie toksyczności)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e415da8f",
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install transformers datasets torch pandas matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36f213fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForSeq2SeqLM,\n",
        ")\n",
        "\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "pd.set_option('display.width', 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48c56057",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73b55fb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# KONFIGURACJA\n",
        "# ===================================================\n",
        "\n",
        "# Parametry\n",
        "N_SAMPLES = 20  # Liczba próbek do testowania\n",
        "MAX_SEQUENCE_LENGTH = 256  # Maksymalna długość sekwencji\n",
        "TARGET_LAYER_INDEX = 5  # Warstwa do ekstrakcji reprezentacji\n",
        "PARAPHRASE_SEED = 42  # Seed dla reproducibility\n",
        "PARAPHRASE_MIN_SIMILARITY = 0.7  # Minimalny próg jakości parafrazy\n",
        "\n",
        "# Ścieżki\n",
        "DATA_PATH = \"/drive/MyDrive/msc-project/jigsaw-toxic-comment/train.csv\"\n",
        "MODEL_CHECKPOINT = \"/drive/MyDrive/msc-project/models/distilbert-jigsaw-full\"\n",
        "\n",
        "# Urządzenie\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Uruchomiono na urządzeniu: {device}\")\n",
        "\n",
        "# Ustawienie seed\n",
        "torch.manual_seed(PARAPHRASE_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(PARAPHRASE_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1054f377",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# ŁADOWANIE MODELI\n",
        "# ===================================================\n",
        "\n",
        "print(\">>> Ładowanie modelu klasyfikacji toksyczności...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_CHECKPOINT, num_labels=1, problem_type=\"single_label_classification\"\n",
        "    )\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(\"✓ Model DistilBERT załadowany\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Błąd ładowania modelu: {e}\")\n",
        "    raise\n",
        "\n",
        "print(\"\\n>>> Ładowanie modelu T5 do generowania parafraz...\")\n",
        "t5_name = \"Vamsi/T5_Paraphrase_Paws\"\n",
        "t5_tokenizer = AutoTokenizer.from_pretrained(t5_name)\n",
        "t5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_name).to(device)\n",
        "t5_model.eval()\n",
        "print(\"✓ Model T5 załadowany\")\n",
        "\n",
        "print(\"\\n>>> Modele gotowe!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0818824",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# ŁADOWANIE DANYCH\n",
        "# ===================================================\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Czyści tekst (zgodnie z preprocessing z głównego notatnika).\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \"\", text)\n",
        "    text = re.sub(r\"\\(talk\\)\", \"\", text)\n",
        "    text = re.sub(r\"\\d{2}:\\d{2}, \\w+ \\d{1,2}, \\d{4} \\(utc\\)\", \"\", text)\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\xa0\", \" \")\n",
        "    text = text.strip(' \"')\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "print(\">>> Wczytywanie danych...\")\n",
        "try:\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    print(f\"✓ Wczytano {len(df)} rekordów\")\n",
        "    \n",
        "    # Filtruj tylko toksyczne komentarze\n",
        "    toxic_df = df[df['toxic'] == 1].copy()\n",
        "    print(f\"✓ Znaleziono {len(toxic_df)} toksycznych komentarzy\")\n",
        "    \n",
        "    # Wybierz N_SAMPLES losowych próbek\n",
        "    sample_df = toxic_df.sample(n=min(N_SAMPLES, len(toxic_df)), random_state=PARAPHRASE_SEED)\n",
        "    \n",
        "    # Wyczyść teksty\n",
        "    sample_df['comment_text'] = sample_df['comment_text'].apply(clean_text)\n",
        "    \n",
        "    # Usuń puste teksty\n",
        "    sample_df = sample_df[sample_df['comment_text'].str.len() > 10]\n",
        "    \n",
        "    print(f\"✓ Wybrano {len(sample_df)} próbek do testowania\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Błąd wczytywania danych: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "723f6a5a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# FUNKCJE POMOCNICZE\n",
        "# ===================================================\n",
        "\n",
        "def get_embedding_and_prob(text):\n",
        "    \"\"\"\n",
        "    Pobiera embedding [CLS] z warstwy TARGET_LAYER_INDEX oraz prawdopodobieństwo toksyczności.\n",
        "    \n",
        "    Zwraca:\n",
        "        Tuple (embedding_vector, toxic_probability)\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "    ).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "        embedding = outputs.hidden_states[TARGET_LAYER_INDEX][0, 0, :]  # [CLS] token\n",
        "        prob = torch.sigmoid(outputs.logits)[0, 0].item()\n",
        "    \n",
        "    return embedding, prob\n",
        "\n",
        "\n",
        "def generate_paraphrase(text):\n",
        "    \"\"\"\n",
        "    Generuje parafrazę używając modelu T5.\n",
        "    \n",
        "    Zwraca:\n",
        "        Wygenerowana parafraza (string)\n",
        "    \"\"\"\n",
        "    # Prefix \"paraphrase:\" jest wymagany przez ten model\n",
        "    input_text = \"paraphrase: \" + text + \" </s>\"\n",
        "    \n",
        "    inputs = t5_tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQUENCE_LENGTH,\n",
        "    ).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = t5_model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=MAX_SEQUENCE_LENGTH,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            num_return_sequences=1,\n",
        "        )\n",
        "    \n",
        "    paraphrase = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return paraphrase\n",
        "\n",
        "\n",
        "def calculate_cosine_similarity(emb1, emb2):\n",
        "    \"\"\"Oblicza cosine similarity między dwoma embeddingami.\"\"\"\n",
        "    return F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n",
        "\n",
        "print(\"✓ Funkcje pomocnicze zdefiniowane\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be181959",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# GENEROWANIE PARAFRAZ I ZBIERANIE WYNIKÓW\n",
        "# ===================================================\n",
        "\n",
        "print(\">>> Rozpoczynam generowanie parafraz...\\n\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Generowanie parafraz\"):\n",
        "    original_text = row['comment_text']\n",
        "    \n",
        "    try:\n",
        "        # 1. Oblicz embedding i prawdopodobieństwo dla oryginału\n",
        "        orig_emb, orig_prob = get_embedding_and_prob(original_text)\n",
        "        \n",
        "        # 2. Wygeneruj parafrazę\n",
        "        para_text = generate_paraphrase(original_text)\n",
        "        \n",
        "        # 3. Oblicz embedding i prawdopodobieństwo dla parafrazy\n",
        "        para_emb, para_prob = get_embedding_and_prob(para_text)\n",
        "        \n",
        "        # 4. Oblicz cosine similarity\n",
        "        cos_sim = calculate_cosine_similarity(orig_emb, para_emb)\n",
        "        \n",
        "        # 5. Oblicz różnicę prawdopodobieństw\n",
        "        prob_diff = abs(orig_prob - para_prob)\n",
        "        \n",
        "        # 6. Sprawdź czy parafraza przeszła walidację\n",
        "        quality_ok = cos_sim >= PARAPHRASE_MIN_SIMILARITY\n",
        "        \n",
        "        # Skróć teksty dla wyświetlenia (pierwsze 100 znaków)\n",
        "        orig_display = original_text[:100] + \"...\" if len(original_text) > 100 else original_text\n",
        "        para_display = para_text[:100] + \"...\" if len(para_text) > 100 else para_text\n",
        "        \n",
        "        results.append({\n",
        "            'id': len(results) + 1,\n",
        "            'original_text': orig_display,\n",
        "            'paraphrase_text': para_display,\n",
        "            'orig_prob': round(orig_prob, 3),\n",
        "            'para_prob': round(para_prob, 3),\n",
        "            'prob_diff': round(prob_diff, 3),\n",
        "            'cosine_sim': round(cos_sim, 3),\n",
        "            'quality_ok': '✓' if quality_ok else '✗',\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Błąd dla próbki {idx}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n✓ Przetworzono {len(results)} par tekst-parafraza\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f81fc65e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# WYŚWIETLENIE TABELI WYNIKÓW\n",
        "# ===================================================\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"=\"*100)\n",
        "print(\"WYNIKI TESTOWANIA PARAFRAZ\")\n",
        "print(\"=\"*100)\n",
        "print()\n",
        "\n",
        "# Wyświetl pełną tabelę\n",
        "print(results_df.to_string(index=False))\n",
        "print()\n",
        "\n",
        "# Zapisz do CSV\n",
        "output_path = \"/drive/MyDrive/msc-project/paraphrase_test_results.csv\"\n",
        "results_df.to_csv(output_path, index=False)\n",
        "print(f\"✓ Wyniki zapisane do: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7506bb4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# STATYSTYKI PODSUMOWUJĄCE\n",
        "# ===================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"STATYSTYKI PODSUMOWUJĄCE\")\n",
        "print(\"=\"*100)\n",
        "print()\n",
        "\n",
        "# Podstawowe statystyki\n",
        "print(f\"Liczba przetestowanych par: {len(results_df)}\")\n",
        "print()\n",
        "\n",
        "# Jakość parafraz\n",
        "quality_pass = (results_df['quality_ok'] == '✓').sum()\n",
        "quality_fail = (results_df['quality_ok'] == '✗').sum()\n",
        "quality_pass_pct = (quality_pass / len(results_df) * 100) if len(results_df) > 0 else 0\n",
        "\n",
        "print(f\"Parafrazy przechodzące walidację (cos_sim ≥ {PARAPHRASE_MIN_SIMILARITY}): {quality_pass} ({quality_pass_pct:.1f}%)\")\n",
        "print(f\"Parafrazy nieprzechodzące walidacji: {quality_fail} ({100-quality_pass_pct:.1f}%)\")\n",
        "print()\n",
        "\n",
        "# Zachowanie toksyczności\n",
        "toxic_maintained = (results_df['para_prob'] > 0.5).sum()\n",
        "toxic_lost = (results_df['para_prob'] <= 0.5).sum()\n",
        "toxic_maintained_pct = (toxic_maintained / len(results_df) * 100) if len(results_df) > 0 else 0\n",
        "\n",
        "print(f\"Parafrazy zachowujące toksyczność (prob > 0.5): {toxic_maintained} ({toxic_maintained_pct:.1f}%)\")\n",
        "print(f\"Parafrazy tracące toksyczność (prob ≤ 0.5): {toxic_lost} ({100-toxic_maintained_pct:.1f}%)\")\n",
        "print()\n",
        "\n",
        "# Statystyki metryk\n",
        "print(\"Statystyki metryk:\")\n",
        "print(f\"  Cosine Similarity:\")\n",
        "print(f\"    - Średnia: {results_df['cosine_sim'].mean():.3f}\")\n",
        "print(f\"    - Min: {results_df['cosine_sim'].min():.3f}\")\n",
        "print(f\"    - Max: {results_df['cosine_sim'].max():.3f}\")\n",
        "print(f\"    - Std: {results_df['cosine_sim'].std():.3f}\")\n",
        "print()\n",
        "print(f\"  Różnica prawdopodobieństw:\")\n",
        "print(f\"    - Średnia: {results_df['prob_diff'].mean():.3f}\")\n",
        "print(f\"    - Min: {results_df['prob_diff'].min():.3f}\")\n",
        "print(f\"    - Max: {results_df['prob_diff'].max():.3f}\")\n",
        "print(f\"    - Std: {results_df['prob_diff'].std():.3f}\")\n",
        "print()\n",
        "\n",
        "# Wnioski\n",
        "print(\"=\"*100)\n",
        "print(\"WNIOSKI\")\n",
        "print(\"=\"*100)\n",
        "print()\n",
        "\n",
        "if quality_pass_pct >= 80:\n",
        "    print(\"✓ Wysoka jakość parafraz (≥80% przechodzi walidację)\")\n",
        "elif quality_pass_pct >= 60:\n",
        "    print(\"⚠ Średnia jakość parafraz (60-80% przechodzi walidację)\")\n",
        "else:\n",
        "    print(\"✗ Niska jakość parafraz (<60% przechodzi walidację)\")\n",
        "\n",
        "if toxic_maintained_pct >= 80:\n",
        "    print(\"✓ Parafrazy dobrze zachowują toksyczność (≥80%)\")\n",
        "elif toxic_maintained_pct >= 60:\n",
        "    print(\"⚠ Parafrazy średnio zachowują toksyczność (60-80%)\")\n",
        "else:\n",
        "    print(\"✗ Parafrazy tracą toksyczność (<60% zachowuje)\")\n",
        "\n",
        "if results_df['cosine_sim'].mean() >= 0.8:\n",
        "    print(\"✓ Wysokie podobieństwo semantyczne (średnia ≥0.8)\")\n",
        "elif results_df['cosine_sim'].mean() >= 0.7:\n",
        "    print(\"⚠ Średnie podobieństwo semantyczne (średnia 0.7-0.8)\")\n",
        "else:\n",
        "    print(\"✗ Niskie podobieństwo semantyczne (średnia <0.7)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66f804bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# PRZYKŁADY (Pierwsze 5 par)\n",
        "# ===================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"PRZYKŁADOWE PARY TEKST-PARAFRAZA (pierwsze 5)\")\n",
        "print(\"=\"*100)\n",
        "print()\n",
        "\n",
        "for i, row in results_df.head(5).iterrows():\n",
        "    print(f\"--- Przykład {row['id']} ---\")\n",
        "    print(f\"ORYGINAŁ: {row['original_text']}\")\n",
        "    print(f\"PARAFRAZA: {row['paraphrase_text']}\")\n",
        "    print(f\"Prawdop. toksyczności: {row['orig_prob']} → {row['para_prob']} (diff: {row['prob_diff']})\")\n",
        "    print(f\"Cosine similarity: {row['cosine_sim']} {row['quality_ok']}\")\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP7test",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
