{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOG7eJb442FlvNNT091/DDM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-project/blob/main/test_notebooks/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ujx4GU0pSU3"
      },
      "outputs": [],
      "source": [
        "!uv pip install --upgrade transformers datasets captum quantus accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "from datasets import Dataset\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from captum.attr import IntegratedGradients, InputXGradient"
      ],
      "metadata": {
        "id": "mhXDPZFkpcyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "metadata": {
        "id": "CpEzoMGNphyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. KONFIGURACJA GLOBALNA\n",
        "# ==========================================\n",
        "N_SAMPLES_XAI = 100          # Liczba próbek do metod XAI (IG/IxG)\n",
        "N_SAMPLES_PROBE = 1000       # Liczba próbek do analizy warstwowej RepE\n",
        "N_SAMPLES_STABILITY = 50     # Liczba par do testu stabilności (parafrazy)\n",
        "BATCH_SIZE = 32\n",
        "TOP_K_TOKENS = 5             # Ile słów usuwamy w metryce Comprehensiveness\n",
        "DF_SIZE = 3000\n",
        "\n",
        "# Ścieżki (Dostosuj jeśli trzeba)\n",
        "DATA_PATH = '/drive/MyDrive/msc-project/jigsaw-toxic-comment/train.csv'\n",
        "RESULTS_DIR = \"/drive/MyDrive/msc-project/results_final\"\n",
        "MODEL_CHECKPOINT = \"/drive/MyDrive/msc-project/models/distilbert-jigsaw-full\"\n",
        "\n",
        "# Urządzenie\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on device: {device}\")\n",
        "\n",
        "# Tworzenie katalogu wyników\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "1EwfO5X0pjE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. PRZYGOTOWANIE DANYCH I MODELU (SETUP)\n",
        "# ==========================================\n",
        "\n",
        "def clean_text(example):\n",
        "    \"\"\"Funkcja czyszcząca tekst (taka sama jak przy treningu).\"\"\"\n",
        "    text = example['comment_text']\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    text = re.sub(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', '', text)\n",
        "    text = re.sub(r'\\(talk\\)', '', text)\n",
        "    text = re.sub(r'\\d{2}:\\d{2}, \\w+ \\d{1,2}, \\d{4} \\(utc\\)', '', text)\n",
        "    text = text.replace('\\n', ' ').replace('\\xa0', ' ')\n",
        "    text = text.strip(' \"')\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    example['comment_text'] = text\n",
        "    return example\n",
        "\n",
        "def prepare_environment():\n",
        "    print(\">>> [SETUP] Loading and preprocessing data...\")\n",
        "\n",
        "    # 1. Wczytanie danych\n",
        "    try:\n",
        "        df = pd.read_csv(DATA_PATH).head(DF_SIZE)\n",
        "        dataset = Dataset.from_pandas(df)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Nie znaleziono pliku: {DATA_PATH}. Sprawdź ścieżkę w Konfiguracji Globalnej.\")\n",
        "\n",
        "    # 2. Preprocessing\n",
        "    dataset = dataset.map(clean_text)\n",
        "\n",
        "    # 3. Ładowanie Tokenizera (musi pasować do modelu)\n",
        "    print(f\">>> [SETUP] Loading tokenizer from: {MODEL_CHECKPOINT}...\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "    except OSError:\n",
        "        print(f\"Błąd: Nie znaleziono tokenizera w {MODEL_CHECKPOINT}. Pobieram domyślny 'distilbert-base-uncased'.\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "    # 4. Tokenizacja\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"comment_text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
        "\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # 5. Labeling\n",
        "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "    def create_labels(example):\n",
        "        example['labels'] = [float(example[col]) for col in label_cols]\n",
        "        return example\n",
        "\n",
        "    final_dataset = tokenized_dataset.map(create_labels)\n",
        "\n",
        "    # Ustawienie formatu Torch (wymagane dla modelu)\n",
        "    # Usuwamy kolumny tekstowe, zostawiamy tensory\n",
        "    cols_to_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "    final_dataset.set_format(\"torch\", columns=cols_to_keep)\n",
        "\n",
        "    # 6. Split (Tylko po to, aby wyodrębnić zbiór testowy, na którym pracujemy)\n",
        "    splits = final_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "    eval_dataset = splits['test']\n",
        "\n",
        "    # 7. Ładowanie Wytrenowanego Modelu\n",
        "    print(f\">>> [SETUP] Loading Pre-trained Model from: {MODEL_CHECKPOINT}...\")\n",
        "    try:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            MODEL_CHECKPOINT,\n",
        "            num_labels=6,\n",
        "            problem_type=\"multi_label_classification\"\n",
        "        )\n",
        "    except OSError:\n",
        "        raise OSError(f\"Nie znaleziono modelu w ścieżce: {MODEL_CHECKPOINT}. Upewnij się, że najpierw uruchomiłeś skrypt treningowy.\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval() # WAŻNE: Przełączenie w tryb ewaluacji (wyłącza dropout)\n",
        "\n",
        "    print(f\">>> [SETUP] Environment Ready. Device: {device}\")\n",
        "    return model, tokenizer, eval_dataset\n",
        "\n",
        "# Inicjalizacja środowiska\n",
        "model, tokenizer, eval_dataset = prepare_environment()"
      ],
      "metadata": {
        "id": "B-EJ7Ktip8SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. MODUŁ A: PORÓWNANIE METOD XAI (Comprehensiveness)\n",
        "# ==========================================\n",
        "\n",
        "def run_module_a_xai(model, tokenizer, dataset):\n",
        "    print(\"\\n>>> [MODULE A] Running XAI Comparison (IG vs IxG)...\")\n",
        "    model.eval()\n",
        "\n",
        "    # Filtrowanie tylko toksycznych przykładów\n",
        "    toxic_indices = [i for i, labels in enumerate(dataset['labels']) if labels[0] == 1]\n",
        "    subset_indices = toxic_indices[:N_SAMPLES_XAI]\n",
        "    subset = dataset.select(subset_indices)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Wrapper dla Captum\n",
        "    def predict_func(inputs_embeds, attention_mask=None):\n",
        "        return model(inputs_embeds=inputs_embeds, attention_mask=attention_mask).logits\n",
        "\n",
        "    ig = IntegratedGradients(predict_func)\n",
        "    ixg = InputXGradient(predict_func)\n",
        "\n",
        "    for i in tqdm(range(len(subset)), desc=\"XAI Evaluation\"):\n",
        "        input_ids = subset[i]['input_ids'].unsqueeze(0).to(device)\n",
        "        attention_mask = subset[i]['attention_mask'].unsqueeze(0).to(device)\n",
        "        input_embeds = model.distilbert.embeddings(input_ids)\n",
        "        baseline = model.distilbert.embeddings(torch.tensor([tokenizer.pad_token_id]*256, device=device).unsqueeze(0))\n",
        "\n",
        "        # 1. Oryginalne prawdopodobieństwo\n",
        "        with torch.no_grad():\n",
        "            orig_out = model(inputs_embeds=input_embeds, attention_mask=attention_mask)\n",
        "            orig_prob = torch.sigmoid(orig_out.logits)[0, 0].item()\n",
        "\n",
        "        # Funkcja pomocnicza do obliczania spadku pewności\n",
        "        def calculate_drop(attr_tensor):\n",
        "            # Suma po wymiarze embeddingów\n",
        "            attr_sum = attr_tensor.sum(dim=-1).squeeze(0)\n",
        "            # Znajdź Top-K najważniejszych tokenów\n",
        "            _, top_indices = torch.topk(attr_sum, k=TOP_K_TOKENS)\n",
        "\n",
        "            # Maskowanie tokenów\n",
        "            masked_ids = input_ids.clone()\n",
        "            masked_ids[0, top_indices] = tokenizer.pad_token_id\n",
        "\n",
        "            with torch.no_grad():\n",
        "                new_out = model(masked_ids, attention_mask=attention_mask)\n",
        "                new_prob = torch.sigmoid(new_out.logits)[0, 0].item()\n",
        "\n",
        "            return orig_prob - new_prob\n",
        "\n",
        "        # 2. Metoda IG\n",
        "        attr_ig, _ = ig.attribute(inputs=input_embeds, baselines=baseline, target=0,\n",
        "                                  additional_forward_args=(attention_mask,), return_convergence_delta=True)\n",
        "        drop_ig = calculate_drop(attr_ig)\n",
        "\n",
        "        # 3. Metoda IxG\n",
        "        attr_ixg = ixg.attribute(inputs=input_embeds, target=0, additional_forward_args=(attention_mask,))\n",
        "        drop_ixg = calculate_drop(attr_ixg)\n",
        "\n",
        "        results.append({\n",
        "            \"text_id\": i,\n",
        "            \"original_prob\": orig_prob,\n",
        "            \"ig_drop_score\": drop_ig,\n",
        "            \"ixg_drop_score\": drop_ixg\n",
        "        })\n",
        "\n",
        "    # Zapis i Wizualizacja\n",
        "    df_res = pd.DataFrame(results)\n",
        "    df_res.to_csv(f\"{RESULTS_DIR}/xai_comparison_results.csv\", index=False)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.boxplot(data=df_res[['ig_drop_score', 'ixg_drop_score']])\n",
        "    plt.title(f\"Comprehensiveness (Confidence Drop) - Removing Top {TOP_K_TOKENS} Tokens\")\n",
        "    plt.ylabel(\"Probability Drop\")\n",
        "    plt.savefig(f\"{RESULTS_DIR}/xai_boxplot.png\")\n",
        "    plt.close()\n",
        "    print(\"Module A complete.\")\n",
        "    return df_res"
      ],
      "metadata": {
        "id": "J1uG43ALs8lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. MODUŁ B: ANALIZA WARSTWOWA (RepE)\n",
        "# ==========================================\n",
        "\n",
        "def run_module_b_repe(model, dataset):\n",
        "    print(\"\\n>>> [MODULE B] Running Layer-wise Probing (RepE)...\")\n",
        "    model.eval()\n",
        "\n",
        "    # Wybór podzbioru\n",
        "    subset = dataset.select(range(min(len(dataset), N_SAMPLES_PROBE)))\n",
        "\n",
        "    # Ekstrakcja\n",
        "    layers_data = {i: [] for i in range(7)} # 0=Embed, 1-6=Layers\n",
        "    all_labels = []\n",
        "\n",
        "    loader = torch.utils.data.DataLoader(subset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    for batch in tqdm(loader, desc=\"Extracting Layers\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'][:, 0].numpy() # Tylko klasa 'toxic'\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(input_ids, attention_mask=mask, output_hidden_states=True)\n",
        "\n",
        "        all_labels.extend(labels)\n",
        "        for i, hidden in enumerate(out.hidden_states):\n",
        "            # CLS token only\n",
        "            layers_data[i].append(hidden[:, 0, :].cpu().numpy())\n",
        "\n",
        "    # Trenowanie sond\n",
        "    results = []\n",
        "    y = np.array(all_labels)\n",
        "    y_bin = (y > 0.5).astype(int)\n",
        "\n",
        "    for layer_idx in sorted(layers_data.keys()):\n",
        "        X = np.concatenate(layers_data[layer_idx], axis=0)\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=0.2, random_state=42)\n",
        "\n",
        "        clf = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "        clf.fit(X_train, y_train)\n",
        "        preds = clf.predict(X_test)\n",
        "\n",
        "        acc = accuracy_score(y_test, preds)\n",
        "        f1 = f1_score(y_test, preds)\n",
        "\n",
        "        results.append({\"layer\": layer_idx, \"accuracy\": acc, \"f1_score\": f1})\n",
        "\n",
        "        # Zapisujemy wektor (direction) dla warstwy 5 do Modułu D\n",
        "        if layer_idx == 5:\n",
        "            global layer_5_activations, layer_5_labels\n",
        "            layer_5_activations = X\n",
        "            layer_5_labels = y_bin\n",
        "\n",
        "    df_res = pd.DataFrame(results)\n",
        "    df_res.to_csv(f\"{RESULTS_DIR}/repe_layer_performance.csv\", index=False)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.lineplot(data=df_res, x='layer', y='accuracy', marker='o', label='Accuracy')\n",
        "    sns.lineplot(data=df_res, x='layer', y='f1_score', marker='s', label='F1 Score')\n",
        "    plt.title(\"Linear Probe Performance per Layer\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"{RESULTS_DIR}/repe_layer_plot.png\")\n",
        "    plt.close()\n",
        "    print(\"Module B complete.\")"
      ],
      "metadata": {
        "id": "vxB9jgU9s_IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. MODUŁ C: TEST STABILNOŚCI (Robustness)\n",
        "# ==========================================\n",
        "\n",
        "def run_module_c_stability(model, tokenizer, dataset):\n",
        "    print(\"\\n>>> [MODULE C] Running Stability Analysis with T5...\")\n",
        "\n",
        "    # Wczytanie T5\n",
        "    t5_name = \"Vamsi/T5_Paraphrase_Paws\"\n",
        "    t5_tok = AutoTokenizer.from_pretrained(t5_name)\n",
        "    t5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_name).to(device)\n",
        "\n",
        "    # Wybór toksycznych próbek\n",
        "    toxic_indices = [i for i, l in enumerate(dataset['labels']) if l[0] == 1]\n",
        "    sample_indices = toxic_indices[:N_SAMPLES_STABILITY]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    def get_embedding(text, l_idx=5):\n",
        "        in_ids = tokenizer(text, return_tensors='pt', truncation=True, max_length=256).to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(**in_ids, output_hidden_states=True)\n",
        "        return out.hidden_states[l_idx][0, 0, :], torch.sigmoid(out.logits)[0,0].item()\n",
        "\n",
        "    def get_top_tokens(text):\n",
        "        # Uproszczona wersja IG dla stabilności\n",
        "        in_ids = tokenizer(text, return_tensors='pt', truncation=True).to(device)\n",
        "        emb = model.distilbert.embeddings(in_ids['input_ids'])\n",
        "        ig = IntegratedGradients(lambda x: model(inputs_embeds=x).logits)\n",
        "        attr = ig.attribute(emb, target=0, n_steps=10) # Mniej kroków dla szybkości\n",
        "        attr_sum = attr.sum(dim=-1).squeeze(0)\n",
        "        _, idx = torch.topk(attr_sum, k=TOP_K_TOKENS)\n",
        "        return set(tokenizer.convert_ids_to_tokens(in_ids['input_ids'][0][idx]))\n",
        "\n",
        "    for idx in tqdm(sample_indices, desc=\"Stability\"):\n",
        "        orig_ids = dataset[idx]['input_ids']\n",
        "        orig_text = tokenizer.decode(orig_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Generowanie parafrazy\n",
        "        t5_input = t5_tok(\"paraphrase: \" + orig_text + \" </s>\", return_tensors=\"pt\", padding=True).to(device)\n",
        "        t5_out = t5_model.generate(t5_input.input_ids, max_length=256, do_sample=True, top_k=50)\n",
        "        para_text = t5_tok.decode(t5_out[0], skip_special_tokens=True)\n",
        "\n",
        "        # 1. Output Stability\n",
        "        _, prob_orig = get_embedding(orig_text)\n",
        "        vec_para, prob_para = get_embedding(para_text)\n",
        "        vec_orig, _ = get_embedding(orig_text) # Recalculate to be sure\n",
        "\n",
        "        # 2. Representation Stability\n",
        "        cos_sim = F.cosine_similarity(vec_orig.unsqueeze(0), vec_para.unsqueeze(0)).item()\n",
        "\n",
        "        # 3. Explanation Stability (Jaccard)\n",
        "        # (Pomińmy dla bardzo krótkich tekstów, żeby nie wywalało błędu)\n",
        "        try:\n",
        "            toks_orig = get_top_tokens(orig_text)\n",
        "            toks_para = get_top_tokens(para_text)\n",
        "            intersect = len(toks_orig.intersection(toks_para))\n",
        "            union = len(toks_orig.union(toks_para))\n",
        "            jaccard = intersect / union if union > 0 else 0\n",
        "        except:\n",
        "            jaccard = 0.0\n",
        "\n",
        "        results.append({\n",
        "            \"prob_diff\": abs(prob_orig - prob_para),\n",
        "            \"cosine_sim\": cos_sim,\n",
        "            \"jaccard_ig\": jaccard\n",
        "        })\n",
        "\n",
        "    df_res = pd.DataFrame(results)\n",
        "    df_res.to_csv(f\"{RESULTS_DIR}/stability_results.csv\", index=False)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.histplot(df_res['cosine_sim'], kde=True, color='green')\n",
        "    plt.title(\"Distribution of Representation Stability (Layer 5)\")\n",
        "    plt.xlabel(\"Cosine Similarity\")\n",
        "    plt.axvline(0.9, color='red', linestyle='--')\n",
        "    plt.savefig(f\"{RESULTS_DIR}/stability_cosine_hist.png\")\n",
        "    plt.close()\n",
        "    print(\"Module C complete.\")"
      ],
      "metadata": {
        "id": "CCvsbqzctBYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 6. MODUŁ D: TEST SKUTECZNOŚCI STEROWANIA (Steering)\n",
        "# ==========================================\n",
        "\n",
        "def run_module_d_steering(model, tokenizer, dataset):\n",
        "    print(\"\\n>>> [MODULE D] Running Steering Efficacy Test...\")\n",
        "\n",
        "    # 1. Oblicz wektor sterujący (Difference of Means)\n",
        "    # Korzystamy z danych zapisanych w Module B\n",
        "    toxic_vecs = layer_5_activations[layer_5_labels == 1]\n",
        "    safe_vecs = layer_5_activations[layer_5_labels == 0]\n",
        "\n",
        "    mean_toxic = np.mean(toxic_vecs, axis=0)\n",
        "    mean_safe = np.mean(safe_vecs, axis=0)\n",
        "    direction = mean_toxic - mean_safe\n",
        "    steering_tensor = torch.tensor(direction, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Parametry\n",
        "    ALPHA = -3.0 # Wartość ustalona eksperymentalnie (detoksykacja)\n",
        "\n",
        "    # Hook class\n",
        "    class SteeringHook:\n",
        "        def __init__(self, vector, coeff):\n",
        "            self.vector = vector\n",
        "            self.coeff = coeff\n",
        "        def __call__(self, module, inputs, output):\n",
        "            return (output[0] + (self.coeff * self.vector),) + output[1:]\n",
        "\n",
        "    # Test na zbiorze toksycznym\n",
        "    toxic_indices = [i for i, l in enumerate(dataset['labels']) if l[0] == 1][:N_SAMPLES_XAI]\n",
        "    safe_indices = [i for i, l in enumerate(dataset['labels']) if l[0] == 0][:N_SAMPLES_XAI]\n",
        "\n",
        "    success_count = 0\n",
        "    side_effect_count = 0\n",
        "\n",
        "    # Hook registration\n",
        "    layer_module = model.distilbert.transformer.layer[5]\n",
        "\n",
        "    # --- Ewaluacja Toksycznych ---\n",
        "    # Register Hook\n",
        "    handle = layer_module.register_forward_hook(SteeringHook(steering_tensor, ALPHA))\n",
        "\n",
        "    for idx in toxic_indices:\n",
        "        input_ids = dataset[idx]['input_ids'].unsqueeze(0).to(device)\n",
        "        mask = dataset[idx]['attention_mask'].unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(input_ids, attention_mask=mask)\n",
        "            prob = torch.sigmoid(out.logits)[0,0].item()\n",
        "            if prob < 0.5:\n",
        "                success_count += 1\n",
        "\n",
        "    handle.remove() # Remove hook for next steps/safety\n",
        "\n",
        "    # --- Ewaluacja Skutków Ubocznych (Safe Samples) ---\n",
        "    handle = layer_module.register_forward_hook(SteeringHook(steering_tensor, ALPHA))\n",
        "\n",
        "    for idx in safe_indices:\n",
        "        input_ids = dataset[idx]['input_ids'].unsqueeze(0).to(device)\n",
        "        mask = dataset[idx]['attention_mask'].unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(input_ids, attention_mask=mask)\n",
        "            prob = torch.sigmoid(out.logits)[0,0].item()\n",
        "            if prob > 0.5: # Stał się toksyczny?\n",
        "                side_effect_count += 1\n",
        "\n",
        "    handle.remove()\n",
        "\n",
        "    # Raport\n",
        "    success_rate = (success_count / len(toxic_indices)) * 100\n",
        "    side_effect_rate = (side_effect_count / len(safe_indices)) * 100\n",
        "\n",
        "    report = f\"\"\"\n",
        "    === STEERING REPORT ===\n",
        "    Method: Difference of Means (Layer 5)\n",
        "    Alpha: {ALPHA}\n",
        "    Samples Evaluated: {len(toxic_indices)} toxic, {len(safe_indices)} safe.\n",
        "\n",
        "    1. Detoxification Success Rate: {success_rate:.2f}%\n",
        "       (Percentage of toxic samples dropped below 0.5 probability)\n",
        "\n",
        "    2. Side Effects Rate: {side_effect_rate:.2f}%\n",
        "       (Percentage of safe samples that became false positives)\n",
        "\n",
        "    Status: {'SUCCESS' if success_rate > 80 and side_effect_rate < 5 else 'NEEDS TUNING'}\n",
        "    \"\"\"\n",
        "\n",
        "    print(report)\n",
        "    with open(f\"{RESULTS_DIR}/steering_report.txt\", \"w\") as f:\n",
        "        f.write(report)\n",
        "    print(\"Module D complete.\")"
      ],
      "metadata": {
        "id": "teXYB3iwtDyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 7. URUCHOMIENIE CAŁOŚCI\n",
        "# ==========================================\n",
        "print(f\"=== STARTING EXPERIMENT (Results -> {RESULTS_DIR}) ===\")\n",
        "\n",
        "# Uruchomienie modułów sekwencyjnie\n",
        "run_module_a_xai(model, tokenizer, eval_dataset)\n",
        "run_module_b_repe(model, eval_dataset)\n",
        "run_module_c_stability(model, tokenizer, eval_dataset)\n",
        "run_module_d_steering(model, tokenizer, eval_dataset)\n",
        "\n",
        "print(\"\\n=== EXPERIMENT COMPLETE ===\")\n",
        "print(f\"Files generated in {RESULTS_DIR}:\")\n",
        "print(os.listdir(RESULTS_DIR))"
      ],
      "metadata": {
        "id": "y-T5e78rtEiM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}