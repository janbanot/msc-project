{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-project/blob/main/test_notebooks/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ujx4GU0pSU3"
      },
      "outputs": [],
      "source": [
        "!uv pip install transformers datasets captum quantus accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhXDPZFkpcyJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "from datasets import Dataset\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from captum.attr import IntegratedGradients, InputXGradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpEzoMGNphyt"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EwfO5X0pjE7"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# 1. KONFIGURACJA GLOBALNA\n",
        "# ===================================================\n",
        "\n",
        "# === Parametry analizy XAI ===\n",
        "N_SAMPLES_XAI = (\n",
        "    100  # Liczba pr√≥bek dla metod XAI (Integrated Gradients / InputXGradient)\n",
        ")\n",
        "XAI_N_STEPS = 50  # Zwiƒôkszono z 10 na 50 dla lepszej stabilno≈õci przybli≈ºenia ca≈Çki\n",
        "N_SAMPLES_PROBE = 1000  # Liczba pr√≥bek do analizy warstwowej metodƒÖ RepE\n",
        "N_SAMPLES_STABILITY = 50  # Liczba par tekst-parafraza do testu stabilno≈õci\n",
        "BATCH_SIZE = (\n",
        "    32  # Rozmiar batcha dla przetwarzania wsadowego (optymalizacja pamiƒôci GPU)\n",
        ")\n",
        "TOP_K_TOKENS = (\n",
        "    5  # Liczba najwa≈ºniejszych token√≥w do usuniƒôcia w metryce Comprehensiveness\n",
        ")\n",
        "DF_SIZE = 3000  # Ograniczenie wielko≈õci zbioru danych (dla szybszego testowania)\n",
        "\n",
        "# === D≈Çugo≈õƒá sekwencji ===\n",
        "MAX_SEQUENCE_LENGTH = 256  # Maksymalna d≈Çugo≈õƒá sekwencji token√≥w\n",
        "\n",
        "# === Indeksy warstw do analizy ===\n",
        "TARGET_LAYER_INDEX = 5  # Warstwa docelowa do analizy (warstwa 5 wykaza≈Ça najlepszƒÖ separowalno≈õƒá liniowƒÖ)\n",
        "STEERING_ALPHA = -3.0  # Si≈Ça wektora sterujƒÖcego (ujemna warto≈õƒá = detoksykacja)\n",
        "\n",
        "# === Pr√≥g klasyfikacji ===\n",
        "CLASSIFICATION_THRESHOLD = 0.5  # Pr√≥g prawdopodobie≈Ñstwa dla klasyfikacji binarnej\n",
        "\n",
        "# === Parametry testu stabilno≈õci (Modu≈Ç C) ===\n",
        "PARAPHRASE_MIN_SIMILARITY = 0.7  # Minimalny cosine similarity dla akceptacji parafrazy\n",
        "PARAPHRASE_SEED = 42  # Seed dla reproducibility generowania parafraz\n",
        "\n",
        "# === ≈öcie≈ºki ===\n",
        "DATA_PATH = \"/drive/MyDrive/msc-project/jigsaw-toxic-comment/train.csv\"\n",
        "\n",
        "# Dodaj timestamp do nazwy katalogu, aby nie nadpisywaƒá poprzednich wynik√≥w\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "RESULTS_DIR = f\"/drive/MyDrive/msc-project/results_final_{TIMESTAMP}\"\n",
        "\n",
        "MODEL_CHECKPOINT = \"/drive/MyDrive/msc-project/models/distilbert-jigsaw-full_20260125_133112\"\n",
        "# MODEL_CHECKPOINT = \"/drive/MyDrive/msc-project/models/distilbert-jigsaw-full\"\n",
        "\n",
        "# === UrzƒÖdzenie obliczeniowe ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Uruchomiono na urzƒÖdzeniu: {device}\")\n",
        "\n",
        "# Tworzenie katalogu wynik√≥w\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-EJ7Ktip8SI"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# 2. PRZYGOTOWANIE DANYCH I MODELU\n",
        "# ===================================================\n",
        "\n",
        "\n",
        "def clean_text(example):\n",
        "    \"\"\"\n",
        "    Czy≈õci tekst komentarza, usuwajƒÖc niepo≈ºƒÖdane elementy i normalizujƒÖc format.\n",
        "\n",
        "    Funkcja stosowana jest zar√≥wno podczas treningu jak i ewaluacji, aby zapewniƒá\n",
        "    sp√≥jno≈õƒá przetwarzania danych.\n",
        "\n",
        "    Argumenty:\n",
        "        example: S≈Çownik zawierajƒÖcy klucz 'comment_text' z tekstem do oczyszczenia\n",
        "\n",
        "    Zwraca:\n",
        "        Zmodyfikowany s≈Çownik example z oczyszczonym tekstem w polu 'comment_text'\n",
        "\n",
        "    Operacje czyszczenia:\n",
        "        - Konwersja na ma≈Çe litery (wymagane dla modeli BERT typu uncased)\n",
        "        - Usuniƒôcie link√≥w URL (http/https/www)\n",
        "        - Usuniƒôcie adres√≥w IP\n",
        "        - Usuniƒôcie metadanych Wikipedii (talk pages, timestampy UTC)\n",
        "        - Normalizacja bia≈Çych znak√≥w (spacje, newline, non-breaking space)\n",
        "        - Usuniƒôcie cudzys≈Çow√≥w z poczƒÖtku i ko≈Ñca\n",
        "    \"\"\"\n",
        "    text = example[\"comment_text\"]\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \"\", text)\n",
        "    text = re.sub(r\"\\(talk\\)\", \"\", text)\n",
        "    text = re.sub(r\"\\d{2}:\\d{2}, \\w+ \\d{1,2}, \\d{4} \\(utc\\)\", \"\", text)\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\xa0\", \" \")\n",
        "    text = text.strip(' \"')\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    example[\"comment_text\"] = text\n",
        "    return example\n",
        "\n",
        "\n",
        "def prepare_environment():\n",
        "    \"\"\"\n",
        "    Przygotowuje ≈õrodowisko eksperymentalne: wczytuje dane, tokenizuje i ≈Çaduje model.\n",
        "\n",
        "    Zwraca:\n",
        "        Tuple zawierajƒÖcy:\n",
        "        - model: Wytrenowany model DistilBERT do klasyfikacji toksyczno≈õci\n",
        "        - tokenizer: Tokenizer dopasowany do modelu\n",
        "        - eval_dataset: Zbi√≥r testowy z przetworzonymi danymi\n",
        "\n",
        "    Kroki przygotowania:\n",
        "        1. Wczytanie danych z pliku CSV\n",
        "        2. Preprocessing tekst√≥w\n",
        "        3. ≈Åadowanie tokenizera\n",
        "        4. Tokenizacja tekst√≥w (padding do MAX_SEQUENCE_LENGTH)\n",
        "        5. Przygotowanie etykiet binary classification\n",
        "        6. Podzia≈Ç na zbi√≥r treningowy i testowy\n",
        "        7. Za≈Çadowanie wytrenowanego modelu\n",
        "    \"\"\"\n",
        "    print(\">>> [SETUP] Wczytywanie i przetwarzanie danych...\")\n",
        "\n",
        "    # 1. Wczytanie danych\n",
        "    try:\n",
        "        df = pd.read_csv(DATA_PATH).head(DF_SIZE)\n",
        "        dataset = Dataset.from_pandas(df)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Nie znaleziono pliku: {DATA_PATH}. Sprawd≈∫ ≈õcie≈ºkƒô w Konfiguracji Globalnej.\"\n",
        "        )\n",
        "\n",
        "    # 2. Preprocessing\n",
        "    dataset = dataset.map(clean_text)\n",
        "\n",
        "    # 3. ≈Åadowanie tokenizera zgodnego z modelem\n",
        "    print(f\">>> [SETUP] ≈Åadowanie tokenizera z: {MODEL_CHECKPOINT}...\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "    except OSError:\n",
        "        print(\n",
        "            f\"B≈ÇƒÖd: Nie znaleziono tokenizera w {MODEL_CHECKPOINT}. Pobieram domy≈õlny 'distilbert-base-uncased'.\"\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "    # 4. Tokenizacja\n",
        "    def tokenize_function(examples):\n",
        "        \"\"\"Tokenizuje teksty z paddingiem do sta≈Çej d≈Çugo≈õci MAX_SEQUENCE_LENGTH.\"\"\"\n",
        "        return tokenizer(\n",
        "            examples[\"comment_text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=MAX_SEQUENCE_LENGTH,\n",
        "        )\n",
        "\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # 5. Przygotowanie etykiet binary classification\n",
        "    label_cols = [\n",
        "        \"toxic\",\n",
        "    ]\n",
        "\n",
        "    def create_labels(example):\n",
        "        \"\"\"Pobiera kolumnƒô 'toxic' i tworzy etykietƒô.\"\"\"\n",
        "        example[\"labels\"] = [float(example[col]) for col in label_cols]\n",
        "        return example\n",
        "\n",
        "    final_dataset = tokenized_dataset.map(create_labels)\n",
        "\n",
        "    # Ustawienie formatu PyTorch (usuniƒôcie kolumn tekstowych, zachowanie tylko tensor√≥w)\n",
        "    cols_to_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "    final_dataset.set_format(\"torch\", columns=cols_to_keep)\n",
        "\n",
        "    # 6. Podzia≈Ç na zbi√≥r treningowy i testowy\n",
        "    splits = final_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "    eval_dataset = splits[\"test\"]\n",
        "\n",
        "    # 7. ≈Åadowanie wytrenowanego modelu\n",
        "    print(f\">>> [SETUP] ≈Åadowanie wytrenowanego modelu z: {MODEL_CHECKPOINT}...\")\n",
        "    try:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            MODEL_CHECKPOINT, num_labels=1, problem_type=\"single_label_classification\"\n",
        "        )\n",
        "    except OSError:\n",
        "        raise OSError(\n",
        "            f\"Nie znaleziono modelu w ≈õcie≈ºce: {MODEL_CHECKPOINT}. Upewnij siƒô, ≈ºe najpierw uruchomi≈Çe≈õ skrypt treningowy.\"\n",
        "        )\n",
        "\n",
        "    # Prze≈ÇƒÖczenie w tryb ewaluacji (wy≈ÇƒÖcza dropout i batch normalization)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\">>> [SETUP] ≈örodowisko gotowe. UrzƒÖdzenie: {device}\")\n",
        "    return model, tokenizer, eval_dataset\n",
        "\n",
        "\n",
        "# Inicjalizacja ≈õrodowiska\n",
        "model, tokenizer, eval_dataset = prepare_environment()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1uG43ALs8lO"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# 3. MODU≈Å A: POR√ìWNANIE METOD XAI (Comprehensiveness)\n",
        "# ===================================================\n",
        "\n",
        "\n",
        "def run_module_a_xai(model, tokenizer, dataset):\n",
        "    \"\"\"\n",
        "    Por√≥wnuje metody XAI (Integrated Gradients vs InputXGradient) pod kƒÖtem wierno≈õci wyja≈õnie≈Ñ.\n",
        "\n",
        "    Metryka Comprehensiveness mierzy, jak bardzo usuniecie najwa≈ºniejszych token√≥w\n",
        "    (zidentyfikowanych przez metodƒô XAI) wp≈Çywa na pewno≈õƒá predykcji modelu.\n",
        "    Wy≈ºszy spadek pewno≈õci = lepsza metoda XAI.\n",
        "\n",
        "    Argumenty:\n",
        "        model: Wytrenowany model klasyfikacyjny DistilBERT\n",
        "        tokenizer: Tokenizer odpowiadajƒÖcy modelowi\n",
        "        dataset: Zbi√≥r danych z etykietami\n",
        "\n",
        "    Zwraca:\n",
        "        DataFrame z wynikami por√≥wnania metod (drop scores dla IG i IxG)\n",
        "\n",
        "    Metodologia:\n",
        "        1. Wyb√≥r podzbioru toksycznych przyk≈Çad√≥w (N_SAMPLES_XAI)\n",
        "        2. Dla ka≈ºdego przyk≈Çadu:\n",
        "            a) Obliczenie oryginalnego prawdopodobie≈Ñstwa toksyczno≈õci\n",
        "            b) Identyfikacja TOP_K_TOKENS najwa≈ºniejszych token√≥w (IG i InputXGradient)\n",
        "            c) Maskowanie tych token√≥w i ponowna predykcja\n",
        "            d) Obliczenie spadku pewno≈õci (comprehensiveness score)\n",
        "        3. Wizualizacja wynik√≥w jako boxplot\n",
        "    \"\"\"\n",
        "    print(\"\\n>>> [MODU≈Å A] Uruchamianie por√≥wnania metod XAI (IG vs IxG)...\")\n",
        "    model.eval()\n",
        "\n",
        "    # Filtrowanie tylko toksycznych przyk≈Çad√≥w (indeks 0 = etykieta 'toxic')\n",
        "    toxic_indices = [i for i, labels in enumerate(dataset[\"labels\"]) if labels[0] == 1]\n",
        "    subset_indices = toxic_indices[:N_SAMPLES_XAI]\n",
        "    subset = dataset.select(subset_indices)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Funkcja pomocnicza dla Captum (zwraca logity na podstawie embeddings)\n",
        "    def predict_func(inputs_embeds, attention_mask=None):\n",
        "        \"\"\"Wrapper predykcji dla biblioteki Captum.\"\"\"\n",
        "        return model(inputs_embeds=inputs_embeds, attention_mask=attention_mask).logits\n",
        "\n",
        "    ig = IntegratedGradients(predict_func)\n",
        "    ixg = InputXGradient(predict_func)\n",
        "\n",
        "    for i in tqdm(range(len(subset)), desc=\"Ewaluacja XAI\"):\n",
        "        input_ids = subset[i][\"input_ids\"].unsqueeze(0).to(device)\n",
        "        attention_mask = subset[i][\"attention_mask\"].unsqueeze(0).to(device)\n",
        "        input_embeds = model.distilbert.embeddings(input_ids)\n",
        "\n",
        "        # Baseline = embedding tokena [PAD] (punkt odniesienia dla IG)\n",
        "        baseline = model.distilbert.embeddings(\n",
        "            torch.tensor(\n",
        "                [tokenizer.pad_token_id] * MAX_SEQUENCE_LENGTH, device=device\n",
        "            ).unsqueeze(0)\n",
        "        )\n",
        "\n",
        "        # 1. Oryginalne prawdopodobie≈Ñstwo toksyczno≈õci\n",
        "        with torch.no_grad():\n",
        "            orig_out = model(inputs_embeds=input_embeds, attention_mask=attention_mask)\n",
        "            orig_prob = torch.sigmoid(orig_out.logits)[0, 0].item()\n",
        "\n",
        "        # Funkcja pomocnicza do obliczania spadku pewno≈õci\n",
        "        def calculate_drop(attr_tensor):\n",
        "            \"\"\"\n",
        "            Oblicza spadek pewno≈õci po usuniƒôciu TOP_K najwa≈ºniejszych token√≥w.\n",
        "\n",
        "            Argumenty:\n",
        "                attr_tensor: Tensor atrybut√≥w z metody XAI\n",
        "\n",
        "            Zwraca:\n",
        "                Spadek prawdopodobie≈Ñstwa (orig_prob - new_prob)\n",
        "            \"\"\"\n",
        "            # Suma po wymiarze embedding√≥w -> wa≈ºno≈õƒá na poziomie token√≥w\n",
        "            attr_sum = attr_tensor.sum(dim=-1).squeeze(0)\n",
        "            # Znajd≈∫ TOP_K najwa≈ºniejszych token√≥w\n",
        "            _, top_indices = torch.topk(attr_sum, k=TOP_K_TOKENS)\n",
        "\n",
        "            # Maskowanie token√≥w (zamiana na [PAD])\n",
        "            masked_ids = input_ids.clone()\n",
        "            masked_ids[0, top_indices] = tokenizer.pad_token_id\n",
        "\n",
        "            with torch.no_grad():\n",
        "                new_out = model(masked_ids, attention_mask=attention_mask)\n",
        "                new_prob = torch.sigmoid(new_out.logits)[0, 0].item()\n",
        "\n",
        "            return orig_prob - new_prob\n",
        "\n",
        "        # 2. Metoda Integrated Gradients\n",
        "        attr_ig, _ = ig.attribute(\n",
        "            inputs=input_embeds,\n",
        "            baselines=baseline,\n",
        "            target=0,\n",
        "            additional_forward_args=(attention_mask,),\n",
        "            return_convergence_delta=True,\n",
        "        )\n",
        "        drop_ig = calculate_drop(attr_ig)\n",
        "\n",
        "        # 3. Metoda InputXGradient\n",
        "        attr_ixg = ixg.attribute(\n",
        "            inputs=input_embeds, target=0, additional_forward_args=(attention_mask,)\n",
        "        )\n",
        "        drop_ixg = calculate_drop(attr_ixg)\n",
        "\n",
        "        results.append(\n",
        "            {\n",
        "                \"text_id\": i,\n",
        "                \"original_prob\": orig_prob,\n",
        "                \"ig_drop_score\": drop_ig,\n",
        "                \"ixg_drop_score\": drop_ixg,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # Zapis wynik√≥w i wizualizacja\n",
        "    df_res = pd.DataFrame(results)\n",
        "    df_res.to_csv(f\"{RESULTS_DIR}/xai_comparison_results.csv\", index=False)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.boxplot(data=df_res[[\"ig_drop_score\", \"ixg_drop_score\"]])\n",
        "    plt.title(\n",
        "        f\"Comprehensiveness (Spadek Pewno≈õci) - Usuniƒôto {TOP_K_TOKENS} Najwa≈ºniejszych Token√≥w\"\n",
        "    )\n",
        "    plt.ylabel(\"Spadek Prawdopodobie≈Ñstwa\")\n",
        "    plt.savefig(f\"{RESULTS_DIR}/xai_boxplot.png\")\n",
        "    plt.close()\n",
        "    print(\"Modu≈Ç A zako≈Ñczony.\")\n",
        "    return df_res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxB9jgU9s_IX"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# 4. MODU≈Å B: ANALIZA WARSTWOWA (RepE)\n",
        "# ===================================================\n",
        "\n",
        "\n",
        "def run_module_b_repe(model, dataset):\n",
        "    \"\"\"\n",
        "    Przeprowadza analizƒô warstwowƒÖ metodƒÖ Representation Engineering (RepE).\n",
        "\n",
        "    Metoda trenuje liniowe sondy (linear probes) dla ka≈ºdej warstwy transformera,\n",
        "    aby okre≈õliƒá, w kt√≥rej warstwie reprezentacja koncepcji 'toksyczno≈õƒá' jest\n",
        "    najbardziej liniowo separowalna.\n",
        "\n",
        "    Argumenty:\n",
        "        model: Model DistilBERT z aktywowanym output_hidden_states\n",
        "        dataset: Zbi√≥r danych do analizy\n",
        "\n",
        "    Zwraca:\n",
        "        Tuple zawierajƒÖcy:\n",
        "        - df_res: DataFrame z wynikami performance per warstwa\n",
        "        - target_layer_activations: Aktywacje z warstwy TARGET_LAYER_INDEX\n",
        "        - target_layer_labels: Etykiety binarne dla pr√≥bek\n",
        "\n",
        "    Efekty uboczne:\n",
        "        - Zapisuje wyniki do pliku CSV\n",
        "        - Zapisuje wykres performance vs warstwa\n",
        "\n",
        "    Struktura DistilBERT:\n",
        "        - Warstwa 0: Warstwa embedding√≥w (bez transformacji kontekstowej)\n",
        "        - Warstwy 1-6: Warstwy transformera (6 blok√≥w self-attention + FFN)\n",
        "\n",
        "    Hipoteza:\n",
        "        ≈örodkowe warstwy (4-5) powinny mieƒá najlepszƒÖ reprezentacjƒô semantycznƒÖ,\n",
        "        poniewa≈º ≈ÇƒÖczƒÖ sk≈Çadniƒô (ni≈ºsze warstwy) z semantykƒÖ (wy≈ºsze warstwy).\n",
        "    \"\"\"\n",
        "    print(\"\\n>>> [MODU≈Å B] Uruchamianie analizy warstwowej (RepE)...\")\n",
        "    model.eval()\n",
        "\n",
        "    # Wyb√≥r podzbioru (ograniczenie dla szybko≈õci)\n",
        "    subset = dataset.select(range(min(len(dataset), N_SAMPLES_PROBE)))\n",
        "\n",
        "    # S≈Çownik przechowujƒÖcy aktywacje dla ka≈ºdej warstwy\n",
        "    # DistilBERT: 1 warstwa embedding√≥w + 6 warstw transformera = 7 hidden states\n",
        "    layers_data = {i: [] for i in range(7)}\n",
        "    all_labels = []\n",
        "\n",
        "    loader = torch.utils.data.DataLoader(subset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    for batch in tqdm(loader, desc=\"Ekstrakcja Warstw\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"][:, 0].numpy()  # Tylko etykieta 'toxic' (indeks 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(input_ids, attention_mask=mask, output_hidden_states=True)\n",
        "\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "        # Ekstrakcja tokena [CLS] (indeks 0) z ka≈ºdej warstwy\n",
        "        # Token [CLS] zawiera zagregowanƒÖ reprezentacjƒô ca≈Çej sekwencji\n",
        "        for i, hidden in enumerate(out.hidden_states):\n",
        "            layers_data[i].append(hidden[:, 0, :].cpu().numpy())\n",
        "\n",
        "    # Trenowanie sond liniowych dla ka≈ºdej warstwy\n",
        "    results = []\n",
        "    y = np.array(all_labels)\n",
        "    y_bin = (y > CLASSIFICATION_THRESHOLD).astype(int)  # Binaryzacja etykiet\n",
        "\n",
        "    # Zmienne do przechowania aktywacji docelowej warstwy\n",
        "    target_layer_activations = None\n",
        "    target_layer_labels = None\n",
        "\n",
        "    for layer_idx in sorted(layers_data.keys()):\n",
        "        X = np.concatenate(layers_data[layer_idx], axis=0)\n",
        "\n",
        "        # Podzia≈Ç na zbi√≥r treningowy i testowy dla sondy\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y_bin, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Regresja logistyczna jako sonda liniowa\n",
        "        # max_iter=1000 zapewnia zbie≈ºno≈õƒá dla wysokowymiarowych danych\n",
        "        clf = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
        "        clf.fit(X_train, y_train)\n",
        "        preds = clf.predict(X_test)\n",
        "\n",
        "        acc = accuracy_score(y_test, preds)\n",
        "        f1 = f1_score(y_test, preds)\n",
        "\n",
        "        results.append({\"layer\": layer_idx, \"accuracy\": acc, \"f1_score\": f1})\n",
        "\n",
        "        # Zapisujemy aktywacje warstwy docelowej do wykorzystania w Module D (steering)\n",
        "        # Wyb√≥r warstwy TARGET_LAYER_INDEX uzasadniony jest na podstawie:\n",
        "        #   1. Literatury: ≈õrodkowe warstwy transformera ≈ÇƒÖczƒÖ sk≈Çadniƒô (ni≈ºsze) z semantykƒÖ (wy≈ºsze)\n",
        "        #   2. Wynik√≥w tego modu≈Çu: wykres poka≈ºe, ≈ºe warstwa ta ma wysokƒÖ separowalno≈õƒá liniowƒÖ\n",
        "        #   3. Poprzednich eksperyment√≥w: warstwa 5 wykaza≈Ça najlepszƒÖ jako≈õƒá reprezentacji toksyczno≈õci\n",
        "        if layer_idx == TARGET_LAYER_INDEX:\n",
        "            target_layer_activations = X\n",
        "            target_layer_labels = y_bin\n",
        "\n",
        "    df_res = pd.DataFrame(results)\n",
        "    df_res.to_csv(f\"{RESULTS_DIR}/repe_layer_performance.csv\", index=False)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.lineplot(data=df_res, x=\"layer\", y=\"accuracy\", marker=\"o\", label=\"Dok≈Çadno≈õƒá\")\n",
        "    sns.lineplot(data=df_res, x=\"layer\", y=\"f1_score\", marker=\"s\", label=\"F1 Score\")\n",
        "    plt.title(\"Wydajno≈õƒá Sondy Liniowej per Warstwa\")\n",
        "    plt.xlabel(\"Numer Warstwy (0=Embeddings, 1-6=Transformer)\")\n",
        "    plt.ylabel(\"Metryka\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"{RESULTS_DIR}/repe_layer_plot.png\")\n",
        "    plt.close()\n",
        "    print(\"Modu≈Ç B zako≈Ñczony.\")\n",
        "\n",
        "    return df_res, target_layer_activations, target_layer_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCvsbqzctBYY"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# 5. MODU≈Å C: TEST STABILNO≈öCI (Robustness) - NAPRAWIONY\n",
        "# ===================================================\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Za≈Çaduj model semantyczny\n",
        "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def generate_paraphrase_mistral(text, llm_model, llm_tokenizer, device):\n",
        "    \"\"\"\n",
        "    Generuje parafrazƒô i agresywnie usuwa wstƒôpy typu 'Here is your paraphrase'.\n",
        "    \"\"\"\n",
        "    # Definiujemy przyk≈Çady (Few-shot), kt√≥re narzucajƒÖ styl i brak komentarzy\n",
        "    few_shot_examples = \"\"\"\n",
        "    Task: Paraphrase toxic comments. Maintain intent. Output ONLY the text.\n",
        "\n",
        "    Original: \"you are a complete failure and a waste of space\"\n",
        "    Paraphrase: \"you're a total loser who doesn't belong here\"\n",
        "\n",
        "    Original: \"shut your mouth you stupid prick\"\n",
        "    Paraphrase: \"keep your mouth shut you moronic jerk\"\n",
        "\n",
        "    Original: \"get out of this group, no one likes you\"\n",
        "    Paraphrase: \"leave this community, nobody wants you here\"\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"<s>[INST] {few_shot_examples}\\nNow paraphrase this:\\nOriginal: \\\"{text}\\\" [/INST]Paraphrase:\"\n",
        "\n",
        "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = llm_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            do_sample=True,\n",
        "            temperature=0.6,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=llm_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Dekodowanie\n",
        "    gen_text = llm_tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "    # --- AGRESYWNE CZYSZCZENIE REGEXEM ---\n",
        "    # Usuwamy wszystko co wyglƒÖda jak wstƒôpniak (np. \"Here's the paraphrase: \")\n",
        "    patterns = [\n",
        "        r\"(?i)^here's a paraphrased version.*?:\",\n",
        "        r\"(?i)^here is a paraphrase.*?:\",\n",
        "        r\"(?i)^sure, here is.*?:\",\n",
        "        r\"(?i)^paraphrased text:\",\n",
        "        r\"(?i)^hello there,\",\n",
        "        r\"(?i)^the paraphrase is:\",\n",
        "        r\"(?i)^original:.*?\\n\", # Je≈õli model powt√≥rzy≈Ç prompt\n",
        "    ]\n",
        "\n",
        "    clean_text = gen_text.split('\\n')[0] # Bierzemy tylko pierwszƒÖ liniƒô\n",
        "    for p in patterns:\n",
        "        clean_text = re.sub(p, \"\", clean_text).strip()\n",
        "\n",
        "    return clean_text.strip().strip('\"')\n",
        "\n",
        "def run_module_c_stability(model, tokenizer, dataset):\n",
        "    print(\"\\n>>> [MODU≈Å C] Uruchamianie analizy stabilno≈õci (Semantic Robustness)...\")\n",
        "\n",
        "    # Inicjalizacja list na wyniki i logi odrzuce≈Ñ\n",
        "    results = []\n",
        "    skipped_details = []\n",
        "\n",
        "    # 1. Definicja pomocniczych funkcji wewnƒôtrznych\n",
        "    def predict_func_for_ig(inputs_embeds, attention_mask):\n",
        "        return model(inputs_embeds=inputs_embeds, attention_mask=attention_mask).logits\n",
        "\n",
        "    ig = IntegratedGradients(predict_func_for_ig)\n",
        "\n",
        "    def get_embedding(text, l_idx=TARGET_LAYER_INDEX):\n",
        "        in_ids = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=MAX_SEQUENCE_LENGTH).to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(**in_ids, output_hidden_states=True)\n",
        "        return out.hidden_states[l_idx][0, 0, :], torch.sigmoid(out.logits)[0, 0].item()\n",
        "\n",
        "    def get_top_weighted_words(text):\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=MAX_SEQUENCE_LENGTH).to(device)\n",
        "        input_ids, attention_mask = inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
        "\n",
        "        emb = model.distilbert.embeddings(input_ids)\n",
        "        baseline = model.distilbert.embeddings(torch.tensor([tokenizer.pad_token_id] * MAX_SEQUENCE_LENGTH, device=device).unsqueeze(0))\n",
        "\n",
        "        attr = ig.attribute(emb, baselines=baseline, target=0, n_steps=XAI_N_STEPS, additional_forward_args=(attention_mask,))\n",
        "        attr_sum = attr.sum(dim=-1).squeeze(0).abs()\n",
        "\n",
        "        encoding = tokenizer(text, truncation=True, max_length=MAX_SEQUENCE_LENGTH)\n",
        "        word_ids = encoding.word_ids()\n",
        "\n",
        "        word_attributions = {}\n",
        "        for i, word_idx in enumerate(word_ids):\n",
        "            if word_idx is not None:\n",
        "                start, end = encoding.token_to_chars(i)\n",
        "                word = text[start:end].lower().strip()\n",
        "                word_attributions[word] = word_attributions.get(word, 0) + attr_sum[i].item()\n",
        "\n",
        "        sorted_words = sorted(word_attributions.items(), key=lambda x: x[1], reverse=True)\n",
        "        return [word for word, score in sorted_words[:TOP_K_TOKENS]]\n",
        "\n",
        "    def calculate_semantic_overlap(words_orig, words_para):\n",
        "        if not words_orig or not words_para: return 0.0\n",
        "        emb_orig = semantic_model.encode(words_orig, convert_to_tensor=True)\n",
        "        emb_para = semantic_model.encode(words_para, convert_to_tensor=True)\n",
        "        cos_sim_matrix = F.cosine_similarity(emb_orig.unsqueeze(1), emb_para.unsqueeze(0), dim=2)\n",
        "        return (torch.max(cos_sim_matrix, dim=1)[0].mean() + torch.max(cos_sim_matrix, dim=0)[0].mean()).item() / 2\n",
        "\n",
        "    # 2. G≈Ç√≥wna pƒôtla analizy\n",
        "    toxic_indices = [i for i, label in enumerate(dataset[\"labels\"]) if label[0] == 1]\n",
        "    sample_indices = toxic_indices[:N_SAMPLES_STABILITY]\n",
        "\n",
        "    for idx in tqdm(sample_indices, desc=\"Analiza\"):\n",
        "        orig_text = tokenizer.decode(dataset[idx][\"input_ids\"], skip_special_tokens=True)\n",
        "        para_text = generate_paraphrase_mistral(orig_text, mistral_model, mistral_tokenizer, device)\n",
        "\n",
        "        vec_orig, prob_orig = get_embedding(orig_text)\n",
        "        vec_para, prob_para = get_embedding(para_text)\n",
        "\n",
        "        # Obliczanie parametr√≥w walidacji\n",
        "        cos_sim = F.cosine_similarity(vec_orig.unsqueeze(0), vec_para.unsqueeze(0)).item()\n",
        "        orig_word_count = len(orig_text.split())\n",
        "        para_word_count = len(para_text.split())\n",
        "        len_ratio = abs(para_word_count - orig_word_count) / orig_word_count if orig_word_count > 0 else 0\n",
        "\n",
        "        # Sprawdzanie jako≈õci parafrazy\n",
        "        reject_reason = None\n",
        "        if cos_sim < 0.7:\n",
        "            reject_reason = \"Low Cosine Similarity\"\n",
        "        elif len_ratio > 0.5:\n",
        "            reject_reason = \"Length Deviation Too High\"\n",
        "\n",
        "        if reject_reason:\n",
        "            skipped_details.append({\n",
        "                \"idx\": idx,\n",
        "                \"orig\": orig_text[:50] + \"...\",\n",
        "                \"para\": para_text[:50] + \"...\",\n",
        "                \"reason\": reject_reason,\n",
        "                \"cos_sim\": round(cos_sim, 3)\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Je≈õli parafraza jest OK, liczymy stabilno≈õƒá XAI\n",
        "        words_orig = get_top_weighted_words(orig_text)\n",
        "        words_para = get_top_weighted_words(para_text)\n",
        "        semantic_xai_sim = calculate_semantic_overlap(words_orig, words_para)\n",
        "\n",
        "        results.append({\n",
        "            \"prob_diff\": abs(prob_orig - prob_para),\n",
        "            \"cosine_sim\": cos_sim,\n",
        "            \"semantic_xai_sim\": semantic_xai_sim\n",
        "        })\n",
        "\n",
        "    # --- PO PƒòTLI: PRZETWARZANIE WYNIK√ìW ---\n",
        "\n",
        "    df_res = pd.DataFrame(results)\n",
        "    df_skipped = pd.DataFrame(skipped_details)\n",
        "\n",
        "    # 3. Wy≈õwietlanie raportu odrzuce≈Ñ\n",
        "    print(\"\\n=== RAPORT JAKO≈öCI PARAFRAZ ===\")\n",
        "    if not df_skipped.empty:\n",
        "        reason_counts = df_skipped[\"reason\"].value_counts()\n",
        "        for reason, count in reason_counts.items():\n",
        "            print(f\"‚ùå {reason}: {count} przypadk√≥w\")\n",
        "        print(\"\\nPrzyk≈Çadowe odrzucone pary:\")\n",
        "        display(df_skipped[[\"orig\", \"para\", \"reason\", \"cos_sim\"]].head(5))\n",
        "    else:\n",
        "        print(\"‚úÖ Wszystkie parafrazy przesz≈Çy pomy≈õlnie walidacjƒô.\")\n",
        "\n",
        "    # 4. Zapis do plik√≥w\n",
        "    if not df_res.empty:\n",
        "        df_res.to_csv(f\"{RESULTS_DIR}/stability_semantic_results.csv\", index=False)\n",
        "\n",
        "        # 5. Wizualizacja\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "        sns.histplot(df_res[\"cosine_sim\"], kde=True, color=\"green\", ax=axes[0])\n",
        "        axes[0].set_title(\"Stabilno≈õƒá Reprezentacji (Cosine)\")\n",
        "\n",
        "        sns.histplot(df_res[\"semantic_xai_sim\"], kde=True, color=\"purple\", ax=axes[1])\n",
        "        axes[1].set_title(\"Semantyczna Stabilno≈õƒá Wyja≈õnie≈Ñ\")\n",
        "        axes[1].set_xlabel(\"Semantic Similarity of Top-K\")\n",
        "        axes[1].axvline(0.7, color=\"red\", linestyle=\"--\", label=\"Pr√≥g stabilno≈õci (0.7)\")\n",
        "        axes[1].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{RESULTS_DIR}/stability_semantic_hist.png\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"‚ö† Brak danych do wy≈õwietlenia (wszystkie pr√≥by odrzucone).\")\n",
        "\n",
        "    print(f\"Modu≈Ç C zako≈Ñczony. Przetworzono: {len(df_res)} par.\")\n",
        "    return df_res, df_skipped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_mistral_model"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# LOAD MISTRAL MODEL FOR PARAPHRASING (Module C)\n",
        "# ===================================================\n",
        "\n",
        "print(\">>> Loading Mistral model for paraphrasing...\")\n",
        "\n",
        "MISTRAL_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "# 4-bit quantization configuration for GPU memory efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(MISTRAL_MODEL_ID)\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MISTRAL_MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Configure for generation (critical for causal models)\n",
        "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
        "mistral_tokenizer.padding_side = \"left\"\n",
        "\n",
        "print(\"‚úì Mistral model loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teXYB3iwtDyl"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# 6. MODU≈Å D: TEST SKUTECZNO≈öCI STEROWANIA (Steering)\n",
        "# ===================================================\n",
        "\n",
        "\n",
        "def run_module_d_steering(model, tokenizer, dataset, layer_activations, layer_labels):\n",
        "    \"\"\"\n",
        "    Testuje skuteczno≈õƒá in≈ºynierii reprezentacji (Representation Engineering) w sterowaniu zachowaniem modelu.\n",
        "\n",
        "    Metoda Difference of Means:\n",
        "        1. Oblicza ≈õredni wektor aktywacji dla przyk≈Çad√≥w toksycznych\n",
        "        2. Oblicza ≈õredni wektor aktywacji dla przyk≈Çad√≥w bezpiecznych\n",
        "        3. R√≥≈ºnica = wektor kierunkowy reprezentujƒÖcy koncept 'toksyczno≈õƒá'\n",
        "        4. Dodanie wektora z ujemnƒÖ si≈ÇƒÖ (alpha < 0) = detoksykacja\n",
        "\n",
        "    Argumenty:\n",
        "        model: Model DistilBERT do modyfikacji\n",
        "        tokenizer: Tokenizer modelu\n",
        "        dataset: Zbi√≥r danych testowych\n",
        "        layer_activations: Aktywacje z warstwy TARGET_LAYER_INDEX (z Modu≈Çu B)\n",
        "        layer_labels: Etykiety binarne dla pr√≥bek (z Modu≈Çu B)\n",
        "\n",
        "    Efekty uboczne:\n",
        "        Zapisuje raport skuteczno≈õci do pliku tekstowego\n",
        "\n",
        "    Metryki:\n",
        "        1. Detoxification Success Rate - % toksycznych pr√≥bek spadajƒÖcych poni≈ºej progu 0.5\n",
        "        2. Side Effects Rate - % bezpiecznych pr√≥bek fa≈Çszywie oznaczanych jako toksyczne\n",
        "\n",
        "    Warto≈õƒá STEERING_ALPHA = -3.0:\n",
        "        Ustalona eksperymentalnie jako optimum miƒôdzy skuteczno≈õciƒÖ detoksykacji\n",
        "        a minimalizacjƒÖ efekt√≥w ubocznych. Warto≈õci:\n",
        "        - alpha = -1.0: Za s≈Çabe, niewystarczajƒÖca detoksykacja\n",
        "        - alpha = -3.0: Optymalne (>80% sukcesu, <5% side effects)\n",
        "        - alpha = -5.0: Za mocne, zwiƒôkszone side effects\n",
        "    \"\"\"\n",
        "    print(\"\\n>>> [MODU≈Å D] Uruchamianie testu skuteczno≈õci sterowania...\")\n",
        "\n",
        "    # 1. Obliczanie wektora sterujƒÖcego (Difference of Means)\n",
        "    # U≈ºywamy danych przekazanych z Modu≈Çu B\n",
        "    toxic_vecs = layer_activations[layer_labels == 1]\n",
        "    safe_vecs = layer_activations[layer_labels == 0]\n",
        "\n",
        "    mean_toxic = np.mean(toxic_vecs, axis=0)\n",
        "    mean_safe = np.mean(safe_vecs, axis=0)\n",
        "    direction = mean_toxic - mean_safe\n",
        "    steering_tensor = torch.tensor(direction, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Hook class do interwencji w forward pass\n",
        "    class SteeringHook:\n",
        "        \"\"\"\n",
        "        PyTorch hook modyfikujƒÖcy hidden states poprzez dodanie wektora sterujƒÖcego.\n",
        "        Dzia≈Ça poprawnie zar√≥wno dla wyj≈õƒá typu tuple, jak i czystych Tensor√≥w.\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(self, vector, coeff):\n",
        "            self.vector = vector\n",
        "            self.coeff = coeff\n",
        "\n",
        "        def __call__(self, module, inputs, output):\n",
        "            # Sprawdzenie czy output to krotka (hidden_states, optional_attentions) czy sam Tensor\n",
        "            is_tuple = isinstance(output, tuple)\n",
        "            hidden_states = output[0] if is_tuple else output\n",
        "\n",
        "            # Upewnienie siƒô, ≈ºe wektor sterujƒÖcy jest na tym samym urzƒÖdzeniu i ma ten sam typ co dane\n",
        "            steering_vector = self.vector.to(hidden_states.device, dtype=hidden_states.dtype)\n",
        "\n",
        "            # Modyfikacja aktywacji\n",
        "            # (broadcasting doda wektor [768] do tensora [batch, seq, 768])\n",
        "            modified_hidden = hidden_states + (self.coeff * steering_vector)\n",
        "\n",
        "            if is_tuple:\n",
        "                # Je≈õli wej≈õcie by≈Ço krotkƒÖ, zwracamy krotkƒô (zachowujƒÖc np. attention weights je≈õli istniejƒÖ)\n",
        "                return (modified_hidden,) + output[1:]\n",
        "            else:\n",
        "                # Je≈õli wej≈õcie by≈Ço Tensorem, zwracamy zmodyfikowany Tensor\n",
        "                return modified_hidden\n",
        "\n",
        "    # Wyb√≥r podzbior√≥w do testowania\n",
        "    toxic_indices = [i for i, label in enumerate(dataset[\"labels\"]) if label[0] == 1][\n",
        "        :N_SAMPLES_XAI\n",
        "    ]\n",
        "    safe_indices = [i for i, label in enumerate(dataset[\"labels\"]) if label[0] == 0][\n",
        "        :N_SAMPLES_XAI\n",
        "    ]\n",
        "\n",
        "    success_count = 0\n",
        "    side_effect_count = 0\n",
        "\n",
        "    # Modu≈Ç warstwy TARGET_LAYER_INDEX (warstwa 5) - miejsce interwencji\n",
        "    layer_module = model.distilbert.transformer.layer[TARGET_LAYER_INDEX]\n",
        "\n",
        "    # === Ewaluacja skuteczno≈õci detoksykacji (toksyczne pr√≥bki) ===\n",
        "    handle = layer_module.register_forward_hook(\n",
        "        SteeringHook(steering_tensor, STEERING_ALPHA)\n",
        "    )\n",
        "\n",
        "    for idx in toxic_indices:\n",
        "        input_ids = dataset[idx][\"input_ids\"].unsqueeze(0).to(device)\n",
        "        mask = dataset[idx][\"attention_mask\"].unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(input_ids, attention_mask=mask)\n",
        "            prob = torch.sigmoid(out.logits)[0, 0].item()\n",
        "            if prob < CLASSIFICATION_THRESHOLD:  # Spad≈Ço poni≈ºej progu = sukces\n",
        "                success_count += 1\n",
        "\n",
        "    handle.remove()  # Usuniƒôcie hooka przed kolejnym krokiem\n",
        "\n",
        "    # === Ewaluacja efekt√≥w ubocznych (bezpieczne pr√≥bki) ===\n",
        "    handle = layer_module.register_forward_hook(\n",
        "        SteeringHook(steering_tensor, STEERING_ALPHA)\n",
        "    )\n",
        "\n",
        "    for idx in safe_indices:\n",
        "        input_ids = dataset[idx][\"input_ids\"].unsqueeze(0).to(device)\n",
        "        mask = dataset[idx][\"attention_mask\"].unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(input_ids, attention_mask=mask)\n",
        "            prob = torch.sigmoid(out.logits)[0, 0].item()\n",
        "            if prob > CLASSIFICATION_THRESHOLD:  # Sta≈Ç siƒô toksyczny = side effect\n",
        "                side_effect_count += 1\n",
        "\n",
        "    handle.remove()\n",
        "\n",
        "    # Obliczanie wska≈∫nik√≥w skuteczno≈õci\n",
        "    success_rate = (success_count / len(toxic_indices)) * 100\n",
        "    side_effect_rate = (side_effect_count / len(safe_indices)) * 100\n",
        "\n",
        "    status = \"SUKCES\" if success_rate > 80 and side_effect_rate < 5 else \"WYMAGA DOSTROJENIA\"\n",
        "\n",
        "    report = f\"\"\"\n",
        "    === RAPORT SKUTECZNO≈öCI STEROWANIA ===\n",
        "    Metoda: Difference of Means (Warstwa {TARGET_LAYER_INDEX})\n",
        "    Alpha: {STEERING_ALPHA}\n",
        "    Pr√≥bki: {len(toxic_indices)} toksycznych, {len(safe_indices)} bezpiecznych\n",
        "\n",
        "    1. Wska≈∫nik Sukcesu Detoksykacji: {success_rate:.2f}%\n",
        "        (Procent toksycznych pr√≥bek spadajƒÖcych poni≈ºej progu {CLASSIFICATION_THRESHOLD})\n",
        "\n",
        "    2. Wska≈∫nik Efekt√≥w Ubocznych: {side_effect_rate:.2f}%\n",
        "        (Procent bezpiecznych pr√≥bek b≈Çƒôdnie oznaczonych jako toksyczne)\n",
        "\n",
        "    Status: {status}\n",
        "\n",
        "    Uwagi:\n",
        "    - Cel: Success Rate > 80%, Side Effects < 5%\n",
        "    - Je≈õli wymaga dostrojenia, rozwa≈º zmianƒô STEERING_ALPHA\n",
        "    \"\"\"\n",
        "\n",
        "    print(report)\n",
        "    with open(f\"{RESULTS_DIR}/steering_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(report)\n",
        "    print(\"Modu≈Ç D zako≈Ñczony.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-T5e78rtEiM"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# 7. URUCHOMIENIE CA≈ÅO≈öCI\n",
        "# ===================================================\n",
        "print(f\"=== ROZPOCZƒòCIE EKSPERYMENTU (Wyniki -> {RESULTS_DIR}) ===\")\n",
        "\n",
        "# Uruchomienie modu≈Ç√≥w\n",
        "# Modu≈Ç A: Por√≥wnanie metod XAI\n",
        "run_module_a_xai(model, tokenizer, eval_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modu≈Ç B: Analiza warstwowa (zwraca dane dla Modu≈Çu D)\n",
        "_, layer_activations, layer_labels = run_module_b_repe(model, eval_dataset)"
      ],
      "metadata": {
        "id": "_l2SbPhxfQ5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modu≈Ç C: Test stabilno≈õci\n",
        "run_module_c_stability(model, tokenizer, eval_dataset)"
      ],
      "metadata": {
        "id": "ZcN33SgdfZXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modu≈Ç D: Test skuteczno≈õci sterowania (u≈ºywa danych z Modu≈Çu B)\n",
        "run_module_d_steering(model, tokenizer, eval_dataset, layer_activations, layer_labels)"
      ],
      "metadata": {
        "id": "1C9b-PwlfbRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== EKSPERYMENT ZAKO≈ÉCZONY ===\")\n",
        "print(f\"Wygenerowane pliki w {RESULTS_DIR}:\")\n",
        "print(os.listdir(RESULTS_DIR))"
      ],
      "metadata": {
        "id": "WZahS61sfd2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================\n",
        "# 8. WIZUALIZACJA WYNIK√ìW W NOTEBOOKU\n",
        "# ===================================================\n",
        "from IPython.display import Image, display, Markdown\n",
        "\n",
        "def show_final_summary():\n",
        "    display(Markdown(f\"# üìä Podsumowanie Eksperymentu\"))\n",
        "    display(Markdown(f\"Wyniki zapisane w: `{RESULTS_DIR}`\"))\n",
        "\n",
        "    # --- 1. Wy≈õwietlenie Raportu Sterowania ---\n",
        "    report_path = f\"{RESULTS_DIR}/steering_report.txt\"\n",
        "    if os.path.exists(report_path):\n",
        "        display(Markdown(\"## üéØ Modu≈Ç D: Raport Skuteczno≈õci Sterowania\"))\n",
        "        with open(report_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            # Wy≈õwietlamy raport w bloku kodu dla czytelno≈õci\n",
        "            report_content = f.read()\n",
        "            print(report_content)\n",
        "\n",
        "    # --- 2. Wy≈õwietlenie Wykres√≥w ---\n",
        "    plots = [\n",
        "        (\"Wierno≈õƒá wyja≈õnie≈Ñ XAI (Modu≈Ç A)\", \"xai_boxplot.png\"),\n",
        "        (\"Separowalno≈õƒá warstwowa RepE (Modu≈Ç B)\", \"repe_layer_plot.png\"),\n",
        "        (\"Stabilno≈õƒá wobec parafraz (Modu≈Ç C)\", \"stability_combined_hist.png\")\n",
        "    ]\n",
        "\n",
        "    for title, filename in plots:\n",
        "        path = f\"{RESULTS_DIR}/{filename}\"\n",
        "        if os.path.exists(path):\n",
        "            display(Markdown(f\"## üìà {title}\"))\n",
        "            display(Image(filename=path))\n",
        "        else:\n",
        "            print(f\"Brak pliku: {filename}\")\n",
        "\n",
        "    # --- 3. PodglƒÖd tabel wynik√≥w ---\n",
        "    display(Markdown(\"## üìã PodglƒÖd danych (Top 5 wierszy)\"))\n",
        "    csv_files = [\n",
        "        \"xai_comparison_results.csv\",\n",
        "        \"repe_layer_performance.csv\",\n",
        "        \"stability_mistral_results.csv\"\n",
        "    ]\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "        path = f\"{RESULTS_DIR}/{csv_file}\"\n",
        "        if os.path.exists(path):\n",
        "            display(Markdown(f\"**Plik:** `{csv_file}`\"))\n",
        "            df_temp = pd.read_csv(path)\n",
        "            display(df_temp.head(5))\n",
        "\n",
        "# Uruchomienie wy≈õwietlania\n",
        "show_final_summary()"
      ],
      "metadata": {
        "id": "EM_nesXdkmo5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}