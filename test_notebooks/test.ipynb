{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbanot/msc-project/blob/main/test_notebooks/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ujx4GU0pSU3"
      },
      "outputs": [],
      "source": [
        "!uv pip install transformers datasets captum quantus accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhXDPZFkpcyJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "from datasets import Dataset\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForSeq2SeqLM,\n",
        ")\n",
        "from captum.attr import IntegratedGradients, InputXGradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpEzoMGNphyt"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EwfO5X0pjE7"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# 1. KONFIGURACJA GLOBALNA\n",
        "# ===================================================\n",
        "\n",
        "# === Parametry analizy XAI ===\n",
        "N_SAMPLES_XAI = (\n",
        "    100  # Liczba próbek dla metod XAI (Integrated Gradients / InputXGradient)\n",
        ")\n",
        "N_SAMPLES_PROBE = 1000  # Liczba próbek do analizy warstwowej metodą RepE\n",
        "N_SAMPLES_STABILITY = 50  # Liczba par tekst-parafraza do testu stabilności\n",
        "BATCH_SIZE = (\n",
        "    32  # Rozmiar batcha dla przetwarzania wsadowego (optymalizacja pamięci GPU)\n",
        ")\n",
        "TOP_K_TOKENS = (\n",
        "    5  # Liczba najważniejszych tokenów do usunięcia w metryce Comprehensiveness\n",
        ")\n",
        "DF_SIZE = 3000  # Ograniczenie wielkości zbioru danych (dla szybszego testowania)\n",
        "\n",
        "# === Długość sekwencji ===\n",
        "MAX_SEQUENCE_LENGTH = 256  # Maksymalna długość sekwencji tokenów\n",
        "\n",
        "# === Indeksy warstw do analizy ===\n",
        "TARGET_LAYER_INDEX = 5  # Warstwa docelowa do analizy (warstwa 5 wykazała najlepszą separowalność liniową)\n",
        "STEERING_ALPHA = -3.0  # Siła wektora sterującego (ujemna wartość = detoksykacja)\n",
        "\n",
        "# === Próg klasyfikacji ===\n",
        "CLASSIFICATION_THRESHOLD = 0.5  # Próg prawdopodobieństwa dla klasyfikacji binarnej\n",
        "\n",
        "# === Parametry testu stabilności (Moduł C) ===\n",
        "PARAPHRASE_MIN_SIMILARITY = 0.7  # Minimalny cosine similarity dla akceptacji parafrazy\n",
        "PARAPHRASE_SEED = 42  # Seed dla reproducibility generowania parafraz\n",
        "\n",
        "# === Ścieżki ===\n",
        "DATA_PATH = \"/drive/MyDrive/msc-project/jigsaw-toxic-comment/train.csv\"\n",
        "\n",
        "# Dodaj timestamp do nazwy katalogu, aby nie nadpisywać poprzednich wyników\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "RESULTS_DIR = f\"/drive/MyDrive/msc-project/results_final_{TIMESTAMP}\"\n",
        "\n",
        "MODEL_CHECKPOINT = \"/drive/MyDrive/msc-project/models/distilbert-jigsaw-full\"\n",
        "\n",
        "# === Urządzenie obliczeniowe ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Uruchomiono na urządzeniu: {device}\")\n",
        "\n",
        "# Tworzenie katalogu wyników\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-EJ7Ktip8SI"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# 2. PRZYGOTOWANIE DANYCH I MODELU\n",
        "# ===================================================\n",
        "\n",
        "\n",
        "def clean_text(example):\n",
        "    \"\"\"\n",
        "    Czyści tekst komentarza, usuwając niepożądane elementy i normalizując format.\n",
        "\n",
        "    Funkcja stosowana jest zarówno podczas treningu jak i ewaluacji, aby zapewnić\n",
        "    spójność przetwarzania danych.\n",
        "\n",
        "    Argumenty:\n",
        "        example: Słownik zawierający klucz 'comment_text' z tekstem do oczyszczenia\n",
        "\n",
        "    Zwraca:\n",
        "        Zmodyfikowany słownik example z oczyszczonym tekstem w polu 'comment_text'\n",
        "\n",
        "    Operacje czyszczenia:\n",
        "        - Konwersja na małe litery (wymagane dla modeli BERT typu uncased)\n",
        "        - Usunięcie linków URL (http/https/www)\n",
        "        - Usunięcie adresów IP\n",
        "        - Usunięcie metadanych Wikipedii (talk pages, timestampy UTC)\n",
        "        - Normalizacja białych znaków (spacje, newline, non-breaking space)\n",
        "        - Usunięcie cudzysłowów z początku i końca\n",
        "    \"\"\"\n",
        "    text = example[\"comment_text\"]\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \"\", text)\n",
        "    text = re.sub(r\"\\(talk\\)\", \"\", text)\n",
        "    text = re.sub(r\"\\d{2}:\\d{2}, \\w+ \\d{1,2}, \\d{4} \\(utc\\)\", \"\", text)\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\xa0\", \" \")\n",
        "    text = text.strip(' \"')\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    example[\"comment_text\"] = text\n",
        "    return example\n",
        "\n",
        "\n",
        "def prepare_environment():\n",
        "    \"\"\"\n",
        "    Przygotowuje środowisko eksperymentalne: wczytuje dane, tokenizuje i ładuje model.\n",
        "\n",
        "    Zwraca:\n",
        "        Tuple zawierający:\n",
        "        - model: Wytrenowany model DistilBERT do klasyfikacji toksyczności\n",
        "        - tokenizer: Tokenizer dopasowany do modelu\n",
        "        - eval_dataset: Zbiór testowy z przetworzonymi danymi\n",
        "\n",
        "    Kroki przygotowania:\n",
        "        1. Wczytanie danych z pliku CSV\n",
        "        2. Preprocessing tekstów\n",
        "        3. Ładowanie tokenizera\n",
        "        4. Tokenizacja tekstów (padding do MAX_SEQUENCE_LENGTH)\n",
        "        5. Przygotowanie etykiet binary classification\n",
        "        6. Podział na zbiór treningowy i testowy\n",
        "        7. Załadowanie wytrenowanego modelu\n",
        "    \"\"\"\n",
        "    print(\">>> [SETUP] Wczytywanie i przetwarzanie danych...\")\n",
        "\n",
        "    # 1. Wczytanie danych\n",
        "    try:\n",
        "        df = pd.read_csv(DATA_PATH).head(DF_SIZE)\n",
        "        dataset = Dataset.from_pandas(df)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Nie znaleziono pliku: {DATA_PATH}. Sprawdź ścieżkę w Konfiguracji Globalnej.\"\n",
        "        )\n",
        "\n",
        "    # 2. Preprocessing\n",
        "    dataset = dataset.map(clean_text)\n",
        "\n",
        "    # 3. Ładowanie tokenizera zgodnego z modelem\n",
        "    print(f\">>> [SETUP] Ładowanie tokenizera z: {MODEL_CHECKPOINT}...\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "    except OSError:\n",
        "        print(\n",
        "            f\"Błąd: Nie znaleziono tokenizera w {MODEL_CHECKPOINT}. Pobieram domyślny 'distilbert-base-uncased'.\"\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "    # 4. Tokenizacja\n",
        "    def tokenize_function(examples):\n",
        "        \"\"\"Tokenizuje teksty z paddingiem do stałej długości MAX_SEQUENCE_LENGTH.\"\"\"\n",
        "        return tokenizer(\n",
        "            examples[\"comment_text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=MAX_SEQUENCE_LENGTH,\n",
        "        )\n",
        "\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    # 5. Przygotowanie etykiet binary classification\n",
        "    label_cols = [\n",
        "        \"toxic\",\n",
        "    ]\n",
        "\n",
        "    def create_labels(example):\n",
        "        \"\"\"Pobiera kolumnę 'toxic' i tworzy etykietę.\"\"\"\n",
        "        example[\"labels\"] = [float(example[col]) for col in label_cols]\n",
        "        return example\n",
        "\n",
        "    final_dataset = tokenized_dataset.map(create_labels)\n",
        "\n",
        "    # Ustawienie formatu PyTorch (usunięcie kolumn tekstowych, zachowanie tylko tensorów)\n",
        "    cols_to_keep = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "    final_dataset.set_format(\"torch\", columns=cols_to_keep)\n",
        "\n",
        "    # 6. Podział na zbiór treningowy i testowy\n",
        "    splits = final_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "    eval_dataset = splits[\"test\"]\n",
        "\n",
        "    # 7. Ładowanie wytrenowanego modelu\n",
        "    print(f\">>> [SETUP] Ładowanie wytrenowanego modelu z: {MODEL_CHECKPOINT}...\")\n",
        "    try:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            MODEL_CHECKPOINT, num_labels=1, problem_type=\"single_label_classification\"\n",
        "        )\n",
        "    except OSError:\n",
        "        raise OSError(\n",
        "            f\"Nie znaleziono modelu w ścieżce: {MODEL_CHECKPOINT}. Upewnij się, że najpierw uruchomiłeś skrypt treningowy.\"\n",
        "        )\n",
        "\n",
        "    # Przełączenie w tryb ewaluacji (wyłącza dropout i batch normalization)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\">>> [SETUP] Środowisko gotowe. Urządzenie: {device}\")\n",
        "    return model, tokenizer, eval_dataset\n",
        "\n",
        "\n",
        "# Inicjalizacja środowiska\n",
        "model, tokenizer, eval_dataset = prepare_environment()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1uG43ALs8lO"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# 3. MODUŁ A: PORÓWNANIE METOD XAI (Comprehensiveness)\n",
        "# ===================================================\n",
        "\n",
        "\n",
        "def run_module_a_xai(model, tokenizer, dataset):\n",
        "    \"\"\"\n",
        "    Porównuje metody XAI (Integrated Gradients vs InputXGradient) pod kątem wierności wyjaśnień.\n",
        "\n",
        "    Metryka Comprehensiveness mierzy, jak bardzo usuniecie najważniejszych tokenów\n",
        "    (zidentyfikowanych przez metodę XAI) wpływa na pewność predykcji modelu.\n",
        "    Wyższy spadek pewności = lepsza metoda XAI.\n",
        "\n",
        "    Argumenty:\n",
        "        model: Wytrenowany model klasyfikacyjny DistilBERT\n",
        "        tokenizer: Tokenizer odpowiadający modelowi\n",
        "        dataset: Zbiór danych z etykietami\n",
        "\n",
        "    Zwraca:\n",
        "        DataFrame z wynikami porównania metod (drop scores dla IG i IxG)\n",
        "\n",
        "    Metodologia:\n",
        "        1. Wybór podzbioru toksycznych przykładów (N_SAMPLES_XAI)\n",
        "        2. Dla każdego przykładu:\n",
        "            a) Obliczenie oryginalnego prawdopodobieństwa toksyczności\n",
        "            b) Identyfikacja TOP_K_TOKENS najważniejszych tokenów (IG i InputXGradient)\n",
        "            c) Maskowanie tych tokenów i ponowna predykcja\n",
        "            d) Obliczenie spadku pewności (comprehensiveness score)\n",
        "        3. Wizualizacja wyników jako boxplot\n",
        "    \"\"\"\n",
        "    print(\"\\n>>> [MODUŁ A] Uruchamianie porównania metod XAI (IG vs IxG)...\")\n",
        "    model.eval()\n",
        "\n",
        "    # Filtrowanie tylko toksycznych przykładów (indeks 0 = etykieta 'toxic')\n",
        "    toxic_indices = [i for i, labels in enumerate(dataset[\"labels\"]) if labels[0] == 1]\n",
        "    subset_indices = toxic_indices[:N_SAMPLES_XAI]\n",
        "    subset = dataset.select(subset_indices)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Funkcja pomocnicza dla Captum (zwraca logity na podstawie embeddings)\n",
        "    def predict_func(inputs_embeds, attention_mask=None):\n",
        "        \"\"\"Wrapper predykcji dla biblioteki Captum.\"\"\"\n",
        "        return model(inputs_embeds=inputs_embeds, attention_mask=attention_mask).logits\n",
        "\n",
        "    ig = IntegratedGradients(predict_func)\n",
        "    ixg = InputXGradient(predict_func)\n",
        "\n",
        "    for i in tqdm(range(len(subset)), desc=\"Ewaluacja XAI\"):\n",
        "        input_ids = subset[i][\"input_ids\"].unsqueeze(0).to(device)\n",
        "        attention_mask = subset[i][\"attention_mask\"].unsqueeze(0).to(device)\n",
        "        input_embeds = model.distilbert.embeddings(input_ids)\n",
        "\n",
        "        # Baseline = embedding tokena [PAD] (punkt odniesienia dla IG)\n",
        "        baseline = model.distilbert.embeddings(\n",
        "            torch.tensor(\n",
        "                [tokenizer.pad_token_id] * MAX_SEQUENCE_LENGTH, device=device\n",
        "            ).unsqueeze(0)\n",
        "        )\n",
        "\n",
        "        # 1. Oryginalne prawdopodobieństwo toksyczności\n",
        "        with torch.no_grad():\n",
        "            orig_out = model(inputs_embeds=input_embeds, attention_mask=attention_mask)\n",
        "            orig_prob = torch.sigmoid(orig_out.logits)[0, 0].item()\n",
        "\n",
        "        # Funkcja pomocnicza do obliczania spadku pewności\n",
        "        def calculate_drop(attr_tensor):\n",
        "            \"\"\"\n",
        "            Oblicza spadek pewności po usunięciu TOP_K najważniejszych tokenów.\n",
        "\n",
        "            Argumenty:\n",
        "                attr_tensor: Tensor atrybutów z metody XAI\n",
        "\n",
        "            Zwraca:\n",
        "                Spadek prawdopodobieństwa (orig_prob - new_prob)\n",
        "            \"\"\"\n",
        "            # Suma po wymiarze embeddingów -> ważność na poziomie tokenów\n",
        "            attr_sum = attr_tensor.sum(dim=-1).squeeze(0)\n",
        "            # Znajdź TOP_K najważniejszych tokenów\n",
        "            _, top_indices = torch.topk(attr_sum, k=TOP_K_TOKENS)\n",
        "\n",
        "            # Maskowanie tokenów (zamiana na [PAD])\n",
        "            masked_ids = input_ids.clone()\n",
        "            masked_ids[0, top_indices] = tokenizer.pad_token_id\n",
        "\n",
        "            with torch.no_grad():\n",
        "                new_out = model(masked_ids, attention_mask=attention_mask)\n",
        "                new_prob = torch.sigmoid(new_out.logits)[0, 0].item()\n",
        "\n",
        "            return orig_prob - new_prob\n",
        "\n",
        "        # 2. Metoda Integrated Gradients\n",
        "        attr_ig, _ = ig.attribute(\n",
        "            inputs=input_embeds,\n",
        "            baselines=baseline,\n",
        "            target=0,\n",
        "            additional_forward_args=(attention_mask,),\n",
        "            return_convergence_delta=True,\n",
        "        )\n",
        "        drop_ig = calculate_drop(attr_ig)\n",
        "\n",
        "        # 3. Metoda InputXGradient\n",
        "        attr_ixg = ixg.attribute(\n",
        "            inputs=input_embeds, target=0, additional_forward_args=(attention_mask,)\n",
        "        )\n",
        "        drop_ixg = calculate_drop(attr_ixg)\n",
        "\n",
        "        results.append(\n",
        "            {\n",
        "                \"text_id\": i,\n",
        "                \"original_prob\": orig_prob,\n",
        "                \"ig_drop_score\": drop_ig,\n",
        "                \"ixg_drop_score\": drop_ixg,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # Zapis wyników i wizualizacja\n",
        "    df_res = pd.DataFrame(results)\n",
        "    df_res.to_csv(f\"{RESULTS_DIR}/xai_comparison_results.csv\", index=False)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.boxplot(data=df_res[[\"ig_drop_score\", \"ixg_drop_score\"]])\n",
        "    plt.title(\n",
        "        f\"Comprehensiveness (Spadek Pewności) - Usunięto {TOP_K_TOKENS} Najważniejszych Tokenów\"\n",
        "    )\n",
        "    plt.ylabel(\"Spadek Prawdopodobieństwa\")\n",
        "    plt.savefig(f\"{RESULTS_DIR}/xai_boxplot.png\")\n",
        "    plt.close()\n",
        "    print(\"Moduł A zakończony.\")\n",
        "    return df_res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxB9jgU9s_IX"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# 4. MODUŁ B: ANALIZA WARSTWOWA (RepE)\n",
        "# ===================================================\n",
        "\n",
        "\n",
        "def run_module_b_repe(model, dataset):\n",
        "    \"\"\"\n",
        "    Przeprowadza analizę warstwową metodą Representation Engineering (RepE).\n",
        "\n",
        "    Metoda trenuje liniowe sondy (linear probes) dla każdej warstwy transformera,\n",
        "    aby określić, w której warstwie reprezentacja koncepcji 'toksyczność' jest\n",
        "    najbardziej liniowo separowalna.\n",
        "\n",
        "    Argumenty:\n",
        "        model: Model DistilBERT z aktywowanym output_hidden_states\n",
        "        dataset: Zbiór danych do analizy\n",
        "\n",
        "    Zwraca:\n",
        "        Tuple zawierający:\n",
        "        - df_res: DataFrame z wynikami performance per warstwa\n",
        "        - target_layer_activations: Aktywacje z warstwy TARGET_LAYER_INDEX\n",
        "        - target_layer_labels: Etykiety binarne dla próbek\n",
        "\n",
        "    Efekty uboczne:\n",
        "        - Zapisuje wyniki do pliku CSV\n",
        "        - Zapisuje wykres performance vs warstwa\n",
        "\n",
        "    Struktura DistilBERT:\n",
        "        - Warstwa 0: Warstwa embeddingów (bez transformacji kontekstowej)\n",
        "        - Warstwy 1-6: Warstwy transformera (6 bloków self-attention + FFN)\n",
        "\n",
        "    Hipoteza:\n",
        "        Środkowe warstwy (4-5) powinny mieć najlepszą reprezentację semantyczną,\n",
        "        ponieważ łączą składnię (niższe warstwy) z semantyką (wyższe warstwy).\n",
        "    \"\"\"\n",
        "    print(\"\\n>>> [MODUŁ B] Uruchamianie analizy warstwowej (RepE)...\")\n",
        "    model.eval()\n",
        "\n",
        "    # Wybór podzbioru (ograniczenie dla szybkości)\n",
        "    subset = dataset.select(range(min(len(dataset), N_SAMPLES_PROBE)))\n",
        "\n",
        "    # Słownik przechowujący aktywacje dla każdej warstwy\n",
        "    # DistilBERT: 1 warstwa embeddingów + 6 warstw transformera = 7 hidden states\n",
        "    layers_data = {i: [] for i in range(7)}\n",
        "    all_labels = []\n",
        "\n",
        "    loader = torch.utils.data.DataLoader(subset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    for batch in tqdm(loader, desc=\"Ekstrakcja Warstw\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"][:, 0].numpy()  # Tylko etykieta 'toxic' (indeks 0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(input_ids, attention_mask=mask, output_hidden_states=True)\n",
        "\n",
        "        all_labels.extend(labels)\n",
        "\n",
        "        # Ekstrakcja tokena [CLS] (indeks 0) z każdej warstwy\n",
        "        # Token [CLS] zawiera zagregowaną reprezentację całej sekwencji\n",
        "        for i, hidden in enumerate(out.hidden_states):\n",
        "            layers_data[i].append(hidden[:, 0, :].cpu().numpy())\n",
        "\n",
        "    # Trenowanie sond liniowych dla każdej warstwy\n",
        "    results = []\n",
        "    y = np.array(all_labels)\n",
        "    y_bin = (y > CLASSIFICATION_THRESHOLD).astype(int)  # Binaryzacja etykiet\n",
        "\n",
        "    # Zmienne do przechowania aktywacji docelowej warstwy\n",
        "    target_layer_activations = None\n",
        "    target_layer_labels = None\n",
        "\n",
        "    for layer_idx in sorted(layers_data.keys()):\n",
        "        X = np.concatenate(layers_data[layer_idx], axis=0)\n",
        "\n",
        "        # Podział na zbiór treningowy i testowy dla sondy\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y_bin, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Regresja logistyczna jako sonda liniowa\n",
        "        # max_iter=1000 zapewnia zbieżność dla wysokowymiarowych danych\n",
        "        clf = LogisticRegression(max_iter=1000, solver=\"liblinear\")\n",
        "        clf.fit(X_train, y_train)\n",
        "        preds = clf.predict(X_test)\n",
        "\n",
        "        acc = accuracy_score(y_test, preds)\n",
        "        f1 = f1_score(y_test, preds)\n",
        "\n",
        "        results.append({\"layer\": layer_idx, \"accuracy\": acc, \"f1_score\": f1})\n",
        "\n",
        "        # Zapisujemy aktywacje warstwy docelowej do wykorzystania w Module D (steering)\n",
        "        # Wybór warstwy TARGET_LAYER_INDEX uzasadniony jest na podstawie:\n",
        "        #   1. Literatury: środkowe warstwy transformera łączą składnię (niższe) z semantyką (wyższe)\n",
        "        #   2. Wyników tego modułu: wykres pokaże, że warstwa ta ma wysoką separowalność liniową\n",
        "        #   3. Poprzednich eksperymentów: warstwa 5 wykazała najlepszą jakość reprezentacji toksyczności\n",
        "        if layer_idx == TARGET_LAYER_INDEX:\n",
        "            target_layer_activations = X\n",
        "            target_layer_labels = y_bin\n",
        "\n",
        "    df_res = pd.DataFrame(results)\n",
        "    df_res.to_csv(f\"{RESULTS_DIR}/repe_layer_performance.csv\", index=False)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.lineplot(data=df_res, x=\"layer\", y=\"accuracy\", marker=\"o\", label=\"Dokładność\")\n",
        "    sns.lineplot(data=df_res, x=\"layer\", y=\"f1_score\", marker=\"s\", label=\"F1 Score\")\n",
        "    plt.title(\"Wydajność Sondy Liniowej per Warstwa\")\n",
        "    plt.xlabel(\"Numer Warstwy (0=Embeddings, 1-6=Transformer)\")\n",
        "    plt.ylabel(\"Metryka\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"{RESULTS_DIR}/repe_layer_plot.png\")\n",
        "    plt.close()\n",
        "    print(\"Moduł B zakończony.\")\n",
        "    \n",
        "    return df_res, target_layer_activations, target_layer_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCvsbqzctBYY"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# 5. MODUŁ C: TEST STABILNOŚCI (Robustness)\n",
        "# ===================================================\n",
        "\n",
        "\n",
        "def run_module_c_stability(model, tokenizer, dataset):\n",
        "    \"\"\"\n",
        "    Testuje stabilność modelu i metod XAI wobec parafraz tekstowych.\n",
        "\n",
        "    Wykorzystuje model T5 do generowania parafraz, a następnie mierzy trzy aspekty stabilności:\n",
        "    1. Output Stability - Jak bardzo zmienia się predykcja\n",
        "    2. Representation Stability - Jak podobne są reprezentacje (cosine similarity)\n",
        "    3. Explanation Stability - Jak podobne są wyjaśnienia XAI (Jaccard Index)\n",
        "\n",
        "    Argumenty:\n",
        "        model: Model klasyfikacyjny do testowania\n",
        "        tokenizer: Tokenizer dla modelu\n",
        "        dataset: Zbiór danych testowych\n",
        "\n",
        "    Zwraca:\n",
        "        DataFrame z wynikami stabilności\n",
        "\n",
        "    Metodologia:\n",
        "        - Dla każdej pary (tekst oryginalny, parafraza):\n",
        "            1. Walidacja jakości parafrazy (cosine similarity > PARAPHRASE_MIN_SIMILARITY)\n",
        "            2. Obliczamy różnicę w prawdopodobieństwie (Output Stability)\n",
        "            3. Obliczamy podobieństwo cosine reprezentacji z warstwy 5 (Representation Stability)\n",
        "            4. Obliczamy Jaccard Index dla Top-K tokenów z IG (Explanation Stability)\n",
        "\n",
        "    Interpretacja:\n",
        "        - Cosine Similarity > 0.9: Wysoka stabilność reprezentacji\n",
        "        - Jaccard Index > 0.5: Wysoka stabilność wyjaśnień\n",
        "        - Pred Diff < 0.1: Wysoka stabilność outputu\n",
        "    \"\"\"\n",
        "    print(\"\\n>>> [MODUŁ C] Uruchamianie analizy stabilności z T5...\")\n",
        "\n",
        "    # Ustawienie seed dla reproducibility\n",
        "    torch.manual_seed(PARAPHRASE_SEED)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(PARAPHRASE_SEED)\n",
        "\n",
        "    # Wczytanie modelu T5 do generowania parafraz\n",
        "    # Model Vamsi/T5_Paraphrase_Paws jest wyspecjalizowany w parafrazowaniu\n",
        "    t5_name = \"Vamsi/T5_Paraphrase_Paws\"\n",
        "    t5_tok = AutoTokenizer.from_pretrained(t5_name)\n",
        "    t5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_name).to(device)\n",
        "\n",
        "    # Wybór podzbioru toksycznych próbek\n",
        "    toxic_indices = [i for i, label in enumerate(dataset[\"labels\"]) if label[0] == 1]\n",
        "    sample_indices = toxic_indices[:N_SAMPLES_STABILITY]\n",
        "\n",
        "    results = []\n",
        "    skipped_count = 0  # Licznik odrzuconych parafraz\n",
        "\n",
        "    def get_embedding(text, l_idx=TARGET_LAYER_INDEX):\n",
        "        \"\"\"\n",
        "        Pobiera embedding [CLS] z określonej warstwy oraz prawdopodobieństwo toksyczności.\n",
        "\n",
        "        Argumenty:\n",
        "            text: Tekst wejściowy\n",
        "            l_idx: Indeks warstwy (domyślnie TARGET_LAYER_INDEX=5)\n",
        "\n",
        "        Zwraca:\n",
        "            Tuple (embedding_vector, toxic_probability)\n",
        "        \"\"\"\n",
        "        in_ids = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=MAX_SEQUENCE_LENGTH,\n",
        "        ).to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(**in_ids, output_hidden_states=True)\n",
        "        return out.hidden_states[l_idx][0, 0, :], torch.sigmoid(out.logits)[0, 0].item()\n",
        "\n",
        "    # Przygotowanie funkcji predykcji dla Integrated Gradients (tworzona raz, nie w pętli)\n",
        "    def predict_func_for_ig(inputs_embeds):\n",
        "        \"\"\"Wrapper predykcji dla biblioteki Captum.\"\"\"\n",
        "        return model(inputs_embeds=inputs_embeds).logits\n",
        "\n",
        "    ig = IntegratedGradients(predict_func_for_ig)\n",
        "\n",
        "    def get_top_tokens(text):\n",
        "        \"\"\"\n",
        "        Identyfikuje Top-K najważniejszych tokenów przy użyciu Integrated Gradients.\n",
        "\n",
        "        Argumenty:\n",
        "            text: Tekst wejściowy\n",
        "\n",
        "        Zwraca:\n",
        "            Set zawierający najważniejsze tokeny\n",
        "\n",
        "        Uwagi:\n",
        "            - Używa spójnego tokenizowania z resztą pipeline'u\n",
        "            - Używa baseline z PAD embeddings (spójność z Modułem A)\n",
        "            - n_steps=10 dla szybkości (kompromis dokładność/czas)\n",
        "        \"\"\"\n",
        "        in_ids = tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=MAX_SEQUENCE_LENGTH,\n",
        "        ).to(device)\n",
        "        \n",
        "        emb = model.distilbert.embeddings(in_ids[\"input_ids\"])\n",
        "        \n",
        "        # Baseline = embedding tokena [PAD] (spójność z Modułem A)\n",
        "        baseline = model.distilbert.embeddings(\n",
        "            torch.tensor(\n",
        "                [tokenizer.pad_token_id] * MAX_SEQUENCE_LENGTH, device=device\n",
        "            ).unsqueeze(0)\n",
        "        )\n",
        "        \n",
        "        attr = ig.attribute(\n",
        "            emb,\n",
        "            baselines=baseline,\n",
        "            target=0,\n",
        "            n_steps=10,  # n_steps=10 dla szybkości (kompromis dokładność/czas)\n",
        "        )\n",
        "        attr_sum = attr.sum(dim=-1).squeeze(0)\n",
        "        _, idx = torch.topk(attr_sum, k=TOP_K_TOKENS)\n",
        "        return set(tokenizer.convert_ids_to_tokens(in_ids[\"input_ids\"][0][idx]))\n",
        "\n",
        "    for idx in tqdm(sample_indices, desc=\"Analiza Stabilności\"):\n",
        "        orig_ids = dataset[idx][\"input_ids\"]\n",
        "        orig_text = tokenizer.decode(orig_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Generowanie parafrazy z T5\n",
        "        # Prefix \"paraphrase:\" jest wymagany przez ten model\n",
        "        t5_input = t5_tok(\n",
        "            \"paraphrase: \" + orig_text + \" </s>\", return_tensors=\"pt\", padding=True\n",
        "        ).to(device)\n",
        "        t5_out = t5_model.generate(\n",
        "            t5_input.input_ids, max_length=MAX_SEQUENCE_LENGTH, do_sample=True, top_k=50\n",
        "        )\n",
        "        para_text = t5_tok.decode(t5_out[0], skip_special_tokens=True)\n",
        "\n",
        "        # Pobieramy embeddingi i prawdopodobieństwa (FIX: usunięto duplikację wywołań)\n",
        "        vec_orig, prob_orig = get_embedding(orig_text)\n",
        "        vec_para, prob_para = get_embedding(para_text)\n",
        "\n",
        "        # Walidacja jakości parafrazy (cosine similarity)\n",
        "        paraphrase_similarity = F.cosine_similarity(\n",
        "            vec_orig.unsqueeze(0), vec_para.unsqueeze(0)\n",
        "        ).item()\n",
        "\n",
        "        # Odrzucamy parafrazy zbyt różne od oryginału\n",
        "        if paraphrase_similarity < PARAPHRASE_MIN_SIMILARITY:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        # 1. Output Stability - Różnica w prawdopodobieństwie\n",
        "        prob_diff = abs(prob_orig - prob_para)\n",
        "\n",
        "        # 2. Representation Stability - Podobieństwo cosine (już obliczone powyżej)\n",
        "        cos_sim = paraphrase_similarity\n",
        "\n",
        "        # 3. Explanation Stability - Jaccard Index dla Top-K tokenów\n",
        "        try:\n",
        "            toks_orig = get_top_tokens(orig_text)\n",
        "            toks_para = get_top_tokens(para_text)\n",
        "            intersect = len(toks_orig.intersection(toks_para))\n",
        "            union = len(toks_orig.union(toks_para))\n",
        "            jaccard = intersect / union if union > 0 else 0\n",
        "        except Exception:\n",
        "            jaccard = 0.0  # Zabezpieczenie dla bardzo krótkich tekstów\n",
        "\n",
        "        results.append(\n",
        "            {\n",
        "                \"prob_diff\": prob_diff,\n",
        "                \"cosine_sim\": cos_sim,\n",
        "                \"jaccard_ig\": jaccard,\n",
        "                \"paraphrase_quality\": paraphrase_similarity,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    print(f\"Przetworzono {len(results)} par, odrzucono {skipped_count} parafraz o niskiej jakości\")\n",
        "\n",
        "    df_res = pd.DataFrame(results)\n",
        "    df_res.to_csv(f\"{RESULTS_DIR}/stability_results.csv\", index=False)\n",
        "\n",
        "    # Wizualizacja z dodatkową informacją o jakości parafraz\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    sns.histplot(df_res[\"cosine_sim\"], kde=True, color=\"green\", ax=axes[0])\n",
        "    axes[0].set_title(f\"Rozkład Stabilności Reprezentacji (Warstwa {TARGET_LAYER_INDEX})\")\n",
        "    axes[0].set_xlabel(\"Podobieństwo Cosine\")\n",
        "    axes[0].axvline(\n",
        "        0.9, color=\"red\", linestyle=\"--\", label=\"Próg wysokiej stabilności (0.9)\"\n",
        "    )\n",
        "    axes[0].legend()\n",
        "\n",
        "    sns.histplot(df_res[\"jaccard_ig\"], kde=True, color=\"blue\", ax=axes[1])\n",
        "    axes[1].set_title(\"Rozkład Stabilności Wyjaśnień (Jaccard Index)\")\n",
        "    axes[1].set_xlabel(\"Jaccard Index\")\n",
        "    axes[1].axvline(\n",
        "        0.5, color=\"red\", linestyle=\"--\", label=\"Próg wysokiej stabilności (0.5)\"\n",
        "    )\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{RESULTS_DIR}/stability_combined_hist.png\")\n",
        "    plt.close()\n",
        "    print(\"Moduł C zakończony.\")\n",
        "    \n",
        "    return df_res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teXYB3iwtDyl"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# 6. MODUŁ D: TEST SKUTECZNOŚCI STEROWANIA (Steering)\n",
        "# ===================================================\n",
        "\n",
        "\n",
        "def run_module_d_steering(model, tokenizer, dataset, layer_activations, layer_labels):\n",
        "    \"\"\"\n",
        "    Testuje skuteczność inżynierii reprezentacji (Representation Engineering) w sterowaniu zachowaniem modelu.\n",
        "\n",
        "    Metoda Difference of Means:\n",
        "        1. Oblicza średni wektor aktywacji dla przykładów toksycznych\n",
        "        2. Oblicza średni wektor aktywacji dla przykładów bezpiecznych\n",
        "        3. Różnica = wektor kierunkowy reprezentujący koncept 'toksyczność'\n",
        "        4. Dodanie wektora z ujemną siłą (alpha < 0) = detoksykacja\n",
        "\n",
        "    Argumenty:\n",
        "        model: Model DistilBERT do modyfikacji\n",
        "        tokenizer: Tokenizer modelu\n",
        "        dataset: Zbiór danych testowych\n",
        "        layer_activations: Aktywacje z warstwy TARGET_LAYER_INDEX (z Modułu B)\n",
        "        layer_labels: Etykiety binarne dla próbek (z Modułu B)\n",
        "\n",
        "    Efekty uboczne:\n",
        "        Zapisuje raport skuteczności do pliku tekstowego\n",
        "\n",
        "    Metryki:\n",
        "        1. Detoxification Success Rate - % toksycznych próbek spadających poniżej progu 0.5\n",
        "        2. Side Effects Rate - % bezpiecznych próbek fałszywie oznaczanych jako toksyczne\n",
        "\n",
        "    Wartość STEERING_ALPHA = -3.0:\n",
        "        Ustalona eksperymentalnie jako optimum między skutecznością detoksykacji\n",
        "        a minimalizacją efektów ubocznych. Wartości:\n",
        "        - alpha = -1.0: Za słabe, niewystarczająca detoksykacja\n",
        "        - alpha = -3.0: Optymalne (>80% sukcesu, <5% side effects)\n",
        "        - alpha = -5.0: Za mocne, zwiększone side effects\n",
        "    \"\"\"\n",
        "    print(\"\\n>>> [MODUŁ D] Uruchamianie testu skuteczności sterowania...\")\n",
        "\n",
        "    # 1. Obliczanie wektora sterującego (Difference of Means)\n",
        "    # Używamy danych przekazanych z Modułu B\n",
        "    toxic_vecs = layer_activations[layer_labels == 1]\n",
        "    safe_vecs = layer_activations[layer_labels == 0]\n",
        "\n",
        "    mean_toxic = np.mean(toxic_vecs, axis=0)\n",
        "    mean_safe = np.mean(safe_vecs, axis=0)\n",
        "    direction = mean_toxic - mean_safe\n",
        "    steering_tensor = torch.tensor(direction, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Hook class do interwencji w forward pass\n",
        "    class SteeringHook:\n",
        "        \"\"\"\n",
        "        PyTorch hook modyfikujący hidden states poprzez dodanie wektora sterującego.\n",
        "\n",
        "        Argumenty:\n",
        "            vector: Wektor sterujący (kierunek w przestrzeni reprezentacji)\n",
        "            coeff: Współczynnik skalujący (STEERING_ALPHA)\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(self, vector, coeff):\n",
        "            self.vector = vector\n",
        "            self.coeff = coeff\n",
        "\n",
        "        def __call__(self, module, inputs, output):\n",
        "            \"\"\"Modyfikuje output warstwy przez dodanie skalowanego wektora.\"\"\"\n",
        "            return (output[0] + (self.coeff * self.vector),) + output[1:]\n",
        "\n",
        "    # Wybór podzbiorów do testowania\n",
        "    toxic_indices = [i for i, label in enumerate(dataset[\"labels\"]) if label[0] == 1][\n",
        "        :N_SAMPLES_XAI\n",
        "    ]\n",
        "    safe_indices = [i for i, label in enumerate(dataset[\"labels\"]) if label[0] == 0][\n",
        "        :N_SAMPLES_XAI\n",
        "    ]\n",
        "\n",
        "    success_count = 0\n",
        "    side_effect_count = 0\n",
        "\n",
        "    # Moduł warstwy TARGET_LAYER_INDEX (warstwa 5) - miejsce interwencji\n",
        "    layer_module = model.distilbert.transformer.layer[TARGET_LAYER_INDEX]\n",
        "\n",
        "    # === Ewaluacja skuteczności detoksykacji (toksyczne próbki) ===\n",
        "    handle = layer_module.register_forward_hook(\n",
        "        SteeringHook(steering_tensor, STEERING_ALPHA)\n",
        "    )\n",
        "\n",
        "    for idx in toxic_indices:\n",
        "        input_ids = dataset[idx][\"input_ids\"].unsqueeze(0).to(device)\n",
        "        mask = dataset[idx][\"attention_mask\"].unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(input_ids, attention_mask=mask)\n",
        "            prob = torch.sigmoid(out.logits)[0, 0].item()\n",
        "            if prob < CLASSIFICATION_THRESHOLD:  # Spadło poniżej progu = sukces\n",
        "                success_count += 1\n",
        "\n",
        "    handle.remove()  # Usunięcie hooka przed kolejnym krokiem\n",
        "\n",
        "    # === Ewaluacja efektów ubocznych (bezpieczne próbki) ===\n",
        "    handle = layer_module.register_forward_hook(\n",
        "        SteeringHook(steering_tensor, STEERING_ALPHA)\n",
        "    )\n",
        "\n",
        "    for idx in safe_indices:\n",
        "        input_ids = dataset[idx][\"input_ids\"].unsqueeze(0).to(device)\n",
        "        mask = dataset[idx][\"attention_mask\"].unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(input_ids, attention_mask=mask)\n",
        "            prob = torch.sigmoid(out.logits)[0, 0].item()\n",
        "            if prob > CLASSIFICATION_THRESHOLD:  # Stał się toksyczny = side effect\n",
        "                side_effect_count += 1\n",
        "\n",
        "    handle.remove()\n",
        "\n",
        "    # Obliczanie wskaźników skuteczności\n",
        "    success_rate = (success_count / len(toxic_indices)) * 100\n",
        "    side_effect_rate = (side_effect_count / len(safe_indices)) * 100\n",
        "\n",
        "    status = \"SUKCES\" if success_rate > 80 and side_effect_rate < 5 else \"WYMAGA DOSTROJENIA\"\n",
        "\n",
        "    report = f\"\"\"\n",
        "    === RAPORT SKUTECZNOŚCI STEROWANIA ===\n",
        "    Metoda: Difference of Means (Warstwa {TARGET_LAYER_INDEX})\n",
        "    Alpha: {STEERING_ALPHA}\n",
        "    Próbki: {len(toxic_indices)} toksycznych, {len(safe_indices)} bezpiecznych\n",
        "\n",
        "    1. Wskaźnik Sukcesu Detoksykacji: {success_rate:.2f}%\n",
        "        (Procent toksycznych próbek spadających poniżej progu {CLASSIFICATION_THRESHOLD})\n",
        "\n",
        "    2. Wskaźnik Efektów Ubocznych: {side_effect_rate:.2f}%\n",
        "        (Procent bezpiecznych próbek błędnie oznaczonych jako toksyczne)\n",
        "\n",
        "    Status: {status}\n",
        "\n",
        "    Uwagi:\n",
        "    - Cel: Success Rate > 80%, Side Effects < 5%\n",
        "    - Jeśli wymaga dostrojenia, rozważ zmianę STEERING_ALPHA\n",
        "    \"\"\"\n",
        "\n",
        "    print(report)\n",
        "    with open(f\"{RESULTS_DIR}/steering_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(report)\n",
        "    print(\"Moduł D zakończony.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-T5e78rtEiM"
      },
      "outputs": [],
      "source": [
        "# ===================================================\n",
        "# 7. URUCHOMIENIE CAŁOŚCI\n",
        "# ===================================================\n",
        "print(f\"=== ROZPOCZĘCIE EKSPERYMENTU (Wyniki -> {RESULTS_DIR}) ===\")\n",
        "\n",
        "# Uruchomienie modułów sekwencyjnie\n",
        "# Moduł A: Porównanie metod XAI\n",
        "run_module_a_xai(model, tokenizer, eval_dataset)\n",
        "\n",
        "# Moduł B: Analiza warstwowa (zwraca dane dla Modułu D)\n",
        "_, layer_activations, layer_labels = run_module_b_repe(model, eval_dataset)\n",
        "\n",
        "# Moduł C: Test stabilności\n",
        "run_module_c_stability(model, tokenizer, eval_dataset)\n",
        "\n",
        "# Moduł D: Test skuteczności sterowania (używa danych z Modułu B)\n",
        "run_module_d_steering(model, tokenizer, eval_dataset, layer_activations, layer_labels)\n",
        "\n",
        "print(\"\\n=== EKSPERYMENT ZAKOŃCZONY ===\")\n",
        "print(f\"Wygenerowane pliki w {RESULTS_DIR}:\")\n",
        "print(os.listdir(RESULTS_DIR))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOG7eJb442FlvNNT091/DDM",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
